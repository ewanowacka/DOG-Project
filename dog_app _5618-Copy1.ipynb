{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ewa\\Anaconda4\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvVuMJEl2pvcdM3P3uGVd+1bdPcNp\nznBu5CyXFy1XWAJLQiRBCQvskxbSvkiAAD7pffmsJ74KECCIDwtJDytRgLDQBVyJiwWIoVYERc5w\nhpyZ7unp6XtVdXVXVd4iI8IvZkcPZu7hERmRmZWV1Z3VW6eQFTd3c3Nzs9/O+c+xY6KqPJNn8kye\nyWliPusKPJNn8kyeDnkGFs/kmTyTM8kzsHgmz+SZnEmegcUzeSbP5EzyDCyeyTN5JmeSZ2DxTJ7J\nMzmTPDGwEJHfFZEfi8hbIvL7T+o6z+SZPJNPR+RJxFmIiAXeBH4b+BD4S+A/VdUfXfjFnskzeSaf\nijwpzeLvAW+p6tuqWgH/M/CPn9C1nskzeSafgrgnVO4rwAe9zx8Cv7btYBFRRNa+bTUeie91+XZ5\n4mnVkPYCvfOkd+J6gWcr7pHlkercO1ifAJZva+bTjtX2P1n9beP5p7XpeRvylCJ1S0MLdHU6VjU5\nfi+bqvdpBTofa/MTjlu53XBKYUBo7qvq8+et2pMCi1ObW0R+D/g9gFdefZW//NvvISIYhb5pFEJA\nRLBiEBFCCNR1TdM0NLoghEDlPQeHhxwdzdk72OdgOuNwOmN//5AHuwc83D2k8YHBYMRoNOHqjVsU\nRcF4PObmzZsMBgOKoiDLMsbj8fa7sudrDA22vWfELO9NVRHR5W+99/GA/HwXfNT6te3d63CyBt4i\nkuorK7+rKiaYtc66fB8ksFmBNRcHhhL7iDFmpd7L9wFV7e5z1fQ2x75fufdUx03HPJGlEiaVG/TE\na9m1ISamXlY5HRsElm1v+I2vPv/e41TtSYHFh8AXep9fBe70D1DVPwT+EOAXf+nvqmvBQMNKhzQm\n3qz3im8aAGofqOqGSgLz+ZzpdMrde5/wYPchuw/3OTiacTg9Yr6o8F4Yjq9wbTThxo3nuH79Otng\nOnmeMxwO2dnZIcsynHNYa/FhE0JHEbJzNYaYCBbp0acvA4oS0gAUFEHivUt6fQLT2Vk7+PpRYkyc\nubu6dQWuTg2m7eSxHU3vWJU1cDgnWKzfQ1sfn67b9Z+uYhYElFQ3WR30YtLxJ9VHV57eyncXL4qm\ndhSRpVLdn0TXQFH7Q1nbyaY93hAuoC89KbD4S+DnROQ14DbwnwD/dNvBAgSt8I3HuDibeq80wVMu\nalSV2gf2dvc5nB2xt7fH4eEhHzy4z2w2YzqdcjA9wohjMB4xGk946bUvMNm5znjnCuPJNYpiiEtl\nixlhre0AAqBRpYETWRxhcK7GEGz34JcdOILFUqtPYNH7pyzOdb1TKnOCnDRYlq+yPqupLjUmhaVm\nEXo6hjmOQOfWLPrnha7cvmYRQTesaEDt6zrYaFg9pju/fwzHz3sSEgf+8l/3XQ+UTe/Y7vfQB4tl\n+7TaxUXU/YmAhao2IvJfAv83UXn/56r6w5PO8d7HP61pglKWJYuypq4987Lk8HDK+x/cZvfgkIOD\nA+bzOfcOD1FVrHPcevU1dq5cYzK5QjYoQByT8RUG4wl5MWJQRIBQVcQUSe2PDyGEiLuq2mkym8Sc\ns707UBA6db2b2VKhSvotaRYKiFz84zmp05ykWvfNEGXVDMEIvbvoiUE5wQy5EO4imXgmaRadxgCS\nNJlY19YUWVcIeqZLX2Fqz9P+QNUTAeUiJCQNKGjoVUdWwCpA7z4lTUI9aftYZ4YEVB+/rk9Ks0BV\n/xj44zMdi1IHjw+exXzBbF5ycDBlOltweHTE3sGUhw/3uPvRx5R1hWCw1nLjuZcpioLhZMyLL7zE\ncDwhzwZgDXk+wGUFWZZjbIH30AAiFtfOcnGEgljEyDFV71g9pTlnY6TBItIhTjfTpbm3r2625og+\nASJQ2d7ZlQhUK9916ndbP+nqmgqB4JfvRVl2UEUSUKhwXJN4jNtb5xeihdQO/FVuJb433fNd1y7i\ne9lYZlKl4jm9949Z/RNurMc50IKzLnn+dd4lAbixx/utpOcQUPQCgO2JgcWjSOM9+wdTmqZhd2/K\n7v4hD3YfcjA9YjqbMz2ccXg0o8HgiiGT8RXGV3bIJlfZmVxlMplQFENsVlAUBTbLyFxBExTBYlxG\n4xtEhTwvaJpVXiJ2jrPwBPUJv20Xae14adWL6PxZIRbbGUJ0qfWEi388mwnA7ptHOnfTLKvbjteL\nZWA2EbAAmjivPhm7Tm6ug8U6wBwvc9UceBIaxfo11z9HDJaNZlTsL35rOUaVcAHQdinAwjeBj+49\n4HB2xEf3PuHh3j4H0yPKRc10NsdmOflwzM3rzzMeTxiPxxTDAcXoBtZa8jxncuUq1magBq+BEAwm\nqaEahCwr4rW84o5pxUnN0JOHy3mbWzo7f1UHltaiVzCinYcvvipyocNrg6x7N47Ntmwm8TYM/HaQ\nikIITVem1wYwiU/oe1ts79zj1zhR3V9XqUVX76X9GkB9GkxtmfFvWWS/M+gKGPSo2e431urVr9tF\n8AJtGUZY1fL67l0Fo6teKZEWRExbEIqn5Ssuoi9dCrCYlyU//PFP2D88YnfvgNoHvIIxjudefJV8\nGEnLm8+9QDYYMhgMcM5RFFeB5DWRHNQCghFLCK3qbJAgG9v90eWcZ7ZeAF33tNg0ZayX/+Rmrich\nppt54/0FSB1V0BDiN2Ijh6AmjYRtXMYZBp1s+71t303tGGtFP1Zk5Zyu8JOv/YRl29W7W+7M51Vp\n3fPd5+6ZRC/WpSU4H1Xm8wVvvvMhvgnkwxE71ydMrlxjNJqQFwPyYshwssPOlWtRe0isd2EnqCq+\n3wHUHJtlWlu173ffJic1qpXtcQ8nEoddgMaG62rLWSz5gGVn8MePv2DpqDFdne3bmXjFXDqxoEBr\n94v61EEbGt9gjOmI40jo9j0ndLP1Ni5g3ftyzBuydlwHFQm8jp+/9hz6XIqGXjk9F2T/r3e9lbI/\nFW8JiC55rmVbJvf8WhsGvRhPCFwSsLBZwcs/+y12dna4ev0meZ6TZQU2c1jjwLqoxppYXRELIjRB\nEDHJl28IIaQ/xVrT01YDwoZOsEFOtEf1hOY68YG0ZbZEZ7+zGkQ0qdbtX/p96wx6cRKplG0gqmuv\n2yRqDNo7P4SA94GyKjHGsTRFEtehBpG2I8va6/pzOGm2Nxvq3Va7be8T6n+q+9awnM7Xp/X2eXWF\nnVLW6dK3sNar3XnvIAUv9rmjPikbJXSges5owjW5FGCR5QW3vvAlRsMxo9EImxddDERV+9TJLMoW\nUk0F1YAKiDU4l1ykgKoHAmJ6JuA5n6ma83lDNl+u18FbL4KseSM+JY248+evX7Dz759SkQ5TDEEC\nKpYgnkYDMcYtgbjKEhSFVe/CUh1YKTJV7BTpeZsIG6wQ091nP37heNkm1UmO1WUTVKxjxUVD+zpw\n9MlN3xGy8X5EfK8dWyI2RBBXf4zmOY9cDrDIcp57/hWMsynmwRDUol6wLodOPU+NkxptyfCmDqB0\nKm9obeWOMQxdOSf5nE9mureDxUnnraiBx2a55AkxG2ZBPV/E6HlFWfUgnFXCCgqbrpOihqBRawq6\nCvSqfXPrtJ58hp7etVs/fqP/us30PEWzaOv5KaHFNs2ir+MFjQF82ms/pV4Cb6pawPSq+Pih9ZcC\nLMQY8sEQYwze+zh4EikYQstE95hnE33mtqdCeu+XAVYpuGpljEq79kI5+bZPGPTmBNfpWcGik75m\noctQ6G5WUwifLljA8bpuNgtWRXrvpO2kRlKI8XJthapG7eL4iSfLmY475aB2zKtZ1SzWRFcAQJbn\n8algxWpdTtAs2mUCkLgl41Md2lr0AFJPMNUeQS4FWBgBY0IyGZbE0WrwjwI+NVA6ztiuAcUo0gGK\nEtcIxNLaJ67aRvQdJxyXUX7b+QIJ57P9jnVj0VXtJmk7y3Uhmnrek+csThTtEXhKapPjC8YaKUAC\nNrnoMvFY05DnDVSHNF6RMEawkexVi+JwWqdw5OPSDQSheyanA1cbzbh0SbeEsaQ+IJIMkWMA3vaN\nPu5r0lBZITiXs/fy4GNRlBctktiZ5P7t1zOoIqFoDzsmFxXcdynAIiJ+6+PetnKQTtt4InXQDavk\nn6B8GgE+n6VEzc50MQ6Pep/LCNfNv32W7fZ5f3bb5FKARctDSM/8aDFyda3Gqip1Jo/elkjDjbX4\nFFxffTk5mvLpEZXVGU0SYSsJLGz63aBx6fVp0W+brrHl2ZzFTHoSclme13pg2JOUSwIW0hGTrYmg\nuim0db3DnE5Urs4Al+MB9+XzAhgQ29pIwAhYsSgeZyxNWPJJYhJQaPzuNHhehmIsYzE+zXbqTzab\nojU/C5BaTqyy8t2TlksBFgI4a5PdZ1Inks48oXtda5AzWCWbHvanrEBslJUwa453xKcKOCSsPJoW\nD1oFQjVgNXQ00FlMvk1myPoiqs8KNNrPn9UzOmmNzpOUywEWItgEFmjr9txkPhxH9pPK3NapPs0G\n3iRPFRCcUSIJGfpfRNAIPgFH1BKNetT0PCRnLv/4kU8dqD7lcmn2DTmWuIQlevf/WjlLJznR3XcJ\nZoX286a6fJZgdh5p7yMuiAsYDaAaY81CGxuTvFgK53Xlfdbtsq3fXBZN40nKpdAsgC4+Yhl+K6eq\nfqF3jPfJz9z6ocNyqXIbdyEi6TrL9Rifhhxf4szK5w1nrLxu6oif9aCBVVA3IhifeAlncNahtSd3\njoBgrMNZi8rSvOyvBl16w9aIbVnVIDa1w7bvNk1ALXfSvl+XTWT4WUjyT+t5bOsH0fu0jQS+mGtf\nGs0CtjfEec7dJJdhgH0eJbRkRLtwS1fDVERjDE0MwX/84KBn8mhyUZrHpQGLddNj2++PUtbTLqvh\n0bryd+lEUmasELmKLug6eKTlL7pqh8/NM3pa5CLa+9KYIcdV9bNxEtvUw89LZzxrjMhnKTGMOyCq\nqIYYZRjAasCEGFtB0iqUdB+GY3lGnsnllksBFkLcB6Gz5yWGP7eLZlYO7Ek/28O6t2Sb9JfzXnZZ\nB83LDhyawqFFl/wAtBrFUiNKgdUJKDaE1V/Ce3sml8UMkX5Q1nFvwb/LcmnNjp60fMSKmdSLiVA8\nEhJQ6OdTA7zschHtfCnAQmAlm9Kx3591qKdE1hIhrxGdUS438H0e5aLGz6UwQ6Lrs+7eRxOkFwbI\nupvTpP+lWzbcrsZrpXOVtd9qQESxBmJaQl1xo62ftzFbyAkZlzbN/m1Z1p4Fk497CUyXlGaD4XTC\n9daD0U7rLO3vIUB/sV6g79K0qTZ2ea/pJbOAKCYDMmVBTU2DcUptQ8pR4hGtQXKgAVECvUxZEu+4\nLVa1XXOyzPXRurwzNt9fQDEi3ZKB4OlydIj1XbuZ1KBRAVrbQc2sa7bRDY8qNu3jEj9qvN4Gbfik\n/KKPKv1n2oWwbXiexmt3P32JSXIuBqAvBVg8LRzC0ySPEj6+DKOG1lMR3ZxLwIUQicp2/41ecZLW\nfIgCKUu5SQvSj0tAUERDXJ6uEr/TWHYqsVsRL23W83PcX5vx+qyrlZ9psCfLJQGLpfSJyrYTXHab\nHc4WuPNp1mVdNtVtfa2FAQgx56QQ4yeW/EO7peIylmJZHhAiOW2IZHUXetEdFJbu0xSYZTS0SeFW\nEtKICBqEICHmKEmlCBCzhi/ziqzn8z5LWzypYz5L2QaesuH388qlAYttKnT/xi8zaLTxIet1/Cw6\n2TYPyrbPXZQrCu1eE5JmduhSFkJrEtmVpDWxE0VtwWr7fmkWCmkjtrSBEupR+ruHafKAtaaC6YAn\nXmbp91JVvMmWg6EXOr7e1mk/6qS1b3fJn6Z5PS0TVisrMUtr3z2OXBqwaKXVLJ6mh7NNPsvZ6KRQ\n5k0am0g7ux9X2TfZ5P2jut3Hu8MMXVZsTQaJWNqtDEMPBvr1iEBiunK677Fd0JfRVZd5x3UkZGu1\nj7YfbQPxzfd1/PenrR9uilP6XBGccFyz2PbdZZVHCSZ70rIe+XnaMUuV9eS0gb47ZxVQTMsLtEQD\nhiAGl3KnqhhULGpsd25QBdMS1b09L/r8giTdRJJZk0jF49JLxtxL43zexV2X4RmeR54kUMBlAYtT\n7udpQfjLUs/z1CEOtOOLkVbNwe3lt0RiQLGiiKatGzDRNyApsZFYgoJHUs5Ui+9Gd7q+9vNZJqJV\nDMfS+J/53vTMi6m2Da6o9TzypT9Tac28i5LLARZbZLv93/ba7e7BTasMWzdXf4XqJo5km7SXOOvK\nx233sa3sbbzNehnbOvRJpsdpNnl8Y/C+wVrbbafg0+rduHGTTYDou1W9zjlMciX6UGNUyDKHweKb\nEhWHNYpzAxBL2dQoFowD41L2djDGYq1FNT6fJj0jY9o9awNiDE4MoV3qnljUEEibSzUMBwNEhKZp\nVswtY5Yu377b/CycTn8S6EcAb+LVlmb05rbeZAKepJEeMxU39NltEjW1Ew95JLl0YLH+MNbXjLQS\nyZvtv7fSLt1diS7sqboXoaY9LWrrtjZqO2tZVxEkiGs8kBi70NQ1uVuSiqTNglTjptaSynA2Bwks\nqgZDgxNBrKOuKupygQuKyQfpGqDG0DRNBB4PGYYsM2TOoZXt6oxojG0IihfFpJgca20EK5fhvadu\nyi3AfP6Yhz4IqC5f13/rv66/32jyrXM1T4FcCrAQloM6vi5Jqc2N2q0u6H5vZ7qN5fcQWVUR02OL\nH4EXMeZ4p7gMZse6HJ/plvd6klvVmZ5Gocud0DNnMVaomyrFPkQzIQa5Re+GCPF3EawxGAy+aRjk\nBb6u2H24x72P73Pv4UN29/aZzmc8f+tlXnrxZZ577jmuXLkGwyFkGcZYQtIsfGiilgA45+KG2CYC\nRe0bqkXVaTzOuaQF9TRJbOQ5jm1KfTZZaa/03pjUn7Zomtu+23TMo7jbP2twuRRg0TZX25nbCLm2\n05/EZp91sPYHSthwynkJyqdlVliXTfV2mWUxm1MUGcN8sKKqiwhZFkHae09VVdR1TQiBPDNkaZAb\nI1iU4D2hrvjxGz/g/Xff4b0P3uf23Xs82N1jUTWIddy6dYsbN25w7fp1bty4yc2bN7l27QaD0QgR\noWpqjo6OmM1meF/jipyiKPiZl15mMB4t1xNpDkZwCSg0RLMpOkpi/c8URHuGNjsvH7Qu5xn4nzXh\nfynAYptcxEA8ZrqIoGHzg3paB/66nMUl2ErfBvaLI65M4iDc39/j7t277O/vd4PtueeeI89zyrLk\nwYMH3L9/n+l0ymy6y2g04erVq2RZRlPVaNVgDfzVn/8Z77zzDrPZDJflYOL+tU3w3F5Muf1O1ACK\n4YirV69y7eoNBuMRWZFT1zWz2YzZYg4EiqKgKApe+5mvcO36DW7cuMHNmze5euM6w8E41jO5Vlsg\nWYaRr2ZS67fD4w6+kzir07Tdp0kuBVj0m3MbZ3HusjfYkyelIPu8yKa2Ww9wO/YX4gB/+PA+P/zh\nD/n+97/P7du3acoSYwy3bt1iMBhQVRUPH3zCgwcPODo6AqlwLmc0nGCMoSpLqGtyZzh8eJ+yLEEV\n30THpgbFqFAd7Xfk5OLogPJwn73793DOMS9rvDYdp2EyR5ZlWGf46U/e5fr167zwwgu8/MqrfOlL\nX+KLr32Jq9duAAZJBK0xgn/Mx3waJ7betvGY456jkwjwp0UeCyxE5F3gkAjbjar+qojcAP4I+BLw\nLvBPVHX3lJKAvufCXKjKtc5mLxcmnZ1ZfppkE4m2DhTta2tmhBCoy5I/+7Nv8+brb/DGG29w9+5d\n6rqO/EBTcuf2B1hro7ei520QKQn+iP3dvWiuGItNcRaDLCd3lqZpqH2gCRAIBO/RugQiB2CCoakX\n+KbGa+jydHYLobRhUS/SXrg1BwcH3L59m7d+8lPee+09/s7uLr/27/8DVAWX5+n+Db7d+/Yxp8X+\nJNb/7qz81XlM3LPIp6mhXIRm8Zuqer/3+feBf6OqfyAiv58+/7OTClAA42iaGkQR0+BDdOFFoi1G\nBBrNYoRfWqpkpO753iOB1a0VWHFrRaRvG7W/0nSdwT7Rddr+dNIx3euy3NZVu83siYSh6S3cSmaB\nxEhE0yurvyYjruU4DnahLgnG4sWQDYcEFRa+BjVkuUXLklAtGBc5VyZj7t25w3tv/5Qf//ANXn/9\nh9z/+BMOp/tQlWShidES6nHzgHUGFzxZCBhCIhc1agAYsqwgzwcYohtU1OB9wARPhscSML6kDg3e\nJGJaBXyDaBY1gl5LBoltaVRT0BeIf0hQSyY51eE+b7/xgOnhR+xcH/HqF17j1kuvEoLQNBWZy8my\nOJvVZdW5hdVX3X41TV1uXPQWScwUMCbQhqN7TS7XzkNijy9I1g3PXLuntna1fh9cBfP1+vSPFkmx\nH8oy/H7DhLCN2H5UeRJmyD8GfiO9/x+AP+U0sFClafrx/WlJtEZQ2CznT/z6aduLZ42LeBQtx7ZA\nYzYcqwZrMowzSNAYK6HgnEF9AO8ZD0fQ1PzkzR/z19/5Lj9988e8/v2/ZVHOUB8T7Ip6vG8IvkaC\nYnODiZvc48QgRpL3pEHExgjLEGiaBiGkJe+Ctvk5jWCtEEKMefChiiFbGs3CoA0Et9IGBok7Z/eC\ni2zmsElV8HVDWU25/f4H/Ntv/xlf/fou9bcaXnzxFQajCS6L2K49b5mxYHCdhgRxsPUT85xEqHfH\nnPKszj5Al1rLWfrlWbSUJ9HH5XEKFJF3gF3i3f53qvqHIrKnqtd6x+yq6vUN5/4e8HsAL77yxV/5\nF//2RykoJ7pPW3W3NUsAjJqE4MmMSJ1wab5s1izWkVr74cWPIJvG5aPIeiBOW89NJGw7yADshiXW\nssGsaGWQF1S1j/yB991xVVWxODriYPcT3vjRD/nOX/w5777zU8rZnOGoYOws8/k8eTpK8AFkCTR5\n5sA3XX6KjusQSeCuGOPIzNKF6VzUGFsNz7pYF+9rGmk1i1a7sl17ZFkRF7O1HjI0eTlCAgChCSDG\nYvMCxDJvAticLB9w9doNfvZnv8zPf+sX+epXv8rzL77AZDKOLt56tU0IGu9zzZWuqssQ+N7D9ynP\nR+ie4QbN4kQ52TVzWj9Zn1ROIlj7msVvf3nwHVX91UepaV8eV7P4B6p6R0ReAP61iLxx1hNV9Q+B\nPwT4xt/9Vc2dTe6yFhwMIXjQZRX72aGfRtkUM7LJNOk6ra7+vpxt6XiXvnRmVoqCFCBL50ynU+7f\n+4jdh/f5f7/9p7zz9ls8/ORjkEDmwNcVh9NFAgclS7sZi7TbSkZTT00bpr28vvb2pzXGxDiLFDEp\nAplrY2jA2gSEWeQT+jER1hja4AWVdtFZO3CTGaCBpomaZ4zIhaZc0Chk+YimaZiXBxzs7fPJxx9x\n9+4d7t5+n9/4rd/CmBcYDoeE0KykcQwa0iK3ZcBVvB60K2VXAHlDfMWjEZhnT45zkjfl7Ne7GHks\nsFDVO+n1YxH5l8DfA+6JyC1VvSsit4CPTy0neIQGK1nMhSASuYkeUBiFLptRevWXIyvgI8m2AJ5N\n4NGZGhvO688mywGVuI6mXpYblIODPd5/7z3ef/cd3n7rTX7wvb+mKo8QAs5YqipqC6OMTkOI8Qrx\nNWjoyhIRgrAyyDPnElHaz6Wa6hmaqFFIiN8FUELSOgQ1CmowEjODdVnM0j2hKQNWCHivaAg0tcc4\nS54PUvyE4MRwNDvCuIzCZeTO0dQLPnjvp+zv3Wdy9Qpf//rX+fKXv0xRDCLhWsWIVYz0DIHNz6X9\nvM08WV+CcTpBvw0wAiKnrdE5DlKXGixEZAwYVT1M738H+K+A/x34z4A/SK//22ll1XXFYnbAeLSD\nsRlWDF51Y6OZTqt4/NRln5Wve51B39QpgXbPno28RAghqf+r6qmqYl2GmEiYzmaHvPP2W/zge3/N\n7Q8+4IP33gFfMSgyQl1TVXN8XUZzQRXxq+7UJShEt2fcwBq8j9ezAtI0HYnrxOCdS67YgDMB62zc\nBqDLV5Fg3gtouyeqgEin2oeVZe5p9g/xz2UOUEJToz5gjMNYYZg5yrqmqitslmNE8GXD/oOSf/2v\n/pjbH7xHOf91vvH1n2c4HHb3ZxL/sao9SGcexc/9Z3Nc7d/mFTm5j61ryKb7/vicIlsBY/36T0oe\nR7N4EfiXqZIO+Beq+n+JyF8C/4uI/BfA+8B/fFpBdV1xdLDHMM8ju66pmxjBNy0r3iZWgZiGjeNT\nwVMgZyXE+uZKt/u4rB27wUUKUNYV2jSor/n47m1+8Dff5zt/9Rc8uPcRo0EBocYHj/c11iqqBpGU\nT9Iut/1rF4yFEGiqCjWCzRxGlSb4BEwOJ4Ix8Zk553DGxnObABriGhGTNh8yButc1EgwHRj1d1hX\nVQ6PjvAxCWfUWFrXZQgsSo+IQaTBiMPaQGh81LB8wFpD5qJXydcNIdTc++gOR9MDFrMjqkXJN37+\nm1y/djP6/Ouw0rbtcgMkLL1a/bbn5Jl8Ew91tklpu3ndpgjsg8b6NbfJRU2I5wYLVX0b+MUN3z8A\n/oNHKaupKh5+9CEO5cVXvsKiXBBMjm807ritKdVbUvda9RRWbf9NxFD7vo/G66rbNpXvomVdgziN\n1V6/B5HoGTDG4H0kMNs4CJdMgXYH+unBER99+AE/+pu/5nvf/w6zwwPy3CDa4JtF9DwQSToNgUYD\nLivi4JS42jQ0kQi01iJFgU/h3c65ZPvHzl2giIu8RZttSyRgMiF3BcNBDiiinjyP5zpnME1sk6Io\ncEYoy5LFoiKEwLjIKRtPWVVUTUMToAkerwGbj4B276K4eZEYizG2C+LSOpoWFsU5ByEw3X/IOz95\nkyLPGY+HTL45ZlHWZMWIxvuuHb2PmzhHrmYJFtsGXf+4TcecDzg2l7Hp/NMmoIvSoC9FBGddldz7\n4D3GxRBzqwZVrDZIllF7XerjEFXvNiXSmkQw2fxgVkBlw3kbB+ZnLEY5tlhJ0gCJi6ToFlHVdR1J\nTRHqoNy7d5c333yDN954nY9hb5VAAAAgAElEQVQ/uoPVhmHuaKoFJg3mGCtASjS1uvxZfTI/LOTO\nYIyj1BD3LfUNFiHhA8MUuyAaYh2SuWFEubEzJM8dcbFZxrDIGAwGOOewjeAyw2gwjGZQWVGWJU3T\nsF/kLKqS6cwwm5fMqxoTlEagCm38iYkJdUyM9ai7mBCQEMAIuQFnYFYuGOZD7tx+n/n8iBefu8mN\n69e5fuM5jAWHJahSVXExXGZdB8qtnGfQbXOPbxTZoFnouqkdJ4NHlYvoz5cCLAiewzvv8m45YzGb\n8cqXvsKV516ibJoYsGMdaMtjSJeeDU5u/E0A0A2QNTmpI3TfXyCAnKne1nTg1+6pEkIgJILPGENm\nbXQ/GoNN7spmNuNP/+RPePutH3Owdx9rPI5AuThgOHDYzBIaaBLXYDGIMWiosRIJSpfZzltoQwNe\nGTnwvsEEjaHXxOtdL5K2kIhPIVAUOUXu2BkXTMYjhnlGlmU424KSZ+hG5HnerXZFAqFuWFQN87Li\nqCw5nM7Ymx5xcDTjaDanLEv2a0PZeBaNp/aeQIM6g7gMsRC8p6qr5PJ1+KZkaHLUz7ixM8SXc/7X\nP/qf+P/+4s/5zd/+XX7nd/8R1jqssxiJGlpdR5I4z/OuzzzO7Lw+aW0Ehk3SHncMND59uRRgoT6w\nODhgPBxx/6PbSJbhxTDcuYrgQCxiwDcpr3Nyy50H5U9ij09iuy/K7jur1rIN1NbFe9+ZJN57Dg8P\nufPBHaZ7u/gqrtFQaowThqMcJ+CCEJxgNe7dEVVVi/oKayzOtjNriudIgUsDZyNv4YRRMUiRkIEX\nrl9hMChiO/kaCAyKjGFRcHVnzHg0YDDIKTKb6hxBalDsYA1drIUxBqxhmGfkzpA7S2ZtLGuQs585\njo4c1SwgUhESJ9GEQPDRZG00ahfRfII8s2TGMq0ajAj1fE6tymi4w+3bt/lX/+f/wT/8jd9iNJ4A\nUDdxHUyW224XtYuQlWd+IlC0v10sOHxuzBA0cLj3gKtXr7K/94C96RHzuuHnvvktcEsfP0gyQ7bL\now7ssx7/JMySR+Us2npElZ/ur+UOZrMZ9+/f5+033mB+OCWTaEIQBIPHIvimiTkzg5JhwIKVmKXK\nZIbMRc0iM9HzkQmQQrtzAPUM85zJeEye50Dg2mSHosgwCEEbLJAXjkGeMRpmDLIcl1ls606VDHUG\nm7SMkMKnXTAgghUlWAuZYoDMpk2D6gaamqlvXbgGL4bQeMpUhkJarm4wGuKAl8CgsCzmFRjFiMFZ\nwXnh43t3+e53/4pv/vy3uHXrZeqgaFCcs6jRXt7RR5O2Xx2P0TgLULTvT4pefjQw+dyYIdYayqMj\npvsPyWzOfnlIPpnw/K2XGV29jhUwtsCriX56sTHN2wn3vy1g5jTOYluw1OMg8+OQWuuaUAsWbZIa\nYwwEZTY74pOPP+b9d9/jow9u8+CTT6gWU7yfM8rj+hLvPaGpcIBgsUbIbY5zGdZacjw7k1HkSkKD\nFXBGcCiZM0jdkFlhNCi4MhkxTNrFZBSBwzqDBJ94DkuWZRSZibvASRvlFGM2LBmVyeJgdqnjp7gO\nbyBX292riuD9gGpQEnzNxIPqjMo35C7DG4v3DTVtCr3kzQlKXVZgDZpn+FCTuZhp/PBwHzE5Liv4\n9re/DRicc4zHk0RyptSBNtv4PPvP5LTnfrbnvwlELmZns4ua6C4HWBhDUy043D/ghRdepiCjLOd8\n8sk9Xh5NCKbGqkWD62iDEOKiJthuUmx7f55x3/rXP03RHo+77s3JMsdiUSIS803u7+9z584d3nvv\nPT65dw9nDHYwwDfKoBC0mVNXFc7GhVqZGHKbk7uCLIsu62sDy3PP34CgNFUJvsGiZALDQYFpKgbO\nMhmN2RmPKIoi5qLI8gQWEmdzEWyK1kQbbLdZkUc1Lh03xuBDcqmaDDSaU4SAV88gy3BOkcaDsXiv\njEYjVJWDpuGoWqS2iLlBDQlo0p4lkgKtYmh6dM9PRuOUS6PGSYwr0RC48+FtXn/9dbI857XXfpYX\nX3wxai4hkJSatcnkbJpoHyiWXMUm4DgDfyHh3LzF5wssgBcmEz7++C63fuYLjF2Gzg/Y/eAdrg0G\nPP/SFxm6AaUa1GR4janboAJWG8NCctr3H0CzzAqtRB6EVbdpW8JJuS7aPTQfXdogmxRwtGILS/e6\nelnF+DirahbvKWh0jUqd1kioBQ/iA7PpAQ8e3mN6tMtR2MW6ijyGfhIWCyTUOK/kIuQmMB4M2BkW\nDIqCQZZTZDmTgXLzSsy16RvB1zEsOreOzFkcE6y1DIucyWTS8RYmy7u2E1GcRLKSEN2ly1iKgJFl\n1Kl1EHN9ChiHhECwNdo0iMuwXsnT8nQzNDgvjNSye/SAkYXKKr6qqesap4JTaMoGFYvP4t5oxjka\nYzCiVGUTNwoQk0hhQXzJfO8Od36ac3PsuLGTc2VScPP5l1g0nqGNRHCe5SzqCsXgsui5cZo289YY\nih7QLhoUlsF0bVzGMZKzG/zH+9XxPtjPG9v+vpw8QvBs0orPw+1tk0sBFiLCeDQgPKi5d+8eV154\nkbqecX/vbXAFjWa8/EqGGxZgLQRoGn8MMU9rlM8q8m1dznLNLiZDYvRiCzQmRWbGzNvxnrKUv2E+\nnxNCoHCWhfoYKu0bfFNh1ZMZsBJJvyJ3jEcDru1cYWcyYVgMGLnAzs4OmXVxoVhyn0Y3qSEzFmct\nee4Y5AVZFoFFxXT1FW2BwRMkoD4g1qYdypZBZvFZgEgMt24zdduWnxCJGGKWQWKtWeKsZTAYsIPg\nsZSqhCCR+A6Cb8PTUwSmE0OQsMxWbgTrHGBoNHB4cMDdu3fJ8gHFaMT4ynV2rt7sIjxX43ROjmnY\n1AP7EbFnff4nybKfR8DYFAW8+fjHk0sBFsYYbty4wUf3P+H9997hlgjDq9exZHz/e9/lk48fUv5i\nxVe/+SuE0OCMQ3LbYe0qEeiPla/aztrpVZ4MN3Ga/M5XRk+o5AHwy+nvmZwm3/jlX6fWRGQag8sL\npof7/O3ffI8PPviQO3fv8Zu//bt8+Stf6xJBe++ThyoQQuJSUlfr70uy1Bp04+R0folrb/oS++oq\nUPTfX6RWARftn3kMGeYZo9GI+fSQvYcPqMs5ubPkRjg82OP+vXs0vqauS+qmZJvduAlh1xuz//2n\nJU8OKJ7Jo8rr3/1/wMed361Ez5LRmDvj4YP7/OBv/pY3f/w6VblYkqxJY1rVMp7c6ufNffh45OYm\nsvWY5+wEreNR5NKAxWg44OrOGA0emhqTgnt2xgMINQf7D1DfxIVkwWMIx5DzpMZsP6/Lun33TP7d\nEFGiWWUMoWlQH8idQX3D/Y8/4qdv/oS9hw/isWuz9qrGsGFTZgnHzruwem9NnBEX6q3/xTU/3U6R\njyWXwgwByF0WXXDOgnoInoETrMsIpWc+m9JUM/LRDg0QQptur5XNO2lv/E4v3vw4OSJzOQP9yVuz\njXUyG56mTQu8ovYZmX4NQkz84rn9wYfs737CR3dv8+H773D7w3f55N5HZLvvMSkcX7r1Ml/+whf4\n4isvcm1nJwZkWWUyGEQPhrFYIlnsjIWsWFG3W87EGIMSQ7lVJKniyy0CQt/ykxjbEELM0akhrdVo\nPD7U3b1aa6FsaFg1G0NoCCEGm7XNpipUZUNVVXjvuX1/n/2jKbPKczifs3twxMOjI6alZ17HgK+A\ndq5lK4bGK3/7g+8un0nwiEYvjBXw6gmNYZDnBJR3fvoWb7/1Ji+9/MpyTxtrI4cUTjIvjgPFec2R\ndZfrsqwlOR/LXW40vckD87niLCS5u4rMkQksZkfU8xnOCCg4lGoxZz6bYosBqhaPYl1xcrmnPKBN\ngHHeWeCkB9LP+dnGAbTX30a6qirR+SJIiJ1fNC7hVhGqqqIs58zn85iJyjeEsoTgcQauXdnhZ1/7\nGX7ha1/jhRvXcChNXSLBUzjHsChiBi4fPRTWpIHQEpWm3Wi4dx9EN7eYRLz2AMP2Ombr8m3bxCg0\nvfbp4kSsQXplpF+h0xqXbaH4bsbWEKMxh0VG4wP7Mo1BZkYImSVYwausbDxVVuWxZ+LrBuPS4rGy\npPEL8sEIY4SP7nzIW2/+mL//6/+wV+/oSSHUZ/KMXQRArJezKSaojV3p1vU8IQ35UoAFCIvFjCLL\ncWlLOw0Ni6MpwRY4U+CD5/DwgMFkB5MNIKVgg5PjLNYH40mI+6jBU2c99lF5kvWHLkS1WdvzE+E2\nnR0xm0052HvIvY/uUC7mjPKMwSDnG1/7Or/6y7/C89evMLAWbWpKUZxAbm1cWo5BjKI+EDQm21nv\njO2fT3udQtwBvbWHQwjJjU10laZ4W5NyWoRa0RSpKT52aoNAaLcfYKODwaXz66rpNjRSjQvPssyh\n0yleDc6auJ9IXtLUChqofYCmie7M5NHJ3Krmlpm4piZUgSx3CMrOeMK8XJDnGfOy4vt//R3+aWjI\n8wl1ymbeuijb3J190ySS68uAsE194KSd8/rPfP289e/WNY5NSahl7Xk+rlwKzqIdDM5YdsYjcufi\nw25qCmugqSnnM/Z3H1DOjoh675JwOpuY3t9mIrT//ba/c93fGlitg8E21jqs1MdiEEzyzQ+Gyx25\nqkVJs5ijdYVpGgzw89/4GjdvXmeSdvcCuiCqAHgf3Ygxi6FFrDsW2hx6Lj/dYra1Lso2y3jXykpn\n4hiJoePt9oJdGbIK6uttEDWKZUYuY3pBWMbgfVxpm1lDnufdOhDDKtgBK6tH2/JihGp0Ced5TvAN\n1hqqcg7Bs/vwk6Q9RU2nX0Y0g1flLGr/RfAXx9qp1brSX//zym+PKZcCLFQVrRvEB56/ep3r4zHD\nzDHOMgbGIr5CfM2H773L/TZvpN28C/a6xByS9hjarl+/fT0NLFp2edP526TPR6z73Dex2N1nCYTE\nHTgx3SBpk+WPx2MEpTw6oJoeUh3sMTTC3//3folf+MZXI9CGQFMu8E0VF5ZBBAkN1AreQDAWtS4m\npU1/muoTQqBKC7+6fBkatwYUiCZMSmvnUrxFC2J9c6soim5peluOiCDWIMaiSMyh0WuXdrbMi4zB\nsCAvsph2TyCIdqZYCIFB1svVKkKWWUajAcPJkHyYUxSrYdsamrQIL25klBnB+5pBkSEhsDMuODx4\nyI9+9CN2d3fJ8zzyKGn7g2WCoASSdpkuYNNzbuv1uGCxDgzbQOCi3aZwacyQNKi9p3AZowGMsgJn\nbezkTQNq2d/fZXd3l+eOpjETuN1sYjyq6vU4JNTjPpDTTJQ2Uk8k5Zluw5gbz2hYIEETOTxioA1f\nfvVVfvlb30CbuKAr2vdKljkqDfjQYFKyWyMxJ4bXyBlhBaUfk9Lv8NqZKbKexLa3SMesgWCrnQTV\njgfZdM/xe9MRo23gQqtRxPpEht+YaKbYzCGhpmnigB+PxyyqkrCAqqlp9LjHrJU2Z2i8D4+qJbPJ\nvJBA42NG8h/+8Idcv36dr1zZieaY9zjrsNYugQ1Pu0l3DNRdnXxOmhAeSVLmrrOUcZEemFYuDViY\n1NiDwYBgY3YmJ3FHdYsiwXN0cMj+7gOm0ynD8QSkSQi/NC9aaVniTWzy+nGbHuo22TRjPIo8ykMU\nkTiLa9xIKWpIdAE/rV2fWWFc5OxMbvGNr3+VWy+9GN3PRgi1R5sayTNMpAowrtWQoncFPI0GLA7a\nwSwgYRUsTKddrbVJv21aVr610SNHnfiJmL5ObHoufjWqcRPQR9ef7Y5RTWZIWqhWBUWqmtAQP6tH\n6gbfaMcrIEKWrWoWJmUSj6+KD3W3bUG8F0+eZ7z++uu8+uqrvPrFL6BiYx4wH1fwGtuS16tksEiP\nX2Jzv3tkkVVOYrW8TTk7u9o83nV7cinAomXONQSGg0F84BJDjMXkFLWnDHGh0XwePQDee4w9HbE3\ngcE6m3xWzWKTTX0WWQess7wCiFXUR7Ks/bpfd1UlzzIIikF46YXn+NIrXyCzMaNVaDxNFWNW6jqg\nISxT8RNQjS7RNh2X19CBtrRJNUMbhAS0JFpYNde8TfVSoMu4RTewVeNSb01JhkkDNHgfQbAX/RiB\nP/Tusf9b1GpCu29MKn84HNIgHJUl3q+CT1CNHpy1RVh5nuNDSziC0tvKUT3ODZDMcu/ePT744AMO\nDw/ZuXqdmAY0YMR04NkBau/Z9U3P7f1qmya55eheH16WucoQb+prFyWXgrMgLZ9umiatIIw3mWUZ\nWVL5bLIRvfdUVdXNGo+qkq2bLf3vzwIWF2ELbrvO+vdx0Gp7cVrvQTtIoiaW07LhO+MJk8kE0UDm\nTEpxF3AuDk4RiSSjhrQ+o+V8lmRxQDvisY2HaK+lmjwnvfUSm9qi/71xNmoSQrfQSqxB7HLAd9sB\nsrxulHUzInT32u18lrTRoihYVGWXCMg51+2tEkKImzP3JN7bkodqPTttHwMI2rBYLNjd3WU2i/Ex\nrUfopOfYPTu52AjK49c8zlesk/EXCRiXRrPI1dNIHTeAcRkmG+N1QD6cMAjCx/t3UCzldA9Z1DQH\nFUXhsSJJvQ5J8UrZmJLabCBtrydA7OybdvhaVmY7o90y812KNeHU5CjbbPT++z7XAnRkYmYsRhUj\nHmNCvKsgaAOihsOHB9z78A6Lw11euDHgtS9eZZgdUBhhsVhgRVDnWNRN9E5Yi1GDFdul/DddXaLd\n7ZwFiYvVNPgVj4i1NjZv6xb00TwKRM3DiIA4oEaTa9uHGFId1QdL8HEmt8ZiXBZzhwYPQpfURzVg\nxKIYrAmgEOoA3mDJKDhibC0LEQ6O5nix5CZjMpgQUI7KBV48djhksZhRliWZzVeeS26UxitYpWrK\nru2NCEObUU/nWOOQ8JD77/2E8uAB9tbzBI35P5oQN2NSDTFUXCMQooLiMEk71nbFLX0QNXFv0g3k\n5LoGudL/jn13+lzf8k8XIZdEs9AVhO9Y5RC6jMuDIiM0kcxalPNoy9rN6t4ml+RFaQSbzIZ1NF8/\ndltZ68dtdR/2vg8hRlLuH+zy1ltv8b2//g5vv/0216/s8PzN61RVGbfka2MajFlh79sy2r/jam37\nPpwKcr1artSvrWPTxSYsd1zvdgELfZfocmMiMWCsICa+78/QreR5Tp47XGYJIcZhhBAY5FELlaCE\nxseAs6RlZNlqEFWrMfU1p2V29GXbLxYLjo6Ountpozm3PStYDs2L9kZ81nIpNAsAcRanDhdgaDO8\nRFfVYn5ECA3PPXeTI32As8rh/h6DIkt+e10JVmptZbAbAbXVCM5Vxw2Df1NRZwGJlfpsOCYmhwnE\nYEmHAovFgod7h+wdHPLuO+/z53/2p8wf3uOVm1f45te+yijPwVdIaOIU1ScoNYU/SyRNQ0zsEbWv\nBCZdQNGGlbshhJS3KZUXliDcukxFwUjcF6Q1b0JIzyRpc4onpD1Lg/NxNhYISYtBQCxpL9eA2OiW\nBTDGIeJj9i4rFJljMihgUbN/sMu0VkyeUzhLUwuz+YzaR9N2PS4iBI91FmdzpI731DQx+FyDIM5g\nVBDrmE5n3L37ET/zla8DDVkxSF6JuHuYSWabaJs5JRzrF48KHMeJ3s9mQ6y+XBKwSLOrpoEihiCR\ngZ/Pj6gJZKMM0cBiMeNwf5fDgz3yGzfSuoXoXvzUay2ycRZZJ0s3gswWc2eFdJUQPRqqab1F7NB1\nWVEuZmhdk2eGV154gas7Y3xdk0mILmfvu9yWIQTsckstxEraASC1e0pdZwGPXzGF+x6K1iuyDhaN\nrmohyXkRNYVEWrbn9xP/dImGEhHZspmti3S97VoQzYsMOwNRj7OGKztjsnzA/OMHGFWcdYRigKpS\nhIDNHAtfrbRzXsRE0G29jDExP4cqvlGMifyQYPE+cPfOPcqyxmJx+ZAQImiZEE0xI8stEM2GMb1K\ntB///WmQywEWPVU82q29jtg01NpgvImuKg1AwKfdvKNGcVLRaVgsKffHSo53GrpvIilPO6b/XR9o\n2iv5yGqiRjqVelhk0JRcHY/4wssvMHCWupzhHDHAqk1lJzGPZrwGIDFh7Yo5JcQZXWPCmW3ajqrG\nLNo9NJHeb915cjzjWAta7et6W0pvsKmutkdD5AXagKTxoMBfmVD7yEU1WIpCqALMyoo6KCI5zghN\nWo02n85W7qdNFtx03p5k7qgSZJlsKDqBAvfu3WOxWHBtcqVnMkuMJu55Q9oJ5KQ2PKtcBm2iL5cD\nLAAlMuaI4LUNrfWE4KnqBboIcYNNGxiPhowG+fIhG1Dt+7llBYDiqwDH1etHquMpbq32mLOaIdsY\n687+15jVyehxgHvuxnWuXRlRaMUwz8mMYTgYUM4OqI2SGUu707mYNZebWWpxfQmdJrDqMerb6sEI\nEno8kYmLyPogYEzb0Q1xC8SlliAaNwSKkaGJM9EYzh5pqgZN6n9r8iy5gLhNAdpwZTxKfNYhh9MF\nizpwfWcMGlee5i7DGZN2M/NYt3qvvqmivUM0aYyN5kfrNTLGENJ9hxDYe7DH4f4BL736ReZ1TVEU\n2BA1sU57O6On5KTxv8n8uCxyKcCiVVdb7kE7sKhRhaqqqLRmsVggzqagoOXsFtcJnMUUeXJ8bl/N\nbD+fdnz7emxmThLT6mdgDeJjKHTjK3xdsn+wx2y6z2uvfYGXXrjBYnaAdYIzElPmF5tJTWiDp5at\nZVrtQunOWSfx2kEDa6AiEi0SkRQ4JgktYlyF19aCj2SjmJg3NISGump67RQ9V6qCD5A7h5HVOri0\nx4hvKqwRJsMBTe27DYFmiwoTArkRXJbF+woeK46rk/Fq29YNYiXyFi6GuvuwvFcbw19jGkcfODo6\n4vDwEGst9eERw2Gxsj9rex9pC+hTnvvl0RYeRS4JWBjEWTKTcViXKQNzBiZQVYmgEo/iKcsZs+mU\nw/0D3M2SPM9TxwVYEpxdgt7eYIwfZfnbiXU6aWY43a11VuDYpGm0A7apy5TqXwgC0+mMe/fu8foP\nfsDh3i6L6T7jwWtcGRUMqbHEEO9NaxT65W7KnbHtHpZcw9J70W/L1j0Yv1sCUzwvkoitx0HVd/Ex\n1lqsG8RySHET3hMaEjcTZ+x2wyBjDHXajS3L4mYG3iuTUYH3E4zO2dMjHJ6yaqjqmJy3qWvqpkEG\nq/eW5446hKisOpvyczbgwZqMYOJq2aZumNc1Dx98wuHePoujGVmW0TQB8XGpvHPLXdj9GbJntf31\nbO7R1fNOO2ZTeRelnVwKsFAUnME30EjcobsJcVXgYDSkmjfUtWcyvsL+tObg4IAHDx4wuDWPG90Y\n0zNDYnhwy0dvsx231uUJ2IjbSMz135YEWPyuMC4mvBVLXXmOFvPoJvQ1i6NdvvGVL/ELX3uNwgRs\nCDhCtNORdP+90G2RuAco0kVs9moESCRT0+7osKrtdBm6JQVVtfa5CMHQAwxAlvko9g+nNE1cYt40\nDYqPwXZZBj5mqorlQ/DtKousO79uPOqXngxVJXcVRhxGYZRb8utXmQwGWGvZH025v3fAwWxOqJXC\nQm4dM1klOAtrMRJovFJVZQxG8yFFoEraxlFQIyx8zcH+Lj/8wd/wd37llxhduYaTNMlJr604G8/Q\nHrupD1y02XGR5V0KsICY/8BKBtUirogMHkdMctNX8eNS5MGKfz6qyEtb8KLdo61cNJD04w02XUND\niDa+s2SFcPXqVer5jJ2dHd778T7PXXEMM0dTlRQuhnj74JEsTxzA6qBOF+3U53494u/LAR+jO+nW\nh3Qbn/YWgyltBKakHc1JHEUbx9GwqErquk5gEQds45W6CYhf1sumSokKKm2IdssZrA5AYww25h8H\nhMzGpDejQY73w6hZVBVNU+JEwDhMWH2u5XxBgLjYLLWTFSW0fIVXghjGg4Im1NQIB3sP46ZF3hPS\n+pD1ZxY0nmd0s4ZxmQjLR5VLARZBo9/fuLhQp65rmrRkIbraYrr4qqoxZkgxGlIUw07dFqPRkKZ9\nGKtgoWG5kvJxSc6LlE1+9BWwSqDnfYxHGA0njEYjhEBVloxHEzIn+LqMmcOM4lD8Brzbpo726+AJ\nx4Ck82J0qyqTqdHbdb32EVz6wVhVvaBpGubzqA1Vddl5r6xt4oxchriDuXMMshxrJcWBQO0DRmKy\nmWWqv6WWIwjet7ubxfgPa5RB7hgOHHmeYRcLGu8RYrKbvjhr46I6DDNfI21gmLbrZgD1aKijx0Pg\nYG+XxWzK6MoEaSzGrg6fFrxOgoPWBOmbIecxLz4LuRRg4b1nXs3BxfUDPgSquo5BRFmGWIvTnMPF\ngsFgws7OFcaTKyvRgKoxD2Hr+dje7MuFSpdF1rmNPlmmGpdbqxB3CmsaptMpZTlnMhkRV4420fa2\njiJ3HIW4vkOTE0h72ksb97CuLqsq1prVRWKqpHVZK6SmpIC3FiyaHkjUdclisWCxWFDVMfqxqhZU\nVYVq6PgLgEKEzDny3KHqyUzUFwjKaJCDSgrW8r18GjF03ZiUEhCbnBqBzAqaO0bDgp3xkLKu8PMF\njXqsWe3q40GBR2hUqedxb5OAElRALWICTdokyUoE0sPpAQcHe1x77nm08WgPgNq5SE0EVFlTbdcn\nho7Uv+QA0ZfLARYhMFvMkTx1YmNRjQlaBinASERADc5l7OzscPXq1ch4bwz3Pn9dnsTD2+Ya3XTM\nurrtk4vTByWkvA1N0xBCQ5E7CIpzBudi7IAxIMESU9Kk3bLQ1dWjziTAoNuIJ14vdG7Otk5t+xo6\n5S1pfHEhVx08R/OGuqkoy5L5fM5sNk0gUbFYzKJ2opF3cM7F+AQgHw0iCdlA01RUCYwMymiQlpRL\njKsJYXnvuU3LAmg3YIrPPMsyAp48zxkOCybVOMZRlHUHet29pftzErNsibN4DQQPXsEEyI0DjUvh\ny6pkenAYiXVnlgv8UlxrHwBYA4uV2JkupmPZxuugfR65SCJzm1wKsAghUFUVRqJ9bq3twuDaFYNi\nEyKbmHNxOBxCFkO+g7wZkrEAACAASURBVKQQ3c69t9QsWqKsff9ZeK0f5SGuu1TxAecyxGvkcZyj\nKDIGKUWe4hkMBgwkYEzUPNYzVYUQVjSBLgApLdxqrxdS9qj2sxXpVrl2GlwwBGI6uzKt1ZnNElAs\nZhwdHTGdTjk6OozPTZQ8z+N6j44/MSv32jQxa7cViWs+0gKztDA11bWN3+i5UyWGpnvfLnOPA89o\nILMuleUwTX3sGcQkNxa1NqZCSFGmfbenSAwLF2Jy39lsymw+jfu6ZhlpHW2MB2HJv2Bkxdpd98ZF\nDTj+1ie1H1fTeNKAcSpYiMg/B/4R8LGq/kL67gbwR8CXgHeBf6KquxJr+l8D/xEwA/5zVf3upnJX\nJfqxJ9l1DEOEBWIaKn8EgLeCF4sOhvh8jNm5QjWw5M5SJ4Y/KOA9kgJtDNDtJSlKzDKUGvQEK6Sd\nETa6tbpkOssHcmLkRgtSPSJgfRe1qNanma53SQEqbzF5QdPUZM5hxVNYYZRnhGpBYYXJcAh+QYPG\nFaoYNCbATGkj4vdqNO7wqoozaRZEIeWbaHyyz0WxRBPEJLa/qit8CKiNy94XVcyKrcFzdHjAwV7J\nbNYCxSHz6RF1E4Hi+RduMBoNEFHEKHnuoidElFB56sU8JeNV1LkECFA2hkGWx8mAGlEX28cHaAAL\nIWXQcsZgEIqQYktdjh05vFcOj2ZRC1rjLHCWutG4F43NsMZGQhnP0BhKKZnPj+BQ2ZmM2LmecfXm\nFYaHtxnc+zFDN2B89RpNnjPLcmY2ow4ZGnKyIDQuAlgb3t7u8WoSCdxqdMtdxgSP4sSteKG6rtQC\n9krv6/221ueeBA9yFs3ivwf+G+B/7H33+8C/UdU/EJHfT5//GfAfAj+X/n4N+G/T66lSlnPsPGc4\nipF5VVXhRcBl3axWDIc899wLTCZXOhfXsjH6jdO6DDVpHEvtImonp6PvyWBxHMXXvRoXIWK0l5/B\nouIQ46LGIIZiMGRR1xSqWCs0PmCNENPCVYSGLjeITa5REYOGJs6IxmBIWwHY6KaMmaNCNHnavBFV\nTdXUYBxeA9P5jMPplL29PXZ3d3lwf0pdl6Axe9d4PObWiy9x7dpVbly7QpZZxMT9LkQE76O3orQL\nfNqn1aWZva7r+OxV8BhQj69TaD8xqfNsccRwuMznWfsmJjM2LiYlzpR6XmJMNE2cs1RHi9Vn2SiZ\ndTTBMz+akeUWK4bMOCxKnuU8Px7x/3P3JjG2ZWt+1+9bzW5OE93t82W+rl6VquqVDSVcSIwoiSlS\niQGSPbEFFsXAFhNGMDGSZYkBzQQJqRAIjEQZz2whCxAD5AkGIWRZgDByU1UvX+bLfJn33mhOs/fq\nGHxrn3MibsTtMvLVpZYUihMnztlnn7XX/tbX/L///1/4F/8ZPv7ed6FtGFLgxYuv+B//2/+ax8en\nPPjoI5ZPn3H8vR/QPnhI0x4RiyEGZTDfgdTy9YT7bdga0JeYgz6aa+d7h+fxNh7JfRmMNxqLUsrf\nFZHv33j6d4Dfro//K+B/Ro3F7wB/vejZ/T0RORGRZ6WUz1/3GTpJhZKCKnY3dbeL6hYPw8A2wkfP\nTjl9+IB+vqDtZ2+Vpjw0FN9k6AW5o2rB60ONfEsZ7W3cRd0xEpgJ9KMAoH62wFoP1hFToTGGUhvO\nbKV+izFqgrRmL5S7UyCDWHWdJWf1RMp+98spkYPezFOyMyUVSRYi4ziwujjn+YvnnJ+fs9qsaWzB\norH/YrHYkfD0jcc7VTAXtNIhZldcQGLGTI1lptZes+7I4zCQU6BpmtoP45Vgdwy1XLtHo6aUSEwG\n0e1Je7yGbG3bYq9eZePORass3hkkFbwXvDWEzZoURh4+/Jhf+egx1mYuLr7m4vnXXL58jqTMxeol\n49VzNquXSAunncMaR5So4kXsNW3kwKt4dQ28SpGn7sFN7IU+N1Gx7D3Tgwrg9YNPq+jV/73neN+c\nxZPJAJRSPheRx/X57wA/OXjdp/W51xsLEbzRTLwxQus8IkryGkMm58IQAvPlgsePnrJcHmNdw+R8\naehxeENmbsrY30c8ODVn3YoAvedQsZSCVAyAQftlcin4ruPk5ISm70AUfJalGgIRSs1VlJwpuVSm\nO1UZr/rr2iVpNQlX0r7SAFBSJsVIHMOus3Tickgpsr684OL8BZfnL9hsNkgpLDpHKY6u6zg6mrNY\nzJnPZ8rmvcNl6NXKMVPGqLIFohR2CuHeJZZwVghBFeecGFqnualAIErcJVhTybXqo/0zqkBW8y1W\ndrmtPkQaP96Y4EQIhVwTo0Kh8aqtK0Db9Tw7PaPNW1588SWr9Yr1y+eYcctxP8eGkXgxcmUS7aKh\nXy44alsav2CUQqrG4k61wYNxbQO65fWHuaab73mbdXRf474TnLfdMreerYj8LvC7AKfLOfP5vCpb\naw9BSZkhDNhG+wdKFmazOWcPH9HPFiokYyZDsTsmk6W+bTL/OCb4XcfhorDWKg+EMTt0YNd1HJ89\n4PTsIUWDCFJOhFgqYKoK/ZQ9yCqXoujNVLBSEBMx+IpjyLW6UpXBCuSUkNrVG6N6GSLCOgw8f/6c\n1eU5YdgiJeGdw+REN+tZLmcsFj3zeVcNxlzBTzkCWgYN48g4DIQw7m6CnPOuHOq8x1rLKq8gK/nM\nNow0jSI0S+0kNUUTvkbjKaBWbqpqmG08XSn0saUPoybED0auHkkuBecs3gikSBwis6bhe9/5iO9/\n9IzN5QuuvvqKJIkTB67tmXtLHjN4uFy/ZPWzT1k/esTx2SPadkYhEaacQV3+h17Fbs+v1am7qiGv\nS1gehsPw+rV9X4nP9zUWX0zhhYg8A76sz38KfHLwuo+Bz247QCnl94DfA/jBR4/KcjFjNQRKSdia\nmJti1Ri1dPro4RNm/by65/Ygu34b52DeZbhfyUa/wRjcd/37NqP1NsMZQ8zX25Kcb+lnC84ePGJM\nmTFESg5sS6RztnIsgJSkcTyij3OucOZEY4QuOrybkmm1DyNmRcLW1+aUdqS/GOHy/CWXFy+JYaRx\nFltxBV4iXWuZ9Z6udVgj5DQSBm36S6lSDcSROIzkGLFYxjwe9JFMvzXcyzmzDbVKYj1S5Quk8k40\nTaOw/lrpgb0AE6i30ImhT5k+jsxm18MQU+kBS9H8jmQNw7yxPDo94vsfPeVsMefypz/BSMZZoZ/N\n6JyFMFCyYBqhxELcrBieP2d7/oLF4hhvPTJ5/zfW0i7JfkcC8tXKyZvH2xiD+1jP72ss/jbwF4B/\nv/7+WwfP/2UR+RtoYvP8TfkK0ElprGM0usCnxTBmoBSGISDS8ujRo5r8jDRtT76RtXh1wu7QxnuP\nIXKQsOIX431MZcKcs+YcxGBFk4hd17PZbll3FjNuIA6M3upOHzPWaNLQGrAFSooKBx8C0RZK05Ka\nhLPaMj5JA6QcyElFjJXXNFWgFAybNeOwxTlD4xtCGDQ34sBWUFTXNcxmM9q2x3nPixcvWa/XjNth\nJ2xsndk1AE5zGbKKYY+Vo+Jqc0VKBesd/WzGfLHAWkvabuj6nrZtEVHNV2rlBswuH2KMwTfQREc7\ntrTt9TBEk8Ya+o7DtiJkZ5wezXn06AFHywVh3HKxWXF6vNQ+JWchjoQw0BihxERvPLFAWW+I55dw\nNuIWs+ufdUdowY31erPStl8Ht+N0ftEe8NuUTn8fTWY+FJFPgb+CGom/KSJ/Efgj4F+tL/87aNn0\nH6Gl03/tbU5CEGZ9TztfMpiOzeUVzlj6pmUzDHhj+e6PfpmzszN8N2Mb9Qbq+n4HMYapIrEHyej5\n6251m8V+l8nWi7uPH99l3Gb1p+cmpe9Dj2c6/pRHsNYyxoD3nrDdElNREFHKbLdbwuqCzfkLZk2D\ncwbTz+ibhr7raJ1FqW/BFb35wjiQ84ZYPK3zFBR92WGQXKCooQk5UWIibAdSiliBRduSStRyrBVS\nzMz7juXRnLZtqhap4mTGMfCTTz9jvV7vQFAphR1Mf3k0o5/1u/b3FMZrIVFIVbi48fi2oRihkQ7i\nPnyaoOARkAiCEgGLFFK9/t4auv46YW/bekiCSUIKI23b8ODhMd979hEfP3pM0xjG9Yr56TGm9eQw\nEkqmpETbdRADnWsQcQiGzYsLrp6/pDm9oO+WBNG2eYvZsYyXnHePd+uq/s4yQbleXZtvU+k4XEfX\nyvJyd5fxu463qYb8uTv+9S/d8toC/KV3PQlBMFhKEmIKtL7heHHMeow8XBwzFmHZd6zOLzhp5rS2\nQaxjOwQmYtnpZ8oMazfgjrv6RjgydVC+7kK8+vwkg3jbhXxXAzKdzzWq+Bs2xU5doOzdU+tbzs7O\n+OVf+VX+wd/971m3gosDcXPJBuWJ2OTCsF4rv2bJSIVPL2Y9D89OOTteqFGQrEnAVisOk3aoMQbB\nknMixkAMI5vNClMSrRdigpQGGjJNYzk5WuC8Y7PZ8Onnn/PyfEXKgm97nj77hOMHC1IqCmraBpqi\neI3V8FxJY0QNYuNVA6TvW2aLOW0qWOcYx5HPv/wCEcFZT+cVNi5WhZFVaFlAEtYo8lOFnA2zvkME\ntuG6F/r40QmpZBKFzdWK4/mM73/yCd998ghb8SPOwPzRA5Z9z6Lr2V5ckMcNFy+eA4KxDicNgmN7\ntWH9xdc0i5/THp3RHD9UY1/uZs6673FbSHOTseybjA8CwamVIeUnCONIEksMgfXVGtvOGFPhiy++\n4MsvPmd5+hBrGsJ2g+nnTAZhirtfLUPt0YKvyzi/z7iPi/C6Euy1WLeevxHN8i8WC1JKhCET45px\nvaaEgRQj6zGQxgFnrdYpcya0DSXMCZsVaXxA6y1t66sUoJDJ2BAQsZUNXCHP1gnOKWo2lxETCyUH\nfb1obsBguXx5yZdfveDyao1rZ5w+eMx8ecrV5YY//PQnnJ+fq7bJcsnly3NW6yv63tN4S1eBWvO+\n5+GDUxrnSSGSSjUiTcNR12kDYdPhRHMdY1TjRlYsQyyDgqvKXu3cWkvyXqHxB8ObQucajHf4nDk+\nWmrXahyJYaSI9tfkygVqrWVxfEQJHeM4cnl+ATnTN4626WnGAjEStxvSMOzRs+ldPVEtnU4h6O2j\n7H5fL51O8L5pHU1KZX+CjIUxllk7Q0LGl0gGnCgwa9xs6eZzTNuTg7rCfeuxNrO9Bop6tbx0/2Pv\nkeg4oHx73ce+wh9Rny7X9ULu+v8EoJqMoTWGo6MjfGMxZPKgCcnJa3IkQhhJsVLiCZAtJUVKFobN\nJXG0DIMlxkAuShPXWa8lVWt3alvT7tQ0jnEYoSQoKl5kjeAMSDGkkBnWA1IcR8tTnGn5+usX/OTT\nz/n0s58RY6Tve548ecbnn3/B5dUGQ8FIi5+1HC8XdG1DjJkXX32t/R1dh/MNzihgipQZt1vGslWx\nqWospChsvfcNzqpnISlSjGjDsoGuvV5K74yh6xrafk4rcLxcsOg81gph1F6UQMYlTxi3XMbI0UyT\n6918wToMhHVgUbEcPmVKiuQ4kOJW5+46FuuNa1OrfvnWTeJtwFfTmpkM1X3fDx+EsdCmJou1gjPK\njwBQUuLl8xf0MWK7yHp1xbDZIMZTxFBMU9+/t6Z33XyH4cJd0/emyb157OkivimPcVuC6q1BWVNF\nZ0KdVmCYc065JmrvxfLhGa3Vun4YNlxdXpCDdoHGMSAkhu0Vy9kJjVcJhRRHri4HKIEyn/PowUNK\nkcrOfcgXAjHW3FCKOAOu0aqHUPjZTz7n5199jevmPHv8FPEdn372M/7oJ5/x4PFHPH360Y6w6LOf\nfsnLF1f0fcev/dqPefTglAenJ3StZ7NZ8fL511yev+QH3/s+8/mcTGEcI7Fk0hiIKeO8epEGUZo+\naqJyHClOvQkrWm7OtcKxvFE67b2h95628fT+mNZ5rCjuMuVACFu8t7gEtjjyOJCs8qv42hsSXVIJ\nC+ewMhBTpMQBS7ydM6SSftyxd9y5bt7GSNy8B6b187ZG5m3GB2EsSlHwlYrpGu18zPuYfhgGbLGc\nnp6ynM8wjadg2aTClLu5K5egHT1ah3+bRNHrxl0X7z4uxmTEbhqSyaOQCteekrXOOZ0Xl1n2DY9P\nj1l0npIjLkXG7UDKgTgGhs2K1WrFdr0ijANx2NL1DdYZpf4X5ZUoAuMwYoyhbR0ibU0+Rr0hbCKN\nUqsjqXaabrh48ZJ5v6BbHPP1z5/zk8+/ZExwevqQP/NnfosXF+f84R/+ITEmQPjVX/1Vnj59yvee\nPeTs7FSlFsOIpdA9djw5e8hiNsMaQ0hJb/yi1AU272n7UtImslIiOSZsAcmCqexpiFSmMaF11z2L\n4/kC1zRKjmM9jbOKOwmRUsWJAEyK9NbivSWOg2JZGodvG/WkzMSWFaFEKKocN13T2zaG2557283j\ndcW9m57q4ef/ceIs7nnsbwCXlczEe0/XtFxuRnLJzNo5D05PaJqGISVFU+IppdKf7QCAr16gmyWp\n9725Xwd6uY9x23GMMeSQsdbUxjAlaFksFrtKkHMd866nayzDJqoAz2Kmc2ogpTNWlxes1lesLi4Z\nw7aquelnpHEkDloG3Qzrndiw9xa8omNLF3HrzGiUGzSUyHq95vLlC7xtaJxnc7Xiq69U5d53c61o\n9T2Xqw2nJw9YzI9AMk+fPsU5x9nZMbO+Y9is1WMshXk3o2k847BhzAOu8czaDmoTW0yZ7MYDzdtK\nulO0RLzrtjVG+SlyRol5ruMsjhZzrFfPdLXZ4LqWzjekFGkah2kM6/WKUgqNM/S+4+cXP8c3Da5x\nzGYzLp9fgGSKqaQ9kpGSkBJeuZYqZXC9qnbfIfPrPOv7GB+EsZBSaIxlUyC6Fmc7lovE6iowDudE\n73Dzhu7BMUOOOGmJY6Fb6m4x1azvvmWnkmoNReDAdpRrv6e/bjab6cV3r1Rdrv/cPuwB9Fymx7uP\nNdXQ7ashU/UDtHOytUIpI0R9SRYwztOfPkJW5xhpGIcNbSw8mLes1wPLtsGVTN9YciwMxVLmC8rj\nGZ9//TXb7cgQkoYXYyAOgUV/xnx+ius6IpZcDH3X0iAqaoQhJMHaFpMKm3Xkq5+fc/bR9xijcH5+\ngW86fvSDh3jfknPkn/6D/5Wub3jSdzTHWnURVpRQeORnGDJdm2iSYTNEUoqEbIhiafqevu8xjdWS\n6pirGFFDKSPOdRhXqfhbfW8xEIw2lik5TkZyZiPXPYvFrEemRrKxgM1gFRMrJVFKxsTMF+uvKVJo\ni2HZz7BkmqIhcuO9cnimzCoVCg2Nn7Gh3YlAmyrDkA561s0uDwGQoFS5h3L7JnYYSphy4IVMxwHS\n1DMiBykyc3/JfPhAjEUpmfV6zVVMxKbHWE9rDSeLueICvOXpD3+Zh4+eEIKlJAu+7FS09RiHycY3\nWNYDpN+BS7J77+24iIP7+w4P4HXfb3ecV7qJ7miHE0HTl0WBUdNp1l0shMjp2QOuwhaMYQyJbBzr\nbcTPjlmeneIFwmbNenzJ1SZxcXFBJHJ69ojTxz3b7Zb1SlmsUozEmDg7e1DZqquH5hwiA9Y5coGm\na5Uw+HKglMLx2SnkjBHDL/3wB5yenmHEkbMyaJ89OKZp3I6zZBxHxu2WzWbDZ5/+URWwdiyOlpwe\nH1ME1tsNs76vPS6BcTPsMBVGhCGoYtmEqylSW8CNIcdESCNSBZl3OqoHpD4AzgrOCb54QtdADJSk\nTGOeHmJg5hzP1y/5+udfc9T1nCyW+uac6LoW5wyxZDbjQMjat9Muj2lmS9Y3wtTXxQ9vHYLc8tp9\naHz7sXaqc/cwPghjoTj9itUPgRITeRxq5aNhvljw6NEjQkyELBjTqIgE8U537tCIvG/Y8er7Kont\n7vMmTMf0+K7jvC8oJt9KAqi5HEc7m7Pxjlip85uTI06Xc4LAVy8uCMPIenVJHLYM2w0XF8pe9dX5\nmtlshneNJgONJxvRHcnITulLil6bYizONbuwp+QM1tD1Pb51PDh7RsmQkjCs11jr6PuexazDS9bk\nYA5YoPMCY2EbI92spZ/PwRr1WoaNqq4Zi/bXO4zRRKo1evw9CEnIkolFGcNFBCMWrJCnpHC9JEq2\ndYN7NSflyEAp/TEGR6F1DjGCJM+4WXO0WFJSwotVQh5r96hVa4kpKXepb5gfHbE8PqHp5qzeduN6\nw7iZG7v5+NCI3IazuM+A5IMwFlOG34qyNo+DJuFMLjSNZ3l6ypMnz1iPiVSUzbmUfahwDUNxY7zJ\nUNz2//vMIOtnlFsf10+7612vnEOukUpGMM5xcnzGxc8+JeUBjCMX4fJqxRcvr7h48Zz1es2wXdNY\ng3eOcRTa9ojnL77m5cv1rvXbVU7M0wePsF5h1I115KLJvpSVbi+hMg1WZNeYVVLYqXdBxjvllGi8\ngzgybgPDZsN6c1X7PCwpFsIwsJHMZhjUYDWetpsplqLriJWoN9S+oGnuRHS3LBOOIO/DQGP0eVM9\nDOXsLJWF+0ZrQC6QIrlk9UAmcehS8NbivCMNW0wxzGZz4nbDxcUF8/kcTMHi8K1je5UoJSLtnP7k\nhG55gvHdLWvg7VDDN3NtdxmBw8d3eh33jAP7IIxFYa+YJUa9irjdUqylRMO8n3F0dKSaEq4li6mB\n2eFCenMDzusMwFvVs2+54LtY8rVhyH1dtamqow1fZw8f8PkfdISrLSkLP3/+gpdffcEXzy948uQJ\n/fKY2eJYsRO5YLcbkMxsfsS4VSxASgoF77qOrleBYQGct4wjpKiU+SFmjPWITVhrKpem0a7Uzciw\nHfX8vGii9Aq8M8x6Zb8qKRKGgVIbwdpZz3qMuMbQznrmi6NKXOMw1pPGDYlCThNvRdrNY8n2Wtg4\nufiHZewJDl6Klk/TDWV4I9rSLvV6W9FO5xgjXdcyn88gJi6//Llq08TE1eU5XdeRs63Yk4ZUVgQp\ntH1PuzjGtj1Z3MG5Tb9fXQO7dVoO/5Y7vYU3GYld5aO+Jd/zpvdhGIsCY9TsthWvLmhVe8oF2ran\naXsG5yjGERLKz3jADwnvVpe+z/FmsM31UOYtj7pLxF77XoBgiHnk5PQBx8enPL88Z7UduFid8+Ln\nX/Px93+EMYbNZqN5ifWa85fPWV1e0TnHrPGkODLvWpw1RK8EMW3bA4aUM8TMMEZSnOj7dfdXGSAg\nF6xrsN7ReIs1G4YhsNlsGIYBUqZrPduNlhatTAbVEEJgG0fmDx7TdR1N1yLWk2ufbKYg1tNZD6Yy\niE9ExSUqd0fRxLG12vmiIdJBxcOo9zW1oU/9Q7vZreeiYaQye8dcMDFTxNI0HWVesOIga3Oj837H\npVFKIaTCmDO0nma5pD8+QtqWgH1lTU7jbaogN2/+u/5/zWBUB0vN0rez9j8IY4Hs2aJ922ivAAVr\nLL7vaboW49rK3aAgncZ7zLcwKbdd5Ps0PN/UkE3Zb8Ewny84OdVQZHV5DkNgNltgRPjpT3/Ky5cv\nq/eQuLq8ZLO64uUwcjbrkZJpHpwgjd+d02YIqjouiqoFtAXNOJrO4jaN4hByrGrqhca3zJfH9N3I\n5eWK7WqNiCUOqtkRUqZxVq9d0grBfD5nNluwePAY4xotieYE1oHzFATjlP1LpNQ8iVYsDA0lJ6aO\nYhEhVahkSrUsCZqYzVoBC+O40zGdhjGOUoQsRb9jLsQMDmVSD0UzRqUI2zHgrcG1HUWMatpkYQiR\nMRda39AdHeGXx0jTwy2eRSm3eaS3r4/pOr/PWrkL13Ef48MwFihPsmsdhUzfdgybEec9USxtM8e3\n3a7bsumU4t7KvnR6iNC8C/Syf/72uPDuSsi+VHUf4zZD9LrdppQMk2pWbTxLKTKbzXj48CFfL5dc\nrs6x3rPebvjyp5/zs88+4+HjB/zSL/0SbdvyT//xP+Ef/8N/yDAGful73+X0+Ajr9Fjr7YqPnj7l\n4eNHIFY5N0dNVHoRAhO/jCVXtK1vLYZ6I4vBtx1HxrNYLJFc2baGNdvN1U5IqG1b2qanbZXIRlxL\nLEo0YHyHa7VtPZbMGCNSaqOdcTivxDYpJbBlB4sWk7HG7ryfmJPmWCqHxtQWn2+k+kLWtvZcCmJd\nLb9bYgZjW1IWXlxdghHaXs/XNZ4YR4pkQlCi32JG1jHiFkva5ZLoGoagxEy3gwQPH796zSchptvW\nzM1Q99qaP3jd4Vq+63jvMz4IY5FyYjVuiDlgR4O3DUdHJ2widPMjTh48JRYHpiGkiLeNUtSbQ5d+\nuumu/3398e1x422hzG3jdbnl16JDr9Ek3UxWXf99beSDc2SK23XBOOdghAcPHvDk0WNkfcHVVyty\niJx/+VNO+oajxrK5+IoghkeLhie/+es8PDrhQW1CywLPL87pOscPf/h9+tlc43wBEUfJmky11mMN\nnJ6ekZZLJBdyjuSkUgApGoY4EIaAFGgbpdgzi54z94hSFEVrjAPjsNYhWJJXVTlTQ62JRzSntONh\nhUwukVQKKar3EKq6mRLf5F3Lu3EWCVkrM5VyD+cx1u08pd3UilIRJnGEmLCNU43YXNjkTLzacDVE\nnn78CaXedOM4so2ZRizGL7VhrLMsnn3C4skzNsaxDYlIc8vG83Zegpjra6Hksvu73AxhaxVOjJJI\nFnVfrlVBpkrdfdBJfxDGAhGMtUAixEjJhq5rIUXmyxNmx8eVXk4JR5S7Mb6/e3UD27C7GBXc9brz\nvHu837m83qO4w6UsCde0lGHvFZUUEaBpPd//7idgCuIMedyCtyxnHUfdEY+Pj9leXBHGqO57znSz\nHrG+Qsu11JtzIiaF3Wu+wam7LoYiBecaZRtnxNbrUir9vULDTQWjRYpR0aNiHUY8uQoOOms0B4B6\nCtOKjjlrZXz6vrnsWK12wKRa4Si5Mn2hqmGl7qTOWLLRc00pKVP8wbDWVoCcKtTbqc3AaFNizJFc\nBNe0pKJ+SdN4inF4Y3F+xhg2iO3o5keYfk42FozF4ojvuTbv8ohvWxuvG6+0DbzX2VwfH4axALLy\nmjGEQDai6DPXeoFQPQAAIABJREFUcHL6gK5fsDW+1tIFS6m28v3dq3cNBb7ROOwculXB/K5ROatF\nmDIV1Kx+yhEnqEAOhbZt8cfHuOWCM2foljOKRMRU8hfnaAATM8SIITOGhGsanjx5pgZD9nDpGJWW\nn1yw1tacUianXIl/CyUrPsYUbWZrfYOtzNqmqEGPpeDFYeoOn3GUXNnbc7rWIQl7UoGcc2UFzzua\nYSNKDpOM2TGOT262/hSMIrS0lFu5MnIprzCsa5lUsEXh3CobUdRQxEDKhZhGxqQVGtf1yqLOGgmF\nmIWrdcSdHTM7OsU2LbF6KyJCTvmGd3E3HuJdx20hzZTgLPLtQMnhAzEWBWVFKqZmpEVIBbp+zvGD\nR1qyq3T4xgil7k3vO+WTa3b97282yW/yEN5/KFO5iOzwKKUU4hjoahxrreVoMUcawZFZ5sL8aEYs\no/JvSiYPA+vNGl8Ea5QgN6WE8Z7Z0ZIkBsl5p0Oq95recFmkqtQLFKMU+lnPIcSRNGzIWRds67xy\nXFSv3zYWaxzWe8R4baDPVd2MfbPgVO6UrExZzkpNYKsHYUrGVkZvi5BCooS4Y5+yzqoinYiGICmp\nEnquMgY3t9baM5KKNo0l1GOVmmw3ZG2Nbzy+n+HbhjEkNkNk9fIKk4UxZI6ann62xDQtY/1sK3eD\nqa7/fdfv62vntlLqnRCAG+mQ+zQaH4SxABjTSGObyo3oEOuYHx2zODrRrHS9AIruSwgRRBuBbpvA\n95mkb8uzuA15d9d403e4llspkGPClEzXdSAJE0daKwybLUNYE9MAacTEEcYR23RYWnIYlBKg73BN\nR67GYzp2qvwYJRdiUUlEawwpGcKopLupEupenb9gsx4gK/PWYjZnNpvRNA6L2xHtirUqZGy0ooAZ\nallZSVomESIjRaUTC5ATOQZyChV1qVKCKWqnKegGoodRrg8pohqnMSkJTqHmTfYjF+X2oGi3rwHE\n2ZoUjBijcHU/63DesxpGVqsNV+sN69WGPCSSaQi5sA2RNiZoVegp5cgkPv2u43DD+iabjEx29h7H\nB2MsAIyzGKftv2KtCtX0PWNBY95SEFuQVPY9Hbzq2r2Lh/AubuGbkKDfdNx+/FeFn0Fj7lw5Jkop\ndG2L5EBIgc16wzatiWlLyiNCpDeWWdcRx0iIiaurDe3xCW3b77yJifNTb5g98U7JIFYwYgg5E0Ig\njqOyhZNprGNgwxijar9UbyDGBrFGGbcq+7YYVxVCZYefmEIfay3OCmD3BMMpkqP+lKTJzlHRYnv5\nANFPDCFi/d6YCtrtqY3xr861UGqVSXMt3lgSSdXYjPaPiAibYcvzF+esNwMGQ9fPKSax2gRW6y3P\nX7zArVbMZkdVA2Wk6fobSfNvZgDe1oh8WyEIfEDGQozDimN+1EMzIywXHP3oh6TTI6Jt2BbV8Uwp\nkUwGY7D59gm8rRR61wS/y8V77xDljse3Hf/V88nYIpSii7qgokK5ZLIRsjMkaxnHDQ7NHTTe0uVO\nex5KQFIijgMhBIaYMV3P7PgM2/S0yyUiBms8JWk7fGOdkhBJopiMSCGGSKRgTUPbC0W2e7bu5UNO\nZifksCUNW9ZXl1y+fEHOmeOzB3T9XFGa8zl9s8U7BTnF6r3kSeLcCBiDsbAdh8pXkUhRSXx2c5Mi\nY9LGMoslZZAipJyJB3onmJr/iAm7vZ6z8E1Hqs1yJSeoORbVlIXGOVrfcLnSvE0nLW7miQjN0yNe\nvrhk+5Ov+fT/+Sd8f37Kd3/zmGIbGuvJQ8GVVCPdKYlbHxtzsM9N0M1St4SAxe6/5w5oVQF6lTzn\ntjVUZPdydux692w4Phhj0XilQzPeMaTI3HvOHj9CrIOKpzBFtLVKUCLabyshec/jdWHIezW4ZQVE\n7VnBFf2ax5FSEp6C99oQVZIgRIrXhePbhohhtjjG9z3Wd4hYyPs6/vRbJNeEjuYuQgoax1dNUSkQ\nJWIlaXLU9kQjhPWadb3Rr67WrNZb7EvLcrnk+PiYvmsYraWfz/QGj1FxNs5gnCVnNMzIWbtBY9Rq\nS9Hv7LsWgtkppemmoGVHW6srUJCy3yhenfeklZ06/5qUVcPimkZ1SUTRrK1v6JqOkCLRGNrFghgz\nVxd/wE8+/UNmn3yi2FNriQm8da+EPVNDoKHmg2T//G3r4SZ26HD9vO2aue/744MwFiJKQhtCQDAk\nMbSzOf3yiLUW/ZnkCLU7Qva8EAfHeJcmnQ91XP8ee8JhLfeag7R3zbbn2gcxjtiYKEZj9VLda3Xv\nPb5R4JGxlk1IGn64FmMbLXFWV/mwOjElHUENyLBJFJvx1qnat69lyok0OWtOaWpOS0k/vyCMY+D8\n/IIYAn3f0nUd1td8RinkHMjidlBqUtAkZFaxo1LZ0ygFsVaNobDTPN15E3XGpt6VlDSsyDf4SXJK\nFHs9sDfGqBpbLtrslra0zRxhUocD6xVzYmoYlUJku1ozjiNzhDFnTDHKWaEXbp9snDwEqpbIjeU4\nJSfvAnPdvkZe/d+7vP5dxodhLFCexG0OOOexxtMdH6ubXSyJfYrCKDROd4E77v37mpx3Ge+bz3it\nARNByvVVJaXCmQu1i6Im8Gp3pjOGrvN4azCiTVzealmxFJUMOpn3JHEMY0CypZ8ZGtsQcq5VkMo+\nlRQgpbmB6h5nBZlZUb4NxGKLVkwm0mBjHEYcMW7pO60khBAYtwOXl5eE7YY0nysbWtdRJGvVpUQl\n6s6xeg36vHoW6kUUkZ1RyOw9hlxzKUBtuzc4a7EiJO+rdurhyDhxZAMko2VT0Qa9lBLb2lfz4Ex5\nP0pbEKOo1WGzZbW65OR4yXe+8x1ms444bncGNsZI423t06jXTNRAXAuG7mmJTqX129b924AN33Z8\nEMaiUHYLwLYtre9Znpwy5kwyBS1kgSlK8WbKoY3+kzFu9gRMo8h+xzk0kKZm/Q/h6tYpV2RrDN6r\noTBK8QDoLmwAjKdEFV+yRpOD4xgQt090ppRUvrAKLEtR99rUPMCUTygpY2whhMi43ZKDVikUfu0p\nlQGqbXta37DZrhk3W16+fAnWMJt1Oy9AqyaTpzQccG0G9Q6ql0Prd2I9Ik6rZyKEoGESB5WdUgpS\nSiX02Y9ScxQFi7EQxoydxI4QtoMatqPlCSlGvLEYD2TLZrVms16znM347ne+gz89rbkPVaW/Dbw3\nGXm9hrfn0fRa3r4uDl9z2/OHhuKutoVvOj4IY0EpDMNAMY5iHdk3NMslmscX0oFhMMUgJWs8at8e\n6HKY9PxFex1vM16b4b7FXTXWgFKRVp0PrSZ478kp4dpmx7FZqgdCMXjXcrUetLHKOEVioru1dwqm\nSmXf6j1xR8Y06ueghiSOYVeJMSkQhg3jEMgxKBt3LljjlOAWW3tD5vR9z5U9V8GhyyuGYaNJ2aah\ncWaHuYhhJJdYkZvKRD59nmv3HBzGqGL6ZDSsVQ+oVJ3WnLWSYpr22hzmHCkJMklh6HVtTMj8EALn\n5+ecnZ0hQN+0pBjI2w3nV1eE7YA1M46PFnTHRzTeklKglIbGN8jOhzi4idkbDL3mN64z08YwvUd/\nH3rQd63fnbZq2ec3p/L6fY0PwlgIQggJaTwJS8gFaTuKMaQCqQq+SJkmYEoLpdce95XP+RZzFu97\n7Lt2ilKKusho2fHmAlGqOGEnGFzBTVIRnda7ipcIdUcWRCaaPipXRGIMgR40oQeVVIa6kmu+Qgop\nK5oy5VwbzYZdwnG1XpFj2BHlhhQZtiMhF57OlywWC/0eoo2AzluOjhZ89sXPSFcbWufJfU9uXTVI\nUEokh7g7h5IyxETJ2vVqxAFm5/bnA9bvw/mc5mUcr2udUipE3tiDRKk/kACEq6sVV6sLWuMYvWc7\nDgw5crHeULJh3s6xbcN8MWPe9QwpU0SxHoeI0Wu7Pu9+/74u5L45vs01/kEYiwyI90g343yIfPTx\nj/CzIwYcxXtIur0ZMiXVRYD2ksD1G27aEW+6Yh+iNwF3X9xDo1iK7hWT+4qoa58x9LMZ7WzO2jmk\ncVjf0vQw5klbI2Os26EKQwiICLO2q8CoafmWXet35w3SNJA92+2acRtIKTAMI9bAdrvl66+/5uLF\nS0IY2KyvoCTSGBjHEe9bzh4+5NGzJ5h2xjapshlGgeJIRmh4+tEztpWPc6iCQapunvHOUHaGTiHm\n3hqwDdsw1DDH1lZzNRqtbw/QoPr9rXG4xlZMyH4Yo3NaKlMWGC3JjiOtb3Zeyh/8k39MDpH5rGO5\nXGKcBedpfUferhhDIX79cy4vXjBfLPHeqypA/Zw7vYCDsUt6HoSWBwtB/1efe53RuI9K2+vGB2Es\nSqmcAgizo1M++cEPaXoNQ/SmN5rswu5j+Bs1p19MePG+oKxyx+M3fJqYg4U17fgGJFcFNjBVcb6I\nslmNTWJMBWdEAVIGDdvQtv7GWC5X57teDdd0lBwJuZDHgVKSqpM1LZhCTpH1ZqWdnnEkx8TLly/5\n4osvWK+vcMbUxGbCGGG2XLBYHvPoyTNOHz5iiIWQEjPfYryBHEm5eoq+obcWsVYThKkya6NhQEkT\npb4aC+drqFXLnKSsiLGdXmyVDMx7rlT1dgTJ4fr1yprsRRTFmdJIFjUqUTTB6q0D7wmVvk9KxlqP\ncYKVhLNCMlm1U85f0D/9jrb056BqaO92ufW7HoSAhyvm2wRbve34IIyFiBAzbK/W/PBPPeHR42fQ\ntKzW24ro21tjkaL18dcc60P1It5/7A2F5i+MttKVgjMG2yox0DZEmmBZ5YFF1+GMYMXi0HIqKWCM\nZ3X+EmM9/WJJ17QYIsM4QGXQFrQ5TETbwVMa2KwuKSVx/uKlJiclcnyyxDmnhDil0Pc9y+Uxs8UR\nXT8H5xU7kSPiHZHaOVqh41dXV0pP17VaOh82xDBgvWfcrBR4B7sbf/ppfQNimQSXBLUZ2D1uYudZ\nmirBeGNGU0p6Q9fdZ1IhN5adZ9I0Df28wx2h82y1Q1UqiMoZpeBfxZHVxblC0iWTQ0JaxQ1RpQBe\n8SZKeT1C78bVPyy5vkvwfZ/3wwdhLBAhpMzFduDRk+/gmhlDzIxjxPi8872MEZ2oUunlboQZh0nM\nb+c0f8GWvWY2s2hiV8cEnBJKsRjf4LtWiVmGoKnMWgmwItisLmwOgRwCm7AmbQaKi0TnCM4iFDbb\nDSmo4PAKdqXWMAwMwwaAy8tzzs9fkktiuVwyn88rliErr2Ylt3FNi1gteYt3OPGkHEk5Qu37SFJI\nIoSa6+haj7UL4ujIKWCjo6TqHaCeg3PK3ylNA5hK2K04jmK0D1lxIpUFi1Rh64Wb5qKUghM7aSpX\nLwRN+GatEnVdhzGBWdvjrTAOGy3bi6kVlC3JWVIYuLw8J4SBFgV8FakgwynPJuzqpje9hmndasXv\nOhjrj9ubOBwfhLEopRBSZnF8wsPHTyhiyAhN0+luWkt+u11kCkP+pDkQtwzNkJsdXBhqjkdqHCtC\n0/a4piOOAzFDK+xYuUPOpJgpYaSMkbBdU7Zb1jFwdXHJ5aynW84JY6JpXZUrVAWvqYSaUmC73fL5\nzz7DGMPRYk7b+mpQEk3b1YpGh3hHsUbZp4yAWDJJ5QVKojGWZDJhHHFtQ9gWNqOqjs/6FuuEYaX5\nhlQqUtcJ3jqaxmnIVfVNI1Tvwij3hNg6N5GCAq/UcKo3en1MngpV92QCsFntaraWrm9JOai3kSOl\nJKyzWKceT5Ki7wOG7ZoctJ9l6qmZrtXhJ09Vjt1Z3MBB7KoacK0E+02Mxn0ZnA/CWORSaPuOH/76\nj1keHbNOmViE2awnhFgtbyKRtG4vdgdymcafzPAD7qItyTmTi2KcnXNY7xgpDGGkI5HCgMminZoh\nUIZAGgdFRG63bLdrQoExag/GEIOCozjc+WrvRkmsVitWq0uWyyViDWMcGKPe5J31KI1VpG0cGLuX\nDahaK7F2f1qjDVwF8I3uvmkMhFwNSk4gQqrgKDE1F2GVk1NzAWY3Mwm99t5op3JIiZIsIhkjjiKV\nm+IGKGsyhECVMVCot5TK5YlKaDrayk6uvB7OW5QXNNC4GVEKSG2uS2OtEEFkX75Ug7+nvrvNY8gH\nscbNtfyuzY5/4hvJFstjfu3Xfk3LXJuRdUgsm312W3M/k1XWZOdt40+W0TCvPi7q0gr7mNw4uyOo\nGXMiSSJbS86CqSXHNAbSMCpuIWca5+m8w83mOG8Zxg2bcdAek6zNY9MuO1HKdZ0CqFLS6oiptP4T\nLX7TtQqQMoYkgqnXSEQQ4yhxVBV0UZCUsZ5u5qDrSMPAZrOBrN7H9D7rHd67HXWedovWqSgFyVBE\nVeaNMTAByWo+Q78DjOP22szuEKLsQVKZyYhUbg3RpLsC02LVWpaafIXiEykoNZ/qrsbdedcH7+z9\nXiuzFmq16u3Gt73uPwhjEYAHP/4xsx/+iJ+tI83REb3A+WpQohYyIkp6kjFIsa/0Hd/WdHNb5+n9\nW94bWNJblF3eV+xlB+wRUHDzXmmrdQ5JQo6J+eyY2fEZn376R7TWsAxrrtZrFo1XqcPNFTlsKGlg\nu12TZcR2Dd3RktnilGw8tEvmxnCxOufy8pztsCankRRUKd07x3K5UCQjlsY21VBY5s2Cru1xvsNN\n4CcR1QAxBimJIJYkiq6c5j+lLWMKkCpozKpuzJC2JDuSS0KcwXWOxvl6WGHY7nk3RMDkTLGJMAYN\nuVIipriTtyyYXRv+NFwRYkjkDL5pELFgTaUPzBQyMSeWRo1hNg0pB/UwpAFr2BjLCpXe7PvICQYz\nrJHG15JuUZ6vguZMKu5bIyKzu7RF8i6qjuXwmnMNubUrm75hPd2GNbmP8UEYCxAePnlKTAXvO5Uy\nnMpe7DkW9pNUeF9htvt30Uq94Pd82IPj71fObVSCRtmcfEMpyoydKAwh0KDw4TCOEJQ8Rl/fIk2n\nsguNx9iGzlqysRiviMjt2hHjqAZj3kMuzKpamYEdTFwotZzp8Fa5N43zilQ0lhBGxCpUPI4DlKL9\nPxQGIqU4Sg4YY7HOUap8gFZ+knYZF5U3NOgaGFPcAcKaysw99bOUHDWpmQ/Lj6/WD7KiuFRkiJqU\nzDrHh5vK5NEA5DJ5GYLUkq+xgsRce1ryrkdF2g7QJrnDUqiIso7tsW81AXeX5u07jJuQ7/sebzQW\nIvJfAP8y8GUp5Tfqc/8e8G8AP68v+3dLKX+n/u/fAf4ieoX+rVLK//CmzzDW8uDxE6Vnt5bNECi2\nJptyJSmZLmApO/KUt52Pm6Ctb2Pct1ScjnLttwKa9HFKWekfAFvxEs63UBLGenIeGGPECsQwYEJE\nKlVc13ik7XC+IYn25mSrvJXaer6gaxwlR8hpR0Rjcq20VF5OW0l+Z23HrO20MtM0iPOEpAJF2jGq\noWMYFLJNygRgmCDkYim5aoYWDVFs42EEEUsxsoO0A9hRz8MYzWVgjJYUpzWiAYV25Nb5mkKEaRx2\n00INecSQyaRU8xm1GW2CmZcs5FRRrgVWm4EkLcXayi9qcN7T2oahyA6qDhNrloooyQ3DoPgKuZdF\n9MeN4Pwvgf8E+Os3nv+PSyn/weETIvLrwJ8Ffgx8BPxPIvIr5WZz/43RNC3HDx6x2ioprDFKEpJS\nwhptSD9UPtdL+nbjti68/7/kNKZp24dSMvmtylNpDCR1s5uup+lnhO2GUmHfpRTSlMgrSklXSsHa\nHtN0iLMKp8/Kfm2ltnZXjEYMA2nYEmPGGnQXN3aXq5hQjs6KCvlst+q1GMc2jIQx8vLyZTUuwna7\nRSroq5TE5foSY1V0aDnr8U7P23lPEwJDmUI8owzhzmERrNdn1WBVI5LK9YQ3sDcYvILgLBOuwhit\nchiPUa0hUg6kVOrac6SoUgRFTGWZtxTbEJOwTYliHP1iiXEKQc9V3Gi6dqZMurV3JCCLYmbKN3cu\ndp95/fC/IJxFKeXvisj33/J4vwP8jVLKAPxTEflHwD8P/C+ve1PXz8jiWQ1XyMxgsiGXia49X4vR\nVD7P/QIqp3/89e1JHVxdKLvDC5Qa/oooSChXqrd+ccwwDORM3a3jjp9BhyHmSGe9Utw7T0Yolfe0\ncZ4SgkYrWXDFYm2D81qmnbd7jtTDnT6EqI1h242CparieCqFF+fnpByxCOO41VJkTRp+8fxLRApP\nnz3hex9/wvHxUm/eXEDsHr2LQdRMkKmgKYoS+U+GVFQY2SaYOuhsUjkDc8tCmbyFqRyfRYl/p/uq\nSN6VkcMkiyiGYi1iW8Q60pDZjAm/bDk9e0jTNIwxMGSwneZuTDF1KemKncBf38b4tjEZ3yRn8ZdF\n5M8D/zvwb5dSXgDfAf7ewWs+rc+9MkTkd4HfBfjo40+42o61Vl77OwDvtclHyf8nnRDZ4S6+PWPx\nLpN+czu4v4VwrSIvefe3lClE0zi6GKFfLFgen/DixQtWQ2BmIIaEpBEpsTZn1SpE2+K8J1ujYk0C\nYpXqLuWsbnXR/IOjwZYeV0SV0Q9i+hgU8bnNI+srrabkSpYbQlDS3JJJIbAZR7ablSq7DxtijKzG\nNW3b0vc9z2fPyTnStR5vLNYUzVUYq8lHo3IAOWeGoMQ4xhhcKdd4IqZQVcWjkzKRl0K6AfdWT0ON\nlq3eRUpSQ1yqdyvErGGJGAPWkpJUgJdnG7c0swXPPvkezz76GOsaili8N7vFud/Vb+QSDli87ntf\n+rYS+u9rLP5T4K+iM/BXgf8Q+Ne5/Wvfek+XUn4P+D2A3/jNf66MWegWS1bDgG86LJBDVJbnG4cQ\n4H3pi98uDLn92FMScx9a3u433tzJ3rZj8ObYZ7MnpM4EZy41UaYlQsTQ9Eva+RLjWjYh0okQcsKk\npHkA0RvB+wbjHNmodF+MiWwSYnUX7boO8Q1lHJVqPwaIARGU+j8ExrBVPs9xVKOAI4asN7MYtpuR\n9XZLypnVdrMTLY5prMLJG0opPHr0hJPTI47mM0qG9XoLuSCtR5z2dJjKCi5iKRUDMSUnpy7T6XrG\nMSjzdw23Yoxa5qznfusclz0WIudcGcYmg6DtJxHBGg/GM8ZRE5xGmJ2c8vjjH/CjH/9pjh4947Lm\nOkrtN9Fz28PNRQSsqWHhjfOov7+NDfCP1ViUUr44OJH/DPjv6p+fAp8cvPRj4LM3HS+VQraemDPe\niBLNot63TB2RVJezFBDdPczBDT0ZlDcpSAvC67zAkl8lDplsS1J0TX12KnHtDcbeSBx+wCGy/5bz\neYPxKugOYXdR+HROhVwE4xvGnOic4/F3vs/leuT55qekMODalhBGvK1SPdaRrGE2n7PNhW0YKUUI\nY8QaCBhW24Ht1Yr1+SXDZktrHSZX/ophW0OhXG9ExRYUPBeXK+0TEUNMmcViQREVGi6VYapv5zx5\n8oSTkyPm8zknZ6e0raeUxHq1YrW60kpCSdB6Ssn0fY9zCrYCCDES4qC9LjnR2ol4M6so0KidqpMX\ngtEb9yZeIdY+GGMMDjWsvuko1jDGRDYGSYkcCyE7LJ6YDNvSMEbDyfED/tyf/fMMOMYirEJAjAcM\nX/7sZ/z+3/xb/PjHP+a3fuu3ePL4KeMYCCFSiqgae6kqbOwxM69dBxWte9f/Dlsd3hfQ9abxXsZC\nRJ6VUj6vf/4rwP9ZH/9t4L8Rkf8ITXD+MvC/vcURSUWTS66iNWHiKdy7hlN8SVX6vg2YddvkvGpZ\n87Xny2HdcyKpPUio3mmYbzUU0zeq4w0O0OsupFSiYntgbKZq0JStB0VHRoS26zl7+IiLP5wTw8jc\ntxS/gWIQo1n9XNKuF0JqUrMTS2MKYRj54rPP2W42xCFCTIxiSDHQGMv64oqYRsgKo45JjYU1vYY4\n3UyrKdZxdHKMMY4vnn+JcxbjLG3rmR/NOTo60qazowU5JgVqOUvXNeQ4lRcnTRANuVIFW6WU1NtB\nczopBVKoydwQFQuRZd9AJmbn2RyOnNOOKtBaC8aBdZWJW/E8BsM2RlIWTBaGUjB+xscff8IPf/nX\nMP2SuNoi1mllSgp92zHvFvyj//cfMmy29G1H+1stR8sTAELac26wa18o6JqsZdQPdLxN6fT3gd8G\nHorIp8BfAX5bRP5Z9Nv9AfBvApRS/i8R+ZvA/40iXv/Smyohh0P5B6o4bt5j7A1mB+udtnlTuKYn\nebPG/LpYbTolfbk5eL5Ud7doeatc/58akdd7CnBoKL6p+7fHmezOgcnA1WayrMCfmA1iPf3imH62\nZHv1kmIdeIvBY6wlRyGmojgFY3eJTysFQuDy4oqL8/Oqy4GCm4L2cUhODFdXhGGLc45Z12jPSoLl\nrFc0ZtdpiJMz4zZQJPDg9IzZYoafiG28oesa7S0pKvaTahLSGaP8JeQKz1bcRCGpFGINZaYk6WRM\nc1ZGrVi1RQoGI8qdIahHuKPkm66R7NXcoGqixsiYEkPKiNVC5xBUwd6JYX58yg9/5Vf55Ie/wumj\npzy/XCGmoe1mmLxBciKEwHZ9RUqJn/70J/z9v/9/cHp6ym/86T9V9WHV4O37UqrnvPMK9ue3v+a8\n8vjmd/lFjLephvy5W57+z1/z+r8G/LV3OouD2NyZKpVXocwR0NvB7NB/pSat3tYK3/Q2rhuVyZZN\n+YDpb+0v2CFDKeyn6waRyq7mNfFP3Gwfer8hE5/j7vjXF0vKNUyrDFgYg287+uURV59nsjM0vsFb\n0bJkrOFE0U7elJL2YlAYhi3nLzZsNhutZNSu3+16Q0qF1cU5p/Mlxrcsj5Y8fvgAbyxXqws6N9Ob\nrwKkEItrVWDo+OwY33qsNwqXdvL/sfcusZZd553f71tr7b3P474fdevBYvEhirL1aidIp4Eg6I6B\nAJ0M0rOMe9TjAAmQHhhIBhlkFnsUwLM4MZDECAKkEUcty261RLValkhLsihSJEsUySrWu+q+zmPv\n9crgW3ufc0uU1bbYdsHoDVwUee65955z9lrf+h7/B3Vd42qLlVxGkYZk9HRVQpfFd6oB6r0vE54w\nENy8b6lMdQujAAAgAElEQVRsjRNDPyvp+xT9fdVPS9R9vXdUX79nveanLcpYCXyMJDFUTaON2pAJ\nGMbjDfYuX+bFV17li//e3yXamjaAVELGsmg7YhaqquHJw4e89867xM6TQ+Sjjz7i7bff5sq1q1w6\nvDwcgv118exJiDwjOMlPuJ6NV1bQeDaDZHXQjhmy6ELIrNCbf5n66xdlFzLUDEUgtTBb1xKXC6XH\nX/w301P//WmmkUYDRRG8MUghj/U9i2IHUFSfyBlXj9ja3uduFtoYGTlLbS21FSREOiDEjHOKwrSS\nsTnRtkvOTk65c/tjYtKpSUI4PTmn6zqsGPaPLjOqHbtbmxzu70HK1E9q/CKoMXOlI9mqrmkaNRSe\n+4UGJQu2Vn0LTAFVFW9RFRU2WFGdDmPUADukwHI5Z7A1LKPM0HlMLTohyllHzE+hJGMp1WJOZVx5\n8ZMV20sQWrXHTLH0E2rcaMyi87Ttgu39y7z40ktcufECO4eXGW3t8vjknDaIasYmNfMej8cs2iUf\nfvghb/7oR4gBV1nOjk946603uX79Otvb2wP0POde1S0WwtmnuGz+LV3PRLAYUADGkMsicgIxq5Tc\noIic0T7BOvjmF6RrvyhQ5JwLNn99dJXWSoaEiB0mEL360l83kEukOIH3DdJsymuTFUdGVOJfsiXl\nhDGWqhJ2D/YxTgVkpbK4SlGDAuAsPkUcTvUhssHlwHmKtPMFT548wYeo9nu24nwxJyW4cumIncN9\nxq6msUZr7xDJxjKdNkwmE5yrsZXDFrXtLKKq2gaq2lHVBfiE9h9C6vsISlxT4Rm9D7ZYB/YygM6Z\ngcTmQ0dlVaEr4bVEzblPtYZMMsZISHE1QfmEK+dM13X4rKbcuQu0aUnbeVxd8fkvfYkXX36F3cNL\nnLUdp+cz5l2gmW6xXAT1mPWOGDNvv/02f/b667z77rvUdY2IYktu3brFG298j72DfV548eXy+lL5\nykPG/EkF7rMEInwmggWoH4Ria2KBxKrQjUWUjp6USry+/X9RMPhFj138t39euRnSb0joM4T+psLP\nT0h+MZb/YnbxV0Hwrv7WxZ7F+u+9AOzpT1VRBur29g5V1RDbRX9sDw1CYwxdjIScsDEhqBVi17bU\nleXw8JDz2ZxlGZNW9YiNjQ2293aZTDepjUBR8HYC49GE7fFUtUcAjGBL3yIJVLZCrGCdKaVEATwV\nBbSUVo1HbWxmcgqDPYB6iIBIhYiqkIcQyHWxDMg6YRhEjQecjn4/JrVA6Onon3jHUtLXLGoS1C2X\nZGt57so1fv0LX2A83aBqxlTZFHmEgLEO71vGI/Bd4OGjB7zx+vd59913OT8/J4gqm1dVxbLzvPXW\nWxweXebGCy/9PA6iX1//5u29v5HrmQgWJmWqxRKMIViLv9Cf0PTalAhsijirIp+DdvpFSxWN0ma4\nAeSSjBSnboY0Na+BYrQf0nekjVi9eSmX0S30VOZegqYPNZG8qosRjDhc1SgiMPVfmcb8/Iy/n0b0\n47tU+BbAsKld1Fo7liCRJQ3ajsbEVWB0SbkRoq9pNN3g6MXP8fG7b/GkCxix1GKpK9h0iTQ/h7gk\nAidlhLgcjxjvev7O9c8TvIHocDQ01YhJM4EcGVtthloyzmiD1Yqhy5ox2MpQj2tsbQlZewuWPCiM\nW6yWJ0V2ILqAJBXmjb6ctCmoi3mGceVISZuLJlviMkCn3JKen1I7JbflUDQ1kxBDRxe88lOSojhz\nenojJnxsiWKI1QQP0EyY7l3iMzc+w42XX+XyczdwoxFnyyV+HqmqKW0bMNmQvGdrUuG7BX/2/e/y\nla98hXdvvjdQ/ANLjHG4pmFaWeanx/zR//f/0lSOL3/5N3j++ecBoe06jGsAwdpGy6G1cymWsjOt\nHZNPDfb/Uurfv8r1TAQL3Xql/kxlM1vVEliNlErKXZ4/TAaGUadceP7FDGPNQJlY0r1e6AUushJl\n+OqnJUMtnFS30bjeSlGISNHC1I+yWy60VLAVVhsig5jK6u/1p0pa6TSkFd+lpxZYuGC8oxWTkqNy\n38fIkFMueIwi5iIVm7uHVNOPWB4/YmGFauSIREVlOkPMER91YWcySGBjY4OD/UvEKORgMcniTEVT\nVTgjxOWcHDwxBnJc6UracaZuRjTjBtc47U+oWCXL2RIfO+WTWIMYpX+nXEhmOQ4oSpxDSOQgmKLK\nnEM/QgXvC6PTWaqmVl/UnFdTG+9JWfDFONnHsOaBcnE3dQkylmQtAUc12WLn8CrXXnyF5158hZ2D\nI8RYzudzhcLbatjEpqh/LxcLPvzwFq+//jo3b94cAn/XdWCLO3v5kJwxBB/41je+iXPqk3LlyjXt\n1ZmKGNWvVqwZMEV6EPZrZm0NcTFg/HUECnhmgoWO0PpxV18aDAArPd4VVbEmj6abw6xlFBcRfcMl\neQ3aldZGp5oZQH9zlJ69gpWvpiM5ryTUVFG6wKRyj/XTBlx2PTagzPdDh5jVx1zXdf92AGhbX/Q0\nV6/ZGKePhQA5Kr/AGEixqFqULGeYvPQ0a31/XXJMtveZ7hzy6PSUs85TVzWN0fPJNQZCwsdMjGoZ\nGElMx1sF06CsTcSQsieEpOzN2CIpDmA55zTQ1WOLtYG26zhfemIK2peg9CdCwNZWlauSIiC1n5BJ\nKSiK0pRmtjFkk6krW9TAugLGKzLFMVHXNc1kjDGGdtkROo9qbqqS+NJ3+BgGTU6xipm4sCSqETEL\nXcoEaTi6+jyvfO6LXL7+ItPtPUzV0PqoczhXsVwuCQnG4zHWKqv2gw8+4Bvf+AZvvvkmKQesFWJM\nNE2FT5mmrlVPpGsRYxmPax4+fMhrr32Dhw8f8vf//n/CjRs3iDEyn8+p65qE9qBWGKxPHtWXFfoL\ndtO/nesZCRZZT+3SHUeszuCJwyxdTBlLlhN59QH2pUEJLk/1NfQpPVKuZBSyqhfz4MYuGojKa8gp\nDR1qbZaqmnOfccTYS85pqWRcCVTRk7PBWJVXw0HrA6DiLScnJ4ORjf42e4GY1Y8HRSzWaNYQc9mg\nRgqSU7kKgsK9+yxJf6UBGTPdvcTB1ec5P3nM4vgeZ+2S3Ah1U1EXG0IfO+0JilBVFmvq4QR2zlGZ\nSilckstMypB9KJBqRSBahOOTM7xv9SQnqvvYuC7jU8FVUtSuDKZwPKwIqdOgmfNKf8JkQywB2iIY\n53RK0k/MrJBtpUEtZ5TYrqxNjE5BEvnCaFT/Xn1hSYgdsWg9XRZe+syv8fJnf53rn/ksbjRlGTNG\nIqaqkRTpQlKlcmDZecATYuKP//iP+d6fvcGDBw8Yj8caAEPQElIgB0/Qri3OCJW1xCpz/Ogxb/3o\nTXZ3d9na2mJ/7xBjDNPpmPNFV+5ln+H213o/raxL+av1xP6q1zMRLJTo06nxbO65IBe7wz1WU5dt\n33/I67+kPLY6pftNtHIG7+v83iF8NV7VDZxKw+0iP2BoOIq6x+RsSjDrEaXQLhSsZCRD9rRL5U90\nXctinoAbALz71pvlBevvruua8XjM9vb2IFtXVRWNc+SkADEfPQPIz5bTMpcA+dTIEMAnYTyasHN4\nhd1H97m/PGMeZtQpk4xgCvizN1SorCU3DZUpmzDmAqsPxJgIMeCM0M5OaM/V5zO0HSlovXyWHuvv\naWrGG1MmkzHiMhGFfWNWIjIiat6cUsKnMGyHlZ7E6m4bo+PUXtbPdx3Be6rC6NS030FtyG1QnEQP\n5is4nZT08xNbXVhzC59YtIlqc5vPffE32Lt8DTfeYuE9rY9UYrGSkKpS46GmwZqKxWJBVdU8uX+f\n1779r1gul/Ru7VpGayfLVbWSIFOiHo8w1hE6j7WOnOHhw/t88+v/EmsrfvM3f5PRaMRstlCsin4i\nv3zj0AeMT3tk/8nXMxEsQNNRGzOpN4UR9ciUrNKnKiZrSvZQat4196Zf9C9wURmcTEyaHfREpL7h\nBgyL7ZMmLSFoQzWjc/KUVHwq58z8/JymaZhMa+aLMx4+vM/9B3c5OTnh+NE5qhUEr/2Lrw6vL6Wk\nPzPe4ODggO3tbXZ2dtjf32dnZ4coKl3XjOu1cXDJSkwmDdDhYcuVcV2gHhnG29vsHh1xdnyP7rjF\n54T3EZMizmg/wpQmcWUqqqrBOEtuPW23ZJESoV2SfUe7nOPn56TWk4I2K52xYB2H+4caKMZjqlGj\ngcFK0eZsVu83JMSKakbENDBBLT3RymhilPVkljIub9sFZ6enzOdzQghsHmzpGDXovTLZFBXyAsAq\nk/AYtLmpzdWLDc4ugZtucuX6i2wdHCHNmEXMZFszaiyVq4tHCPiYmS1amgaq0Yjj42P+1Xf+NWdn\nZ6UsKexoYfBatUZlBfUAXCmNG4waVItw584dvv/69zg6OuILX/gCTT0ua1NNmIY1nP9ijvVfV4bx\nTAQL7TkEEAhxicVirEXMCqabsyqT9KleSuCHkkTh3xcChRRMgrUg7kJjMQXHQEojljLFYKzF4Mgp\n4qyDLMMiBBCrNf58NqOuKupqhEmZ+XzO8uyYD2/e5c/+7DucnDymXZwpLNkIjff0wcIcf8CQuYiQ\nzhInMfLgHWVwprTKNkYHL3B0dMSXvvx32NrZZv/SIY2bMFvOqeoRVdUMpKlspPh/go2dYiXqmuc+\n+zmMzdz8YcvxozuYVrixpaxeWxkkWOadZzzZoKob6rrGtx3LpadbLghdh+/mJO/Z2dlkczShqUZY\nsUgSwFBtqBhOKmWQsVpaqSu7BrLYozGXetqCUKFlAugItetUdxN0spNCR46J5XLO8fExKSX2d7b1\n50LGRCnZW1Az5hBLv0WRrzkVp3hbMX+KdTrdOeQzX/gyL776eUbbe3QpY6tKpxMhU49qchfI1lE3\nDVXd0LYt/+cf/O/84R9+hePjY5p6PEw/VraJaWhq1nWtk6bFAmMt0+kmZ7MFiYyrGo7293jv3Z9w\n83fe49e/8Hl+67d+C7Hw4NEMgMl0SgiRLqu1gSl6Gz1WSA+0RI5agj99yH3a+IxnIlhAeeNFUWl4\nv6VW1/8OhJhWc/YQaVN6Kvla1XOSC6RX4vAuc+6bg5WOZtFTzoj0gmeo2I4MykqpMA9BI3jwnsl4\nRFXVxM7TLhek0HLzJ29x/95tHt56n0zEWRg5bZhuu9Wr3K01QKnKk6Gux6Skm6ltW+bzOd4vYbnk\n5L6jPXlE7M554cZLzM9O2NrZZTLdUL5DDNiqwVhHzJnQK2HFQLaJmIWmURWy3YPLPD47ZXb+hDCq\nCxktI9Fic6+erq7jxqiWCKnGkLBmRLOxwfZkTFPVWCw56rxOEZBCSEX6zqnUnlovmiHQxiiQHa7w\nMPQmBXrv01wCuynZUS6Cu92yZblUSntVldFrhNninBDS8PdT1MyvqkckDMuuJWZomjFZhO4ppt+L\nr7zCS599ha2DPUW6jicqz1fMhbrlkvl8TjPZxjnL6ekp3/nOd/jGN15jPp9jrS3BPZc+l5RehSmf\ngxuywb4n1Xu5Nm6kIjnLBbW12Lrmzse3+PrX/4R//z/4jxg3qyAjxlAXw6bB+FlyMU8SDPr7Q9Dv\n9XiSPmj0JdKnwR95JoLF0xFRyshxnXKbUsKHltj5coIoW3H1oZSfBSgNLlXDduUkA3KRL1tLSfvx\nlpgefGXAJHqzXW1i9vLyevIjAYcKwZw8vMtHH7zPzbf+nMXshLEJTEYNdWOpnPIhDtxqoV7arIcF\n5JxDxA66FHEkdOPVzX0YKnKKPPzwp0g75+Gdjzi6+hwvv/o5WDZM9y+RYyRmEOuATIpQGx3DhhRB\nGnb2L3P5yg26h4948vAx89mCSkQ/B2MH+DGSiNFjrW7KITNLjYrSWIcRowraScuGylrAkrJQ2QZX\npPf6uxGLl0Y/hsYa7MB1sWUitbYOUBHczmumNV8u8K36kzRNU1CckeT1RE2iE60QIoil9ZEs+t9i\nLD6Xgbq9uNRf/eIX2Tm8RHI1tmiG1lZNjFJQbdDpdEozsbz//h0ePHjA7/7u7zKZbGgTE1GhoNL8\nVmSt0bFx0eT03gMy2CWkqA1hZwxdUmvIqmoYj0acnZ7yrde+ycbGDq+++iqT6bT0MHQ0H1M5YMRc\n2BP96Lnqae9PZRM5r/A7v+r1TAQLWEXCHs6cRS58GCklom/xwRfPh0xKOss2auhQTiX9sKwVnNGu\ndiYOHy4pQyrNLslDo89kZaH0IDpVWoIUMykEUoSOFmcsXWhpz485efiYm+/8hA9uvkNaztisDZeO\nLlNZsEazClcZtnM3vM/DDS0dhJV+ZCynYiSTGt181lo27RZJ4PHJGd2Te8weP2B5+gS/nLN1cMhn\nd3bwXmhTph6NMXVdFof2ZLyH82XHdj1ic/cSuwdXmN+9w2x2TFNemzSqtJWLgVMCjHXUonDlfpPZ\n4gfapYhkAWOpXYWtGno/z2ysfmU1ujaiAjvqn9qTt9DMpIzG0xr1fCg5EZZdy7LoZYiIwsUrxSOE\nTkfmiUgMAR8SbQxkqTT7JKuArkCKCbEG31sUlGvn4JAAhBRw9ZTlbIHYqpRhashcWcfpWcvt27f5\nvd/7PUIItO2C6XhC27ZDfwJKGUyfoQXc+vfMCg0spQdjReUQU/DE0NHUjlsf/oyvf/1POD095saN\nG4ymG+zs7Kgzewe+8z+HqejX/2KxWO2fsq4+bfm+ZyJYDEhJ1oJGjgNuol9MXbcs/Yq+sVdpw69s\neB0vlmAhaiyScyZ0C3o8ho43W/0bRpGeuXAXKQEKIEUzIDBjjKQIzgIx0M7P+ej9n3LznbdZnDxm\nUjuOtneZVpbtiSXHADlQO7Xcc+2qPNqsqyH4FR44tTFIUxGjGQKhCDirp3K9OWIZAl2yLOcn/OQH\nb3Bw9Ro7+0eMtrbJrqHDQCjppk2IbUgG5m3EZY9ppmzvH9Jdusr5zfuY2pCtMvaCAUxFjajPZ2VL\nMKsQscP41KRCrc4GK47aOpyr6WKnoLKU8OX+iFgljZXPWd+TNmiTqC5oTgWyXZp/fRllsinTj9Kc\nFMHVCveeLRdIPyUKibYLdCEQxRAkqrWBCF3wxKjkNWMMm4dXLqy5NmeMc5iqofOeyWRCKOVgXY9o\n25Yudbz55k/4/f/tf+XB/fsFM9Fp9jNM2PJKBWuQHITO+7K2hRiUHCdlffVN9Kq2dF3H7Fz//vz8\njB98/w1u3/qQa9eu8dLLr/DZX/scV68+x2S6SVVbYigEuUFDFEiJ6XS6BgdYffVZx6dxPRPBom/a\nKMYok81qvKllgJrqhtANmwkA6+jl9fRkBEqq12td9ICeHgYOkEsPwljBGgdZU0S9iqCMLxiGXNCV\nMWONyuPf+tlN3r/5E84f32N3Y8K1w10mjaMm0eSg47NkFMAUA26tlzIptey6mIu1YIwliahCUynL\nQlYdy8ZUTK0l1yNmXeLe8RmP79zinbf+nGsvvMLW/iEpRDAOV9W0LlNvbVLZCu8jPmdqcUyme2zu\n7nMGhBwUYo0hVAZjI+NsBuj7qjgwmuUh6vdhtbsvGZIo1gEV0L/Q2zEmIbkCkwt0XpG2uYDJEnEI\nMOuN6f6z6bqO1ncqgmPWSlHfYamJWYM8RkjG4kMi1aqkJa7C1A0pRXyGUVXz2S9+6cKai8YhVYXY\niugj1cjikjZMkzFsbo6Yzz1/+Id/yM2bN3Xk6yo9rGJgMpmQY88Y7T1uVlyertgd5qzq6SkVtG5W\nwlxKEcFSVRowYvQIidh1vP/ee9y+fYvbt2/z0Ucf8cLLL/Hrn/8yW1tbGKfN47quB2RvjJHFYnGh\nnO9fU13XQ9bxq17PRLAgZx2DFdSjzqyUXBFz0TEo4rApJcRknb+XVMsU8JIiM8vGTAC99F4ZsfY+\nDj0QS2yBcq24IX2w8EEJSvQj0pRoT2e07YKPPvgphJYXnr/CweaUkc00BmxqcUEl6nLMxByIIeBY\nRfamvMcslpwcbqSkq7ZtlXG7VnPW4qmNYdZ5MobUaeC4srfLg5MZtz/4gLYL7F+bMd3eY1QIT03t\nsNq5VeOeBAFVg0pZF1tKS3y3VGk+V5GNSuA76xTOXbgtxKJKncHVFSatxImsAFZBVrrRg5aDovck\npYCYqKeBUHpF/ekbhxJkvc7uM7v5sqPrOoyoQXKMcQBnhQxZlHyYjSGJyufZqma5bBEJTKbbWIGY\nEgeHh1x/5dULS87UjQL/U0ScJfiEKYjUEBJdgFsf3+XNH/45JsNkPOb8fMZoNKKe6Pi0rusLqb6W\nG5ZSYJSlXVZY7hXFo077ynuxZUNTxuDWCaNxDRkePLjPyekp77//Prdu3+Hq1ecYj8dMNze0n9Lo\n9Go6nVI146EPZowM3ie+0Oc/jcnIMxEsUs600WstKxYphjAphXLSeELwIAnrBGuL96WpVtE0rZnB\nlMAACszSNkYkpdJFJmAsSA5IEiSWCUiyhJDJyRZ9yQ5XCz4seXT8iPMPbnF6csILu3tcunEDlxMS\nIyNjaGfnZJ9IywCFYu9wKteWVx+zy7aUPbkcRIpeNTEMSNMB85EXEGFHRuSQCeGcjIOm4frOhLeP\n73HrBz/lvbdG7Fy7ztELL7O9f8D5aJvJ9pjsRoyriq0WRucdfPiQ+t07VHdPCaNAtWU59x7TChuT\nKclOMGJJMagieMykKBg3Iotlnh0mK4vVmEJotYJ49XcxGGzJ5CQrb0X3jClIVCFEFOkZMzYVP1Gj\nfI6e6drDn8OyZVw5msmUBofxYFJNGy2L0DHrZnjJpKqino5YpIyvRkjV0LktLj93g8/+2hd47vqL\nzJuLTb4sFa7wPUyZ5KSg486zszl/8Af/N//PP/tnSO2IS8A5JltqVdD6jnqihEENgplQmIN9v6Dq\n4f6K/FsF1H4ykRS6LtZisvJausWCPFakqYiA9yTpOH18zrf+5KeAwdqKLDqSVm6coaoabAWTyYTD\nw0Oef/55rl59jr3DA27cuEHqKiaTya+8T5+JYAFrDcWcy/w+DdDnvkaDi5OTFVBJMRP993qU5nCl\ndeCSnnApqRJ1CAUqnUTHhlKVWb8KprTLBY+PH3D/wV3aRw9oXMX2ljp2mRAJvmO5mEPwpOAVXCaZ\nbFYpqXnqY76AulzDh/ycolfhx4WsqXuISXVwomM5n7GztUnrKj4+n3H/49t0KbOYz1luHXH1yhUa\nV2ES2DbhuhYWHTw5w3/8iMmlCaOtCUYM8wSpy7TZU9dFuzIpdV0yELQTLzGq/m1SHEtOQhaD6TM2\nWQkVCSrEbJABYNQbHg1fvhCnRKnnXdcpByN25BipqoqqUkxMiBmJmmUuomJspKppKkd0QhKDqWqs\nMWzu7nPjpc/w/Iuvsnfp8mCuvH4N66hkH5ISzjnOZgu++93v8tprr/H48WOaUT2MSft7BwUVvDat\nW7/SWnn19ISif74rQUWxJ6pOVlWVcmP6kqv8DudcsYu09Kpp2lNSsmUsCN/j5RNOnhzz4c8+YDKZ\nsLWzzauv/hqXr11ld2f/l+7BX3Y9I8FC+xKCvTATXg8UPdpvfcSacuh/fBDMld6cxqwxU0264PZk\niAoaTwFf2JM568Fuje5QyZEcPafHj7l76yMePX7IJXHs72wzrhx+uYTgoeuI3RIT1OqPwuLMrMmn\nPd3BLrGs32DQA24uLq5YNl4K+vkkI4UP0jf+EtsbY1oy988XPLlzm8XZKU+2zti8dpVR3WDFYRcd\n1WxGPj6meXyG/eAhbrHJqB7jNwxp7PAzT2xakmuwxXrPYjFoSWUJ2hCO/euMZdQqENVEKIlOYSxK\n209D3a7IyhACbZlmpZwhdhANvU5Fu1yyXCzoumVpzNWDO7xPOtUiZdokdCmSK4utKu2piME1E/b3\nDrn+wmd44aXPsnN4hBhH6EfAa9fweQPWVoVF2vHee+/x9a9/nbfeeouqQL2dc8P0w5Qgo2tsbdqw\nvi7TSj/j6YNAyvelTJlCCLSdh1xMjcrakAySEgklxFnj8KnT9Q1ks2YFAYSs+yYbYRk6ZmdnPLh3\nj4f3H9A0Y7a2tn7pLvxl1zMRLHTkpKc9sBYsFBIci5r0OgFL636/tsFWylEGuXDzgAsbUUws2YX2\nRIRiZCOC9y1S3Lxm5yfc+/g2xw8eYElcvnKZvZ1tsve05+cKm04JkyLJB3XzjqGUF4b+ZZg1LO5/\n+t/85eRJP71rH/gs8F8A/wO8C3zrb+ilPBNXmXxJf0LD7du3+fa3v82PfvQjUkpsbW1xcnLMuBnR\nLlrE9mLOnwxw6gPCL8I7aL9MR50xJ3LQAKoAvVoDRVS6vjVW9UlR4adMIrYd1tUYa8jB6yGSMtWo\nUcTz0GgVAoEQI2HZEpYty/n5r/yJPRPBQsOFX40wS0My5Yvdcp2QpOH7uXSDJRVvCmMLO1I3aQ8F\nX93cHoqro76QPEmSGvxSshKTcdZwfjrn8f17PLxzB6Ln0uEhO5MRjQgkrwK5KSIxIiGVCUi8kERo\n8PrroPj8u+sve/Wb3jpHjIm7H9/je9/7Hm+88QbHx8dqttSjMkvjsM+SnsYv9BnsBRDh0FxfXQo6\nkyGr8QWl2n8vpYRTaJdOeRhAPxBTEQFKWo5lwQhYZ6lEICcdRQehL7orEWxORLJaO/yK1zOxjrWJ\n2ZGSJ6VISpEYg3bli1bhiim6qntDXJJjS8odxhSvS6eNNzFaVxvpgVqroJPCkhCWRN+VEqfwKwRM\nZTAWnjx6wIM7H5Palt3pJpf39hjXlRoghYgjawMwaJBYlUGyKoVENICJ8I3/6b/7G/t8/9118Xo0\nawc+h3OO2WzGn/7pn/KNb3yDO7duMx6PS6PzrEyO0oAe7YPGelmznlGsuEyfrCqOcRcz55xofUfb\nthd+VwqKUo1dJActlWtXQcrqrpYCzkDjLDmokjgxYXIxsa4sdWWJwZO8X6GYf4XrmcgsMhnfn/LF\nryOlIgg71Hw6m4Z1wlhXdDktGVvSCUvOK8BMysoG7IMPoBs+555Aos8tN1XBPp5bH37AkweP2Nva\n4lM6fyMAACAASURBVNrhEVujCY0kgu9YLFsIXsVpvHpWGNGmk0LGQZk++iVFfuObv/3fD39jXRNz\nMM/Jqs/QL7yQFJTlWwWF9UrVqjVlaCUTcdhmQt1MCMmwbDvunwrv3LtFaCzJgMmevQj7x0uuP1zw\npXfuUndzTvOMO/WMzS+/xOiFSyy3d1lWUyYHl/BVTWcsOEuIHSYlOH2ECYEqaqCUwqtJtabX/Smp\n5sZSrPy0FxRSIqReqLf8S5kIZP3XpEhlnTZmjSGJQtYjWfsDRa9CdvbJtsbUI/YPr3Dl+g2Orr7A\n7t4RMVsFcFWViuHkVIBZMvRB+tdojeImvv/97/Paa69x8+ZNUkpMNjcRUdf36VTRmk0zWvP7WGEZ\n+vv5dOmxnmVkGFihxhSx4xgHPoeIWhm4NabpCrjXEy3XcEOSkAJgi8mTclqhW6MGl1TIfNbV5BAx\nf1u4ITknQmxBHEVFrWw8MNaoC1cKxeR21WVOzMrp7RSHEJU8lWIGDDn2N00h29GrFmRtdUNWdQPV\nGGNrjFHOge8Ss8ePuXvrNlf39nn5uRvsTCfYZPCt2uuZKMQg+KWa2lQCrmkg5aIQpc2KLEqocn2z\nU2RgzfYNsH5ReO8vgJMAbFd0GaKaDQuGbIUkShqzxSA6dR3LNhCSok43LVxuMndPH3F3dspp8PzM\nOHbF8cAZtg4Mk5M5Ts64fMlycGXB5Rc8j8YL/uyDD7n14QeY7UtUW4cajESw0ZPmx1Qx67gPJYnF\nmGmXXjETcbVJlEymql89QrMNvkjflY3mMk7UPlDh4IUrI4Yg6lLnsZi6wjUjotHfv33pGjdeeplr\nz7/MZGtXJQOkIckIW9f6OQc9CKq6IvcBHD3567omlRHuvXv3+O3f/m1iUsEfgNAqPH88HlMZi4+J\ndr4YMoqceuGjFcK4x+KAckOqqrpwCPSfSY99WCzUA7anuHtfNDfKc5IIpqqGDKbrOg2mVYWry1qN\nHullGrQ6B1sIkiS6ENSCkUz4FLT3nolgAXrjskBTNeUGFD5HCR4mgxjVbVREXECsEnU08qotX+qF\ncn0ftS0GFXDJRIqnjUJ2xZJxpcNsVQw4LTk7m7Gzscnezi7TZoQVo6CaBDmqOS9Jf7cuICVAZRMh\nGZWg6yceKSN2Vdc+TSFeT137x/oAYkuZmVLBgaBYSR1R5mHBxqylVEg9uCxQ58A4J5oMJiS8RM4z\nPMFxPLakDrYqw+igYufAsrkdMHuW7RPD+cmCxdkTumixTmXkcuxoomphxOxVBTsJMWVSdMNm6XVD\nTNZFu1ws8WkVIEM5nY0xjOpKP1v6k1fKKBVChGDAFKPmZdapxXRrm8989tfZP7qMq0fEJNSTKdY0\ntJ4V4KnkX5o8Cj6sCHK9ifN8NueHP/xRmc7IwKuQMtWRlKkqxfLEmC7cnx6evnpsLZsQ9WQZZAhK\nBtkzT+OyKIGVvwWr8iOWr1weS/39N0bRsjFjiDpNktKfsFJAhBcPm/4g/VtFJMuZAcZti16lwNp8\nWzdfyopjSKlXRPKIGMQquxGspusxk0JpYFnF6oskbBFkcSUTEWtRGZdao3FSleknjx6zu73N/u4e\no6bCRE2To1e9hBxiYV0KVoTKuAIgK+BTGMZYiJANZdKTSEVn4wJGRFZNr76hm1LCFaBFTiU4oBiU\nZNZKAPoyhgJ8ygTrcUTGRtgQyzwbYkz4KMwlcWbBNDDZcIwORkz2KppNwRzU7Dze4kF7zvHJkuzP\nMBKJrsb5jsp2JBMJBNqMGhYhZL/iKzyNyFxv4IkIzlpcpaNKZ1fCRMo/KYQrY8lW/VmTrUhYcu3Y\n2DvgyrWrXH3+BcbTTXwSoiiQL1uLxKzGQcZgc8nKREVvyVp2SE9Ai5E333yTr371q3r6syJerZcX\nMWjJKrnftGv3K8S1+7fa9FKyg/XpXf89zQiC6oIWKn8Wg1hHLuVILIHTiCGhGZaxjlR8UAz97y1B\nATN4zOi66t+DrsG4jjn6Fa5nJFhkgo+YorcZYy5uVavIrNyQFhXfjSBar/XlSpAOsorc5EIA68k9\n1hk1ri1ZhRGr5rkiOFfjrCOEiF92nJ+ccn78hOvbW0zHDZBIUfUzkg/kPq0sLY+MIkxjVNyBtbpA\nVUvD6M1KWv70dWMq+g0KQPv5xlO/ebL0DLui/kQv2yYlQ1IXt5T6BZGwYmiTQs6byjKpHFPvCF0i\nF9LcKZmqqYlbwNaYPDHkBpbAaHuH6qQmzRNLPyG2hnn0bJhMVUeM9UST6EymM4ZkbKm/A977gejX\nb5EeVFTXqvql5DP9iqkr/R4znLwiloxQ1SOCcXQ5Y8ZTdvYPuHL9Oleeu0a2NSEbTN0gYtUbJHhc\nNRrusULBe7MjbVDOZjMwU0SEt956i6985au8/fbbQza03kvqs8BeNi/3mUMSbKWeJj07WkpQ6t/v\nOs6if6x/vP98FOm54gmtApSaQveBJqHgtlWWAZIzrtJGeoweSRnrqlVvo89M1pin6RPW2V/2ejaC\nBVqDV7n0H3KP8uuK8nUsBrmFpJU1YGjzMkPI+Kwy8n1JgSTVrkgCVIX1p3/NOa3tM6ZoNFjabsn5\n2Rm3b33I7OyU8eVDrNFZODnifaud5aR9AktpjsZQApRSqSniPVKQNYZicpPiStiF1UkbU7povdcD\nckTAlYUWUWHapN31lDPOVgqEKoHL5AHhXijkhSJfGyrvqIgED4hj1sJGU5M2auKkJtQNoXKczB1z\ntllWm3Rjw9yOWIaIBM+8XbKcPcaYJdkEgslEK0QRWDiIfUMuDRmEMYbGKQpzVKYJesO1USxrHq5i\nV0CnCARjyM4x2dji4Ogy+1eucXDpiJ29XVqf6BLUVht4PpWyzyoeIZVTWYwhSyLExGhUw0Louo57\n9+7xz//5H/Hd7363ZHRFg/WC2lU5+Yce2WrD9dDtEIIqlBkzHAR9E3q9Efpz+KCsPbX+/9efE9LK\nEiKW5ynwygw/o5m2llr64xFnRwzQgpwxKNy+n9Ll9KtnF89EsNAb4+iiyqMZ09dwfXkSV8CtHAq8\nNYME7QpLgqyRWlWvVqmkKR6mmpVpvR9JiKmw0lOdPednZzy6f5/bH92CrqWp3TCyTTkSosekXBiX\nZSSbNUMgRVxx2JLiu0nZ2Iho97+oPg91qklIlpUmQowrEWAp9aopuh0uIbF3rVKDH1LUUy1lJBcD\nhYw23wrZy9tEdBAbFc8NWfA5cZYjO1VFGo/Io4bgGrpccd5u82ixycNgeGJHLCbbLMlIveR89pi2\nfYLtPMSOJF51LnOmxikGoGQ7VREdroyhripVuCoCMJTTtW88r4+aEasaFMYSxTCaTrly/Xmu3HiR\nje09TK0ZhWvGYKxK4eWIuGrIIBKWlf2TEuByzrRtx/7+Fu+++zO+9rU/4cc//jE558LKVHSms3Yg\nyfXBQgY5pVU/IIeiKJ4Sg5a8XDQ8XoeB94/3fRuzNj6Flb5bKg23ISzoiaaK7rFs/t4KowQdZfRq\n36c/hEp7HC1LpGQ0f0syixgCXau+kT2s1lo1x9VmX1TQVoykrCY3msr1AK3SecQgEsjitJch2vAC\n5YNQAF0+eJxVB6iuXbKcex4/eMi9u3dpFzO2xhXG6gSGFEg54JNnWnwhUojqLSql0ZmjmuSg1Gul\n2QsSc+lzCiEnJPVCPayykKLpgEhJdVdCMMEqxD1qW7NsLoaNobDxQsFP6OdTBHtC1gZhrEuzMKrZ\ntM8GX1k6B6Guyc0EbyrmHhZ+h0fzCU98zRO3Q1vt0jpwMVBPttmslpj2Ic7PkNTR+680lapM9SQq\na1TyvwcRGTEqz1fQQsNnVd6LcQ6sRYwyicUYDi5rNnF49Tm29w/IriYiBCzjuiEZNYlOGIXoSynb\nyKvJU98Xsoa2bTk9PeXb3/4O3/zmN5nNZkynU5bL5bAO1/tIvXiMGYLHqq/Uw7adcxfEaESkvPdV\n1tCDuPpMpA8WuZQVYlaOJikxIDxB2QN9QI1x9RqglEs5YowtmfJFEuLwmjKa0X4KbYtnIlgcH5/z\njX/5Jl/60pe4evUSXdtS1ULbzhAJWJeQ7FksHiNGZemNVYk8rTctIlFFZEXRbMYWOK+1+OiGzCNn\noasTEiI2ClXY5s7H9/ngp+9z8vge0xoOtsfUBCRCbKFbCLlrmMUz3cSxkIooYKykHWpnBBMVHFaJ\naG8kZ4hLTGEg+gymUofymFZiwMFHui4UDU6vKk3Yob7VsV/EGUNVGSRlyBrIQuoIKRBTV4KrxaYx\nTZzQdJZREJZJx2xuNGFRd3wUT7BuhDSf4QnPwfmU+z7xff8ct6bXeeSuMs9bpJAY+wXT6glJdqja\nO0h7l4ZTGrugcZGNNlP5U5p8TiOR4D1RKtx4C6RBYsC3HR1RDZdchbEVY1sRxUI9JrmKmU+YZsTO\npQM+//f+LhvbOySMiuZWoyLGowHCicUVklnpMFDXjWpDZKFyFklQjxzOwXe/9+f8/u//Ph9++CHG\nGGaz5YVRpw+6nlYTCj2VPdrsyiVbXJksr/ppoBOXhIJKstFMoJ9CDLiZ0teZTCo0qK16NTnnoT8y\nNLuL8ZYxlhS9QsB7ij9peK6eNRcNtoYyqM/2mou+KX+V65kIFu2y5Y3vvc7D+w/4B7/5H7N/sIsx\nipxz1tIuW8geZ2uVTUsR50zxYMhaepSaTxuYfQqYAJVpzynhnKa84BCjvYpBCSuBZD0Re4Uo0HFV\nStCFNOA0UoirDKG4ajWupu+uVn0KK0aZm50SzFLKWjIZoAjX+qynTZegjZF51oCSvafLlq4L5NwW\ngI1Od5JRgJo1QuwSvoDXDDWQySbQD96NZKzohtAJgQFbE2LFeRt5fNqSncfawN12xDw4ggUhYfFI\ncUNLbkxoDjCScJIJHZiYIS6YezVFcrliGTotw4yqVVnjcGVMmomEqIxc55ya+FhHFAVgjacTtg8P\n2bt8xHiyga0rJAtGKsTZoUHb4x2e5mi0bYuIMJmoCHLbLgjFE/VrX/sad+/eVbyCMYMuZk8Ug9UG\nG1CUa/2DPljAaq1BKRFAR76l1xZCYlTVFzZv/9/rcnciFzOQHi26fvVj1z4wrU9q1n/v048//fOf\nxvVMBAtjDMvZgh/+8IfMZme8/Jkb3LjxPJevHNBsNuRoibliNJoiMlZMRQrlZC11ncggbiOVIYRu\n+ODaNpGTNoqsEzJjjDQ4O6ELQghAEpytGdWGytV0rZYNOZqiwZDU6i4oqEbr79J0jAHXaq+lNsKo\ngZEYvIEYOnZTtaJn+zJBkUiXM/OQmLUtPgkn8xmz+Zy2nEDJT3X0mEJJ8RXG25TAsbO5QUTFYbQR\nquVZtIGUTfFViUjR10hlUySp6WTEk4VldJpYOHBOuJsPmbNNNmNszFQEYjaQDV7GnFVXGJmGxlRM\nbAPdfQhnzPMJzmSsiaR2hjVCU+vfav0SnxNm1FCVtD0WGTw3GoGxJOuomjG7l65w5cbzbB/u02xM\niu2jUfk7U5HXsBDDBhIGJmgzGrPoWrrOE2OmGY8wRvj443t85zvfGe6b936Ab3edArCeRmAOWcea\nEG4u43CTGaDi/ViyR6pGMqbI8/VBoM8cnt64T5cO699b3/zrSN/1PbN+9QFuPeD113qp9atcz0Sw\nyDkzqhsMwscffczJkye895N3uHL1EpeO9rh27TJXr+5DsoWCp7J1KQVFSdQ141FN17WE0GmTNGd1\n0vYe7w1iapRh6MhmA2MbrJ2SUkuKCviZNGPGI0tta1ofMFFhy21MLHzkdKGK07P5eVkIsQi1RiQn\nmrpmZzwhlNraopt3q9XF5mMihUyOQkdimRJniyVn7ZJFzjx4csL5fEZIkUXrSXOVQzMZQuhI0Rcb\nPMfW5hTjGpyzGDNWSbYcaH1LFo+PEJNRan9RsBKpSBiCNCQ74TxVPGhrQjvCscnJ6AUCEzAjLIKU\n0XQSx5KapewwMjWbVY01DbVpMPGMRYDKWEyO2HoDyS2+NG+ddZiCrOvT/KoZYeqamfdkK4zHEzZ2\n9zi4epndoyPqyZhsnYq8iMHYCrCI9A3iVc9jsCnssQ2iY81Q9ELv3n3A6997g7ZtqesV5f1pMNwn\njRa1n1KYplFKyViEbMoYe9CukCIqTT8GtT+HpQEulCbrLOunv/rHn94n649/Ujbxi37m07h+abAQ\nkevA7wGX0dz2d3POvyMie8D/AbwA/Az4L3POT0Rf7e8A/zkwB/5xzvmNv/BvIOr14xW0NDubc3Zy\nysmTU957Rzi6vM+1a0d87tUbHBzusjndoKkMhmaYZy8XXptAhd4bYof36q5t3Vhl6muVH0t2gtgG\nsSPIAUkGh8M0YyqjyMwYwIdI1wZmJ0tOTmbcenybkCKz2YwQOoyByjqq2lE7y6SgRhfeM+m8En+A\nk/MlxlranAnG4ENmmQLz4DntOhadp7WW8xA4TyCmItcGazexCNaAi57YtapbmRMtcLxolTDkLJWx\nqolpLZE0oCa1ORww4sqcQOhwGLvFwjgcGzjZwsoO8+qINgshG50niA4ik4FoLF3epOuNjMgqRydb\n+OqMsOzoomW7npKS0EXPyEHlhJFrkJSIKWOswY0b6tGU8+Wc6cY2l64+x8HRNXaPLtNMN0imlEvG\nlGagBVHfMuOqAbk4gJKM1vXzdok1FVkMo1HNbNby+uuv8ydf/xeMRqNBwKautX7vpyFPp/F9I7EH\nUcFqLN1fw+g0K8SfjDZ8SwBzxg2nf29fsA7AWxdzUik8cwGb0WdQ69nH08GnDwR9ebL+PtZHtX+d\nZUgA/uuc8xsisgm8LiJ/BPxj4I9zzv+jiPxT4J8C/y3wnwGvlK//EPify7+/8Mo549uAZMNmUSme\nz885eXzGdNrw7ls/5eY7N3n04AHXr13j6rXL7O3tsLddY22tNafxBFkSU0fwgc5nFstADIlp3WCr\nKa6a4NyY4LYwpsKYBqFFhXkpKD0IIdESaecdx0/OeXD/mCdPTrhzeh8R3Qg5Z1xlmDSZRrTjPg+B\n8KTDWWFzPGEyGlNbR2xqpGqKmW8kx4hvE4uQEdfgTAV1xRaGkY/Uo8JwtKqdOBk1NHWtpUQMpKBg\npnZ2qu5hKdAV4FfKCRGnJkJkSAFJ2vzNRvsyGUuuaiIVc6ZUZoPKbuLFsEwKzgoYvEk6zZGEj55o\nBKRiYcYY2SEiVDKmrveZz88JcQbe02SDRb8WIWj/Q8Bah9iKLkTmZ+eM9/c5vHaN555/ie39Q+rJ\nJlLVShrr/8Ugxq00R4ylgCVLo0+bkdrfqofRYc7w/s9+xnf+9Lv8+Mc/xrl64N9MJpMhI1jfdH0A\n6AOFlLF3LzCDvpoLa1cnIH3/RIOsrGU8/QbvAWt9AOj5JH2m0Qemp7OF/vX1/Y7hdfQEyPL402Pb\np/fXp3H90mCRc74D3Cn/fSYibwHXgH8E/IPytP8F+DoaLP4R8HtZX+G/FpEdEblSfs8nXiIGZyps\nZfGLjpgTtWswWEKXMTRYMj/6/ru8+cN3ODjY4+joiBvP7XNwcMCVq0dcvXaImBq/OGW+6AgB2s4g\n4qiqbVy9ibWbiDQ4N1EFKaNWAimCbztC6rA1LCXgc+bsbM6jR2ccn54x71rG022sE7ZrnU4opkCd\nvceVw5Bh2WKNxVYN9aihdhXjnW0mkwl2PKYLkUXncbM5nM/wOZMQvAijekpMmaZpGI/HBKOLaXdn\nm/GoprZGcSY+0FSOe3c+5vz0jNn8jPl8Trf0ar1oaiI98U767YTpCXsFKdklQ46ZRmqyHZGZ01LT\nmppOip5YTjgCOXUYAojFZ+Hc1XjZwrgRB+kSl17Y5NA9oXv4Lu3Jx9SiwdPWljYmamfU9Nc5gqhw\n8OG1q1x+7gbTvX3cZEPRoCi0uapHAIQMiEOkKtwdQQpYrdeMyDmTFIJCM65p28hHt+7wta/9MT/4\nwQ9oW0/Mgq0qxBh8OcHj+qldGoh9X2M47Xudij6giAwBQzexWk70Eggha/Pblp9PSU2dF4sFXdep\nKvjQH1lZA/Rl0Xo2cXGPrDKN1Qh1lVX0r2f9+f31N9LgFJEXgN8AvgMc9QEg53xHRC6Vp10DPlr7\nsVvlsV8YLEgZCcoMBcGJQ6J6Yfp+8hCgqSZkIo8etNy/+z4//P5NAJqm4rnrR+ztb3HpaI/d3R1M\nlanrKZONLTa2XkLsCHFjmukmKYna8JVMom1bFosFowq8j9SuYtnOycDu/hYHR5eYTjeoJ9OhV5FJ\npODp/JLkO8ajhtS1bI2n7G5tM64buqXaEYZRpG2Eza0RU1ezIYa9BMcnZzx+fMxy2RKzxVvPctFR\nYdmwDfW2YzQasbe/CymyWM6Yz89prGO5OOfgcJuXXr4ORnjy5An37t3j+OyUsDDcOf6YdjEjlhGu\nRZ3q/bIF6RhtbJBNQ5daFu05qdliJPdwzSVCruhsQ8oGfIuJS8YmQFjgxeLFMstCtA2YERO5zMu/\n8SL/8O9dY3HnJu/+8Ju8//b3eXjnI5wiXGhDYlw5nNRsH17mxosv89IXv8hkY1vZluIICGIcrqmV\ntJczdSlF+sxCg4RHjK4TrKHzntFoRLLCg0fHfOu1b/PVr/4R79x8DyOOnb2DAcjXQ7HXkZP96d5b\nY/algXOKCh6ajEX6YF03tZ/IdaGUOJWDMmVZLpcsFgtCCDin97J3Dqvrmt6Nrn9N64Hi6clMz7EZ\ntsxT/Zb1Cc76z/dZTV96/SrXv3GwEJEN4P8C/quc8+nTqc76Uz/hsZ/Lg0TknwD/BKCqR6u0qngx\nKGITaqt1f0JVgNTXUrH6rpqoJ2Qb+OBn97h96z6jScNk2rB/sMvRlStcuzYiPj9ic7KHmJrlMmMb\ng8naIF1h8DMxJkabUyqXqastjeauomnGjMdTMFWxzvOa5MeK4C2xq3BWmC9bKiNsbUzU2apbsmzn\ntIsZXdMozbluGE82cE2tqt6SccaQYyAHT/QtpICvLO2jjGxllk7/7mI5IwVPdkq3r2zFdLrB5vYW\n0+mGoj+t40zA2ieIqCJYxONjZBk6vDcgEXEZ7wKmOieEc2p/DkwLRFgBVAr2MhgspABZFdWNaCmg\nKGQhdo7j+ZKzJVy7/gJ7W5aDvS1++Pp3uPv+OxA6tiZjalNxcOkKL3/u8xxdu85ka1cZrVgwtWp/\nGiEnFTte3zAK7NJxdTZxRdLLma2tKadnSyYbI9577z2+9a1v8e5Pb6Karpnj0xNGo/rCKT5sJlGI\ntogyXl2t6y3+/+y9Waxs2Xnf91vDHmo4587NHtlssckWJdMUTUpKLBmxIVq2kQCCAwXIS5zEhv3g\nCEGCIA/xgxHAechD4iBPART4RYgTI4gkeCAlU6REiqRENUmTokix2d1kN3u43Xc859SwxzXk4Vt7\n166653JqQroUtICDc05V7apde6/1rW/4f/9/FM6Irk+lVm3ks2MUIJwSgmef2vKtMRgruI/eddy6\ndXs0SiNYbVSg2wkEHRqF75TUnD42PD7Fe5yX/Pw2a/V7Gt+VsVBKZYih+Ocxxl9LD98Ywgul1CPA\nzfT4a8ATk8MfB64fvmeM8ZeBXwaYLS/Ew9hxCDwF7BJRQZCMMUZQQ91ZfmK09H2k6x1t11BVHXfv\nrHnt9Tu8+MLr2Pwa73q64OLFGVpbbEI/xiQqo5Ti+PiYC/OcC4sS72tsAr0USUFbkIkS97Zdi2ag\n9EtkgD5giMyKYtw96rZhtVlz0fRYY+jcBt125MpQ5JbcKog9Sgdc21M3W1bbtYQIvqOuetorHd22\nBhVo20aEgbWU6GYz6ZjVMSN4Td9E6k2Pa5OCd1YINsSRem2kSS9Gh5bufoKroN+Cr/A+A4TZO4sC\nESdIAUqHHDGRWbL8ARNdEhsKzBZzaXRCs7hwlR9/3we5dOkKv/3hjtXd25jC8tDbf4Sf+Mmf4u3v\nfEbyEFlGHyLBa4wyGGvGpinFrgNUjIUaoddeQfBecAlE2l5i/Rdf/BYf+63f5qvPPUeMkSzPhIx3\nbHAj5R9E03aaDzBGp8pSGLESonsSybTCWim9u64fSZTKPBdvJAQUGpd4KjbbLV3XjaFDnuejxsi0\nC/e8XMJ0kZ8XVkzXyNQwHIKxpp7Gn5ixSNWNfwZ8Lcb4TydP/SvgPwf+5/T7X04e/yWl1L9AEptn\n3y5fAUAcLPm9MdgYz6WSlVIqtWeLRqiKEkEabYjRSlOWkyRkU684vb3mox/5Lb7y6Nd554+8iyd/\n5J1ce/iIxXzOcjYXLEWMZMWMo+MLzGaG7doTQ7vzOryjqTu0Dri+pd1uhAfRObzriL5nfbZCa7h4\nfMzpaoX3nnXd4FBYF9C90J11zYaVSwI7ShD8joBzHU1f03YNzns61+A7izGWtqrRGpq6ZjYriDFw\n4dJFCIpqXbFabbl+/TovvfwKd+/exWhJiOrFglD3dLFDRSGs0X6AHfcon6Nch+9r8B2dzwXpivRc\n6Cit/zoIe3avxAvw0aODMFJbpCM4ywxKQdd7gms4nl/g7U89w4f+5i9w/bWXCK7j6aef5h1Pvwtd\nzKi6nlwbgpcwRSsJQXRM9IYH+OTAwG8qgKq+79HWkNuck7srLl855ld/9Vf5vT/4LH3vKMo5XefI\nc2ksDKG6Z1EdLsghKTkQ1GitKUoNaoB4R2E4D2LKhvCgbVu6rqNqGvH+QuDCxctjOJNNSGyGEq58\n7v2lAs5Zh7trcdB8NjUa0/+nhuVPquv0Z4D/DPgjpdSX0mP/CDES/69S6u8BrwD/SXruI0jZ9EWk\ndPpfftdnow8SNSomDQ9SJ50aDYY81DOA8/2gXUEAFcjzTHggQuT1b73Gq9+6zgtff5G3P/EOnnjq\nGm9729t4++NPEOoWrQ1lOScrSkIMuD6OFGfBeTpXs91uafozog+EphKiXu8IqUXYtR3WaqqqAmuE\nhTnPyBdLsqpHB+mI3XZdKr06bDkT/EMIY0t5nltUH1GI2K91LSYrKYxFW4UyitDLefVNy+nJQYxf\nhAAAIABJREFUCdtNxRvX3+Du7TtUVUWZt8zLS6hMM48CwKob8dZEMEtQqSoEonN0TUvfdihfgMox\nUYMOwsalglg0LCFqIjYhkHpslAlk6SmLjKIgEc9YfNA4LOWFKzxZzrh0YcHVq1eJxlL7gC6W0j9C\n6q8xGShDTM15ykwIgSTtmbp2mdD1BcnHZJaT0w2f+sxnaNuWxWJJ1wvGpe88ddeS252nEmMcQwHY\nLbrDsikg4L9heiaIt1FSyTi9e0Jd19Rts0eTuFws91ToD99/aqgOgVbneRaHIchhaDGtlBwaikMw\n11sZ30015NOcn4cA+LlzXh+B/+p7Ogs1XChQ+pxmGLVr506fgSRCQ+LQHGI2aR5XSoOPAm+Okbww\nOB85vX2L9d1TvvFSxuULF3nkkYd52+VLZAouXTgGldF2FT4aVIh0oad2FU21Ybtdc1pLVjzzjlxr\nLBETQ+oNESh5QFPOFiwvXqJ0HWa9orxRi8xeIk4ZBHV0licGL0E8qrgkM3as3xcXSy5dukRhgBhw\nncVqQ3Adno7V+i5ZXdA0HW2zRXlHpiCjw7cbqRZoJaFUjGjvUZnGYFAmEpUlhLjjWIgZwpAusbwo\nkwoCVLg3BCQFArE3UWFRZNozy4Rt2nvPspzjQ8/JquZTn/4cT739EY4vXGLVeIp5Tr48ouk68RWs\nkTBHZwQlhDdGkyDruzHm85VISw7Yha6TPMFnP/tZ2rYfKxEZGu87PAOC8v5Vg/MW2JAQzPNB4c7j\nAoTe0XU9TdOxXW/GsGJWlhIiJi1SrLlnp4cddR9M29X3cw3n5RkOvaLpuZ9nPA4xGT+IisgDgeAE\nCGroyFSjZRKocmI6IiHqUpkMIM8kyxajoqk9pFgSpdCZIZuJu9clTkWFhthTbzre2FTcvnmLl8qS\nixeOePSha5xcOGJRWIiOme7Ad1SbMzbrM5zrqFRBDI4FAlW21iaNkshqteLKtas8+vhjXLz2ELPj\nJauqoo0B7iTcQxTcQlCSea/rWmgYspzj2YJYRpxbjjf5wnvezWOPPsJrL7/EG6+9ig8RH1p8dOgI\n1bYRaLrrCa6B0EPo0dHL7u4CXXD0iZMBI/KDSuco5YkmJ2jxDRQWEwTaHZQkGkMCZRklau9Kadnj\n/bB45X+Fw/UtIjmqyDLLa69c50tf+jJ3zzZcOKv4xrdeJyrPlbe9jceefAc6LyA4YblSEJKMYXpb\nhpsu1HjJo0yoigGJ2fc9vXd89atf5ZOf/KQYYh/oOqlouBAIfY+1WerNOR9ePeAhBpzDUDqVBdfi\neyl/tm2L7zx9L+XrLMtEbzQrmc/nidCnp3M96iBEGD53io+43x58Xihy+P8hR8Yh1mJ6zOF5fL/j\ngTAWIQR816PSDRpdxUAiJNmPuzSS8MLZ8YIUmRaR3iHH4R3txo3JMTlOQprclsJb0DWcdC2n1YZX\nbr6BUpGyyDiazXj6kcfQ3lGf1uhgOD56iHdcdDRNQ2YUF46WzOallKY6x5VHr/GBn/xpfuIDP82r\nb7zJ62/cYn2nplt5LhRGkJN9z3yWo/A0TUW/2bBcLpmbjKNyTh+DqL8bjbaGv/j0O3ji0cdwN9/k\n1nbD3PUUZUaXaOFUZui6ln6zhmpDGTsKHZiFhZRJu5auaVk3PWddoPYaXWTCK2ly+mDFWOTgbENf\nVFQcsfGixyoMYwblgU5TmJLWbcH2oBtidChtWCrHo1cvspxBf1Lzid/9FF/63O/zR1/6Im3QfNSD\nKUpWmzVaa97x9id473vfy3//3/5D6YEJPbbQ1G1DVmQjO1RuLa4XFfJZMSdGqLc1s6MZZ6s1Wls+\n8pGP8Kv/36/z2muvc3zxKmVuUVE6MXNTSAXEQ6kzhEU+caJEAdJZLSpjzjm8SxWKQnpY1tWW7d3X\n6HsJMazNmRUzysUcm0ieSRR3HmEwRylMUY74DyK7XNs0ia9EHnIwAcNSHnIhhwt9CtialnuHcOrQ\nYByGK9Nw6vsdD4Sx2HkS+8mme153TlloOoZSnjoI0Q4zx957YeGOkWA0StC6KBWpqp7Q9tzQllJb\nlHeCxlwcgV4xmy1YzArKUuj4lFJkNpdFX5Zs12tOb93h9PYtuqoWrgUJUiQ8mmTigZHtu21rHAqv\npHlNec3X//hrXH/lVd587VU0iiIvyKwh9I6iyOi8S+xg0m1rtSGgRlakoTQYEJ3RPnhiI7gVraSk\nik6yCOlcvJXEG1qQm0EJyQ7G0LseTyRTkcwIxFqHSKYUwfV88Qvf4vkvfIaX/viL3H7jVdq6ofbQ\nRU2OpsxF9fvmjRt8/MYb/IVn3sl7fuwZrly7TMGMxWwu5MMxjNq1eZ4L30nqFp0vZ6w3FVpbPv2p\n3+N3fvuTrFZrLl6+inMiIuUnMbpcgx2WwQUhrkHrkfNz9ChMFMV5J8Q+pIU4JCjzvCS3earWDLv1\nZMdWO89lYN0SgxGFxPmc+XxYBTwvBD/8fV4CE+7FW+wMRmSKQv1+xwNhLCITyrKw4wEYkjNTF2oa\nBw70ZsCIsFNKgRlQd+cninyUnMDwuUoFtBRDUUrRupYbt+8wzwpm1mBtTusDKCF4KWYLyrzAJio4\nHQN90/LaK6+yWTecnK3o6grtg8gbKtnVNAFhW86w2hNNMl6hp3ct0VjZibQnOs3N629wI3i0c8yL\nnDwzkpy0RnpN2lZIY0PEJNeZGMFpqRKF1F6uJPQZQiEI4D0uSO5DB0emQmLpihijcDbglZANKQV5\nniX+Kbl+eaqIhKZlW93g63/4Jb7xR8/y4pf/ALe+i+6lzHu8vMB62woYTCuK2RyUousbfuVXfoUP\nfOAD/OWf+ff4sb/wHo4uHuF6R55ndDHQVDVFIeFel9S7jLH4qHnh+Rf4N7/xG3zz5VcRlvUM50Q8\nCDVdGDtDOLBxaz10su5CgiypU/XeCTGfitgMdJaTGyt5n6xAoUc2MplPshAj52Ma9pKYeveclgmZ\nSI32vYEpFuR+xmOalzj8vHvLqD8YyPcDYSyAvez04FKdV2c+HGPIwu4CDS3EI17DH8ZuMSXwpKMz\nhkEWEUwmbNpV3RB6T6MNPij6CDZkLBczHiqPsNagYkjK6YGiKKnO1vSNIPlsVEJ91/dolQudHAGD\nJTOekCjmd6zeThikMhEjDmk38C4wz7PE9O3xwZErQ9872rqia3shRlEalSUuTm3ofEQFJ56W1lJd\niBprNEZD8J0YFdOTRU+hJfzpdKRDKAGdcgTlRNmbgM5Lgu/kO8UO09W0qzPuvPEtnj27wa2Xvo7f\nblhm+ViCXBQLXA+q7WjahvpsLU13Zc6NGzf5zGc+w2uvv8IHvvF+fvZnf5Ynn3yCTBscDp3Yt4ak\n8KAqf3a65d/+5sd58YWXKcs5Shm6riMy3PNU4kz3OiRI9lDK1NZI0jYIxZ+K03g/0doZ6UnpSgEM\nqpSwjCEle5ns8IOydhxYRO6zmEPcC0mUUtjJ4g8TQzE8dr8xfe/D8GPqhRhz4P28hfHAGAtgz5pO\nKcm+02tRKlGhkxaawhiNHijrjN6nFdMqgYkYj2Gw6n26SUHT+kDnAs5vqHpHrCLXHrrC40+8A2Vz\niH1SpsxZLOYYpSFKWIGXi5tZC12KF73wbkRlwYp2ZeeThGLfoq3olg/aGaqPBKXI0UTfC6YjBKLr\ncZ0I3vquk8qPipgUf3tlRlVtdBzDMy0EohgV0BpKY9BWs8gMs8TT0HgnDWshgk78p0rT9j0WjfGO\nEsdCeei2VGe38ZsTbt6tyaLnaD4ntFs0mrqpqVZrurbHak2hLCYArsf0hkuXLrFdrXnuq89x8403\nefP16/zch/4aTz/9NA8/eo2qcdRVLddCaYqyQCn4nd/5JF/80pdRWmDivXPYLMeHdjT6Qxk9KNkM\nUBalI9qaZKC9SE96Rla1IVSRPLswwg8Au+ChCw47SFPqXUJ28GAHQ6GU4rDSMS78ENOLALNb1NPf\nh2OayDwvJzH1kA5LtD+o5CY8IMZisNLnxWPnjdEDOcdoTnPMA8ejPD7obkSCkb+H7NKgXJbS8CiS\nTJ4SkFftI65zvF5vOW16ji9c4bFHHuba5WOWxyVWgY4OEyNdXRFdhKGhCC2kPMFJRSHKHIlaPsk5\n8Co1MFnBapgIRC3Q9+CJrVR6dCL+DV0j7x+CzFcFUcDQiH6FaGgMzU0jSacSti5NRpFZVFGSzWcc\nFZaFCcS+JtdzCUmUJphICDq1iFtC48npmWlP2W5oT2+iT97A9g2+3XB8PCfzcLo+pchEtMm5QGZE\nCiC3ljK3aBWl/8Z3OOfIM8utGzf5zQ//Jq+89DLve997+Tt/978gL0us0nilKeYFxsDpquGjv/Ux\n6rpJ5dMoCUhEdiFGPRoMHYVUWUfkOSYJcCcoTYMwo+02KKkCidyEUB5EN2nyUsKOdU+4kap432nB\n7z0fIkor8YSQzNZ0zn67cZjfG9rbDxObhwblrYwHwlgMq/u8JprpOHTrPDsXLBgleIe0egblK6WS\nTQhTNKhcWI1ID+hImmRRdm6tcCiR41NCDGs8dGrGuoYPf+ZLlJniocuXefLxR3j46mUee9tVTAzY\n0DPPMsqiJHhH3bYcmx6lArmVr+q9wqBEN14FvHe0lSP2HjpHbsSNt0rgxXlmiM7j+hYbJPmH88y1\noVCGXnu8G3YyRYiQIZn+TAsiUwVRE8tQ6HpNWeQsZxmlDcxoyOtTMnuGVzOiWWC8AS/doVCSxRll\nbFmoSLa6xfqVL6NPr/NkaTlrT/DbNWf1KXjHoiw4PjpmeSzcHUprjo+PaeqK9fqM7WZFYTOqwmB1\nxundE4pMJCD+3R98gXq14amnnuLClUtcuvYQ1x55GJPlfOWPn+dXf/3XaPqWpm9ZFkecnZ5S5DPZ\n5f3QD6qldydIu77WEUwu+i4uYA1YnQxr1OggTOFBaUyeo3TA+x6MYjZb0Pc9XdcJ2tT3OKXINAxC\nX4FdmXRgdB882UO8w1CSHcY9gKnJlD+c74dt71PJgX0cRkxpmxTKTiopb2U8EMZCJYvvvR8FXiXe\nurcl93AMzxtjRliuG3aTdPGM1kSjialZjIlnYoKkHoExJo2y7dP7lCwzEvtuWinF5sYSUdxYbTn5\n+ovkL2je/dTbubic86PveJzc5NQpKaJyhffbhFYUVamhAzJ6l2jfFa7p0B5cNGS5JaqIzoSmz7ed\nNHcFiN5Jlj79hBBIlMUj6CuisV60LKxS5NqQKU3nHTp6VGiFvEZXeE5w0VAoBfMVtrhGkVtWLuBQ\nmKwkBosKmrnR6HrD+uarhO1tFmFNvW5RvufC0VwM27zg6OiIGBV5ltH2NfVmI6ztVjOfz6mbLU3f\n0Wtp65/NFviupW5aHn/0EZ555hkuXrzMcnlMCIHbt+9y5xvf5Hc//Rm++MU/ZFO1OOfYblegBQre\n96C1pa0brJXKi/c9BGny6tJGkOWGXEeiE3IfH3piLKXxL0S0MXS+pu8jhbGEIE00ZTljdXo2QWTu\nchJBBQaxqyEcxt8L456WPmG/mjH8+LgjqxlCI+DcjtTDY4fwKcZdKPJnz7OAPYsJuwsyTXYeWkcR\nINpd8OHZoVIyCqxMHtdaMz6BTpS+6cgomMXhud2REKNClXNU8HTB4R10vkPofyNffO4Frl24wKVL\nl1DFTJiru5a27cmtSupVyd0c4L9KCIGjSpPBQ+wDyirKvMDFFoVAz1VyjwdtEBMQFxvJLwwUbRHo\nuygUfERybSi0pdA93hpC13N5XrDILbkBfIffrmijolme4fOePvZ4VRCUAN60Uvi2IoQV/dkNXH0b\nywataqJq0lWEopQSY9NK45ZKaFRlNcpqXAg06xXLowusVqciKl2WeNdRdx3L5ZKn3/0u3v2j7+Gp\nd/4I67pBZZbT1Rkf/ejH+PJX/gi0IUZPUcriFGJcRUzJSlE+M5KD8iGFP4a+bylzg8UR2obgejHU\nQVHOcroAdecICdhn8xl5ZmnDShYiatQ9DUEkBo3RY1VNpfDBJPSrnuz+sL+pHRqRGJP+B4Axe2th\nWAeH3J1TIyDFAU+cJGqn3ob87Fi4vt/xgBiLuAcymX7R+2HnR5cPUk4gTJb2YHHT0wNyTw5EqYlQ\nbEwkqwleMGiVCkVaQvhFwRwoJXogMQi8S34HuhhwEdrbd/jks1/k8Ueu8dgjD/PwQ9c4Or6M7yrh\n5fARqw2EQOsDm6alD4MXldG3jrpryKNlrgtUFsWQhRRGREaRo0Q8P37P6bUaRHuMMeQ2Izc9udb0\nUUKChy5dZGENJmoap+mjQ4ea3m2INtAS8EaMWYgei2I5t3D7DtXJq/jmDsqt2LZnxHrDkw89krgg\nBjV4nxaWxmQZNpVKsyxjW/VcWFxGNzWu2tA2In3giTz6+OO8+z0/yuVrV+ldYHl0hM5yvvzVP+Zr\nX3uOu3dOyMoCrWXhNk2HTSzwRtukpRFwnRA6G6UxSgBJKtRJtqHDxo7ZTFLTzkeq+pRo52SZaOD2\n3qGiFyMsSplYK+XTEAJd3eAIKaQgGQjZdoAkxrwr+x8m66cAqr382zlzfPozvOfwM3g5Xdelqse9\nqM0h0RvjnxHPYljUxpiRfEQe35VSd67WpMwlclN7HslenJd+3S//IX8k0Za958RQqCgJrqHE6l0/\nJqOk0iB4BqIkvrroef3Wbc7Wa27dXfHE6ZorV67w5LEXVm5yqrqhr2uqqqLtGrLCEpEmMlpH23XU\nqqawBcbGRC0nRpQYU3LQ4rqd5upwEceJYoSvI8OQ2UBuLbk2dEpo8t926SJFcsWLHtpgiVpzN3q8\nTvyd1uMUqODQsaZQmtM736Rbv8rSNMxnGuc1vVOJgt+M3ZqgKcsM73uuXr5M5x2vvXZCludoYzjb\nrKnaRrREWqkeXbx4LF7FM+/hoYevEbTCFiVf+/pzfOJTn2K9qSiK2U73cwJAGu5nnltcL2FDkWcY\nIsG3YrxCRWZhMdM8/fa38yNPPopVmjdu3OF3P/88vXdg52hboFXEJerDkESBsqxIjOtuL0wWvY5U\nXWOCMjZmrEINFfxhOzvE/QyPnYfAPExW3g9bMSQ77of1+EGMB8NYsA9XPTQW04s0vaiSwxnCj52C\ndIzJwKgpVfxwc9JOza62LUcjd1QPKbJhMpJ2cYWJskB9AIwmRo0bYk2lKDKLyXKqvueVN25x8+4p\nZVny/nde4aGr13h0uWCzXbNdbwh9RyTSJACQnuVCVBsjbduy1WtyazHljNxavBftE0g6nkqSc3GC\nIZDvp/HRSR+K3nkXZV4QY+TiIufSxSV58HRdD00gusQdSRSR49ATlcPagIoN1jVkwdHdfIGsv83R\nhcjcWqo8R/VzmkZ6b0jkulM2qiwzhOAIwbHZruh84OxuRdt1zMsM5wR9+tgTj/P0u9/Fw48/xpWr\nlyiOjnjt9df5+G//Di88/w1ciLS9cFjiIy4EymIusnw60nf9qISWWY1REd83tE2FJvLYQxnPvPMp\nnn7yEZ54+BLHi5KuqXnocsnXvvEyN+5WbKoWXSxQRvhOY/A7xiyl6Jt+ZL7Ks0zaCJQaYDxj/0mM\nUaQp2C3esfyZqBb0fRbwuZse3954nCchsBs/mI5TeECMBfH8L36YNZ5eCKUGQpTxgXHHgeQGqp1U\n3l6FJZ5/AVXaBYbPVkphohgKImglACyCRpEaoJKRMtbiQmRbtRBEnaz3gart+NJzWx66UtE+9jC5\nCsCMPngRY65PQXmuHB1RBIXug1DKOU9hMjJtmM/n6JDhnUuEP7J7H+4YA+2cVzu3dgDmFLlFm5LF\nrGRe5uQxYI0SOUij8CjmKmPbRTKjZGIoj9E1M7/CVGfk9ZsUak3eKXznibUjJhZ0YwzHx0uKoqRv\nGyEVioEbbwrvUW41m6pCmQxtDHleYAg0znHtbVf54E/+JO97//u5cu0qQcHp2Rm//+yzfO3rLxBQ\nWCs5nappKLKM2AeCBef86H0Nim3edfjQo5Uns4Kcfe+7n+CDf+lHefyhizTrO9y+/jIaxaXFEYVp\nsaalpCQqT6Af8yA2z6QDuW5Yr9d0XSOiz0WxW9CBFBKrEZnp471ehJkkHEeGron3ezjnp/P2vDUw\nnafDZjgN389Lqr6V8UAYi+mFGuKy4SJMwS3TOG/wHA6fG95vvDla7VnlqBD6NsQDSdXpSZl1l2xS\nDDuEuPt56CUcCIApUAa8kqar4MQ1tVmBTW323nn6zvGm19xe3+T1l97g4qLgwizD+4qqPsOHhhh7\n1heOubxYcJyV0pPSB6qzrYCEnMMaRaYHFOEU+Zl2q0mOxhFBBYzOyG3GUVYwPxY0pPGtVFKUQ+uA\nzhSFEczFVXuRkzZS5FCHhtBXUN2kuv0Sm9df5uFwgokV1fU1fYyU8yvMiyssLkp57uT0DK3XHM0X\ntHVDU68xVjpsZ8slx5ev8ObtO5ioWFcVV65d5uf/1s/zoZ//6zz5zqcwmSWmisn/9I//Mc8//3xi\n0hJdl77ppH/ER/Jcsd3UUj3rA1meQXQo5dHak+VakK3G8/S7nuLff/878d0dXnvxJY5nhisLR5kX\nbJoTTm6+QAxLytlVGq/RPiMvZnSdp944zs7O8L4Xrox5uac/EoKoiKgo0oLjPEysWHvVPK1Hgew9\nz2GCrfhOeIlDD0MplYBkO96MHcdoGL2iISf4VsYDYSxQuz7/ga790IuYXqjxIk3iVp84Du7JJIdI\nmNaZY8SFSbIp5UiU1iidqNuGG4TCTzyS2imwlhgSLX2qFKA1LrVFt3R0ShJePkZELd2RW0uIllXt\nyPpAbg3GXiL6hswG+rzkzHnmi4JiMcc1NbN+BluDLxUWjQ8iS6hNRPcdOvbpu2UonfRTo1RnelvQ\nW/EwSqPJdcT4Dl/VmH6Gi466q3Ghw+YGaxtms5ewm4LLVlOENdX6edTmm2xvv4zfVnSXH2bbB9a+\n4iibc2wV1vesG0dwXqoDXjymLMtwThjOm80Zd07OeGx5hDUKF3rKLPCBv/xBfu5v/TwPPfIw2WKO\ntoam7zitG1546WV0PmNhLev1GpubJKjo8XhsjMwLCKEVucqgUGYmADe3ZW4VWdjwvnc9xrufOIJb\nL6FC5OLyApmf03Waqi157tXXuREuweyYaOd4Y7FkGJURA1TVHapqQ9d1LBYL8mImuQptBIOikg5t\nFO3SccMKkUwJUAwSLs4fNLjFOHogSsvGuBPyjkltLyTs0C75T0rGR+8A0Y0JWoHRhBjxeCnnxpD4\nYQKuf+uqZA+GsYj7jWSwH7MdWtdh7FVGDvIa55WthnGYiZ4ao0M3bw/4Ikfvn3ocdCLExgQkQSp4\nUDEY3ouwswoS1gQn6EilA9aInms5X3AxtyyOFuR5htUaU0kXat00QEaRye7lvROK+iAIizHHKU5Q\n+i5BPI2JG2oiaG2o65rM6PRdBLVqtaXILNcuHfPqesPp+k1cd0LW1bTVluVsBgGuXLxKmWv8tqNu\nW2bR0IUOFaHrWrpGkJXL+YI8z0dWsLbveO2N68Je5TuuXLnCL/7iL/LYE09QNQ15nrM4yli/WfH7\nz36O9XotbeaJgm5QHx92zpiSU3JvhMagqmvKWY41OT62AoO3GfPFEYvjgr73RJ1BXuLawMvX3+Ar\nz32DYnaBmM/wOkPFDBs13nm6rktehSfP89RMlonurJmwjYdwzxw7zDnsNqv9qa9iTOrv7AzBeMyB\nAdpL1AemwspODZ642+X1IqOMZ1R/RsIQ1Plu19T1OqxoHGbCh9+HiLhza9r3ST4dHndorJT0HAIm\nYTLieO7T+nmMccRrCE81BM+oy6qi0LKp6GiViCZHlVEujyhnc4yKItHjpTV723ZoK1J6KIWLgtKM\nSsSVQ0rkKhQko0UM0iCVjIdUbQTRV9cNlEWqESsyXXI0P2J27RLffOUup7e3bKpbWH2G6la4uqKN\nke1aUXQ5dbPBbWrKMEPPStBQliVlWVAnUpqma9Ea+uBZbzccXbwgic8QuHzlGh/60Ie4dOUqAbj6\n0EOgFTdubfjKV/6Yz3zmM2ht8b4faewH6cBpH4adJHVDBGPVuOB8SOXOxTG6WLB1DWW5IKicO5ua\nV16/zbN/+HW+8eoN1MXHCN4ilIIK1weaeku1XtMnmYHZbDZS5cmPiDNPlcX25s5hm/x9qhQyV6b/\nD3/v4yIOE//Tea21hFyDMLNwb4h0ZN+39wgjfb/jwTAWaQwXdJq3AO4xEuexAg3HD6+ZWuMBJju8\nZup5TN/vfoZmeL01Au2d6EKNnoXkt5RQ5o+EGglBahQ66pFJSqWGMx8jXdcTfWBT9biLGV4ZOgSN\nRy69GXXfk4eAjVG0MlTEYYRlCi+fO5boFODFNZesVyKDCeAjfduTBY0Pch7GlJTFksX8EqvQc+P1\nF2lrhaWB/ox2e4qN0NcdDz3yTlQZIXjqLOKaSFXX+K7DKM1sVjJfLmlTabjrGrBynovlklt3blMu\n5vyVv/of8Ff+2l+l945Sa7JCs9q0XL9+nc9+7llefOGbFEVBURRS7Uj3wPcuJZMzueZ64OrSYvRM\nCi2J9F6R25xgCmK2wLFi6xR3T055+ZUbvPjKTV55c0XILhB8TohGxKRDoKq2rFenNNWW2WzOYrEY\nMRai75HkKcK+tMBwnsEzGovpPBwoFe6peMQonb0xJo9wf4O7t0zs9+awc44Q3STHF0S+IfUGqfus\nl+91PDDGQq7DLqu7S+Dd2713v/LS/caYYILxhk+rL1Oy0/O8ld1niZ8fJ1WT4TmllPQjxNSwJmAM\n+cwYU6+ClRmutDQ1xQjK0jjHybrm8rZmnhcoK30dUfcEK4pptfPo3iXiWelXiQqiFuZtkL9VjKOt\nMqlPJMZIiIrgAn2IWJ0RVYbJM2G/1gWbqueF61+jWWeU5TEqNqxWd/CrE45tztGlKzxy9RGa2Er3\nax9o2i2ta2mbiqaqRsr7EAScFYhYa7j60DXqtsHFwF/6wAf4m//Rf8iFCxe4cPmSgKu/5G6CAAAc\nHElEQVS6yMnZGV/8wy/xuWe/QNO1AKLVgiQWp56bMhqjFFoJZD5G0SbzzslzWjyNqgu8eeeMy5cv\nM6fmtevf4pvfus4bt1dsW40pL1LOLtK5Ahcioe9o2pb1asV6vSLiuTS/muQX9vVJ9xOJE8SxB6X8\neO+H5+43L8dqhUqcGMR7jtvf3PZb0AcDoc0gGk1qjBTpB+JI3vAd18l3Gg+GsYjThZ92yMnPsENP\nKySHhmO60KevHQzC9IIfCtCeN6au4+738Ob7LqfUVIawJKCG3tfUJq6Sqyldj0O+RBGxaD2j7zec\nbRpunmwobIY+WlLmQlyrcqGWq12H6iIh5KnaMhTqduhUyZXIY4PrObjpwt8ZUbYQ2jqdY7MSn2mq\numN7suK1054QruEaT+8rYleBd5TZgsvHlzi9dcY2tDS9g+Tml0VBSUZTVVTpZzDAJrNSejSauu75\nmZ/9WX7hP/7bPPr4YzRdSzmf4Zyj6Rxf+/rz/M4nP8XJ2SlHR0dUVUOeJ1HiEJLQ8IBMzeQaR0eM\ng0cYMMaKkHMUt8M7xQvfepPbJ6eY5jo3bt5m20dUvsTrGdaUhJiR2RLXNLR1R7XeUG+lClXMRUVs\nqCRIZcFMdvCDNvGoGepr92AftCTDD8ORKQ9oCGGPYevQGx7ea7q57XhfnHhZSvJcg9L8lCD4rY4H\nw1jAnidxP8Tm0Ho8WNTDcGWKsTg0ECOtmnOouAtNplJ0592U6c0NIZwb/8kx0qCmU3uz7DCKqDxK\nJ31Mr4n0hKDQ2iRmJo9TltOmp3v9TVzwqCznSGmWWZQqQAtt69BdlJ42ZRANC6S5DI+Pwq4dCCht\nhUELSyoAE4zBG4tZGpqo2DhHu+2o7zRUTU3dtTT2GIrI9uw2dbMCV5OrjHlxTLOVZqlcZSyWl0Ev\nWK9O6JuW+XxJnmew2l0b7z2eyJ2Tuzz1rqf5+7/0D/ngT/+UkMgUBRo4WW9Yb7f863/9YT75yU9S\nNTWXLl2haRrm8zmb9Va8layga1tmMxGK9t7jg0da5nYG3WhJ1PoYiDEjmxecdg0nNysuqQXm6CJz\nrai8xyuDykoCkTtvvka73lJtNkQ8i3nBlcsXsWWOtfnI4J3nxVgyHecJRhL0HulITa+12T6tngq7\nOTTM0cM55L2XNvVJkn84JibKA0nmxoPnpOkwBDeWa3UiwPbBY9W9mJzvZzwQxiISMSZLF8mNC3yw\n6IfJz+GxqfsRY0z5gCEZuR9Tjt17WqMmLcWHlZThvcfn9ATkAiP6Tm52JIQDV1RLLmDc85XkECKy\n2ykUMXpCYESLeiJ99DQOzrZb7qxWlHMBbg3lYJVpvOuJWIzSON9jFDgdE3tTYrcaFL4yKaf2QOeF\n+cnMS1CKumlZd47VZoNXgMmotWfjFF3sWW1WKNWRa+mpt9kcFYUtXHZQYbfWMaBNYL1eY4xhNpsJ\nY3mMZGVBVLDpGh557An+4vveR9SKLM9wIeBC4OLxMR/5zY/y7LPP0ntH3/cYY1gul6xWqzGBaLQo\nw4EssrYTwFeeWcyQa5qAm4w2eC+CR6Rwq28Nvoc2ODoFpjRgoG9r2vaMpq0h9MxnM+bzBVlegN4p\nnA9Vh2FOynwwRH0vBPu8XNt5lZHD+Qe78ujQ9zHd7Kae8tS7UDrS+164UNR+A6QKEcz9PejvZTwQ\nxmIaTyk1aDzsW9fDEWP8tlHYt8tnHAqyDKQjsLP8PtW5h/b58z5veP+xu1UL3VpMz+khM6+kMmFS\nh4AkFYR/0Q2JqfSZTdtztq24UDeUhcdpN4lNPb7v8Dq1I3tP9A4fQqLST30kSlHkJUEbgg+JVzKi\n8ARtOGt7tn3H0C8aYqRXFlUuqDcimGNNAhGh2baOpc0JXaDtarR3KN3hncN1LSHYERZdliUuSren\nySy3V6ey2GxGCNA2PYvjI2xe8NnPPsuzzz7L3dMTsixjsVgAsNlsxGtM0Mip6x1j0mgZduYEeZcE\ntJFcp9+FtBiNMgaMEBkZ48mM0A2Gvqeu1tTNChUjRZkxnxXM8gKlc4IS2oOB2m+aQxs3q4P5Np5n\nmJbbgZTcFNlFk0Lv4dj0d9QcOBykWtr4GVMDM8wjrYxUveL5BuoHYSjgATEWivMs737P/3R8N4nN\n6W5/GFrsWeWJlZ++/jA0GSffwecblLRyE9BRfI+x9yTlLqRSIkZCRY1gLMM40dHSLxBRVG3H6dmG\no+WW5XGPVpa5tqCccDRGJ+wVWurwQ/okpnJhAHRW0rhIFzpaLx2uTd/Qeo8pSk6ritZLnd6HgPcB\nrzRaWZp2la6geGCZLYhkZHmB6zx9LWCwLB86Swt82IHhtNa4VhiwillJXdfyblpLPG40ealp28An\nPvEJvvnNbwqN4HA90446XOddnnh3P7Qm9cfo1GuhU1I54RSUGJSoxctAa4zNidGhlSbXBh86qnpL\ntTqj71tmec6sLCRHkWciAI0i6H0quzHfFfc3nCFpfr9msG/rbQyhyj3P+71qy543zE4BbRiH5VUd\ndz7GD8JcPBDGAjWAi9zkIdlJz3PfvvPb3Qu4Gv4+LKOOx8R0HuNn3GuMhlZ24ZVIN1CnTHaaPBFF\nTOzh4zBI6TSClLNUQug5lPYj/kFZTe8C603Fel3T5hqjPIVVZFoT07kPiD6MxoQMpQIqSALWR4i2\nYFs1rKot26aljUJk4yLoPrDtewKKrMgJQeGIY8evdx1aRXzv8SFH5zlRZTgERKbNbqfLsozZrODO\n2Ya7J3ex1nJ0dESpSk5XZ6y34iEsl0uJx42mLIQe7+zsjOeeew6lFMfLoxG8pTJGVTajxAgMi3S4\nl0bv56wEuDRp0QeMTYnk1DjYxzBWGoIT3ZbNekW93QhGJC8oyjkqs1KGngDchvkyKMXJeeyM4/2k\nEKchxHT+TQFc6WMO5l2YRNj7qGWtNdpA9PuejjxvdtUSL6Q8Md5L3/D9jgfCWIzOWtR7C00Yf8Le\nK8e/lNpzA8fHDv7fu8jpRhoGRvCdazeM8WZO3m/qgvqUzdIGyTgfuH6EKNyTk3q5mZCFDptkTA3L\nVmm60EvIozKIiq71bLYN1TxDa8tCaQwpbrZSYXBdn3ZVhLhWC1AMH9m4wFnTcrrZst6KHkk+m5PN\n5rTe4zHEqCBaFBFrLLnNaKuO0NXoqIiB1B+T0bvAtq7IUJSzjBAivWsJIWJtOcbyIQTatqXvd4m+\no6Mjrl69KnDpsqAocs7OWl544QXOzs7GpKVSahQ8FlCRHe/FsBgHr0MTRnjcQCEAU+MvxhOlIKmZ\nRSXQabyja2u2Zyuq9RaCYn60xGYF0WZ4LaVtrRW4e8MM8XYM036O/RyEGJbOC8t7VPfOy0P2t2ky\nfgipDj93fx6zV9HbbxrToyxGjBGCGhGib3U8GMZCTW5GUKAmzN2TcU/Mdu577d/A4e/p/6Olj5Ep\nJ0hk0pB2jiHaned+qXd6bsPfOjJyZMSQRIaGD1MhLcYoeYEQ0EpKb1ErolPUVc+mAqtzWg3WQm5F\nTQwtDOCujXQBegVeG1xQ9DGwajuu375L03UoY8nyEp0XBC3ixpILCPS9F4IYNHjwbY3vaqzOKOyC\nIl8wK5Z4JxwXZSGku7439OuWpuskqZoYqgZFce+9yPrlOeu2lq5Zrbl08Rg0fP6PvsonP/G7owFv\nmga8GAgfIfgAduDKFk9GKdKuKe5TTLR2KIUyaRonFKOKEsCP0gBKoXIh6fWuo20qXN2iPZT5nCxb\nSH5HGbTJ5f2CkrBR7ZKMcjOFcnG431Nim8FQxCj6sUNSVJlEe5gAVOOc4l5DsmeUzvGQld7NueFx\nCUekD0qFaTFgV8r9QYwHwljADu8vAJMd5+B+h95BxSIdfegdMHl8+tyu5GV2j09u8BD/Cyx5p3s5\nmI0x0eWGWFLwFcMxQw0e5CZahnhWEHpEneTpPCHdc9dLJ2tZFvSdyAMor1idbdnOlxQ5dHlgnlny\nMqec52jl6b3Dp0YlHy1V6zlZ16zWW17cVMSoWC6XZHkhDNguoqN4RrnJMJlGx6Ri1vWcnZxQb29T\nKocOEeMyQuyJdtA4jbRuQ+dAeQGF1W3Ppmq4cHHJcrmkbVvOzoSnMiukp+P46AI/8RPv56l3PELV\nwm/+xkf5v/6f/5u2bSmznLbejte/66WvZL4oJSRKO2ae5/uhY3D46IkmQ5tsbMLqnVAJWqMxxkpJ\n2Yk/v+5WtJuK7WqL3/bk0XJhfpG8XKDLJY3r6XQEm6WKT4/RIt7Up3uklUXr/ZzWji5vl7cRHo9s\nz7MNab5NqxzT+TsNY85DEseYgH1xVyEc+0KcQwWFR6ABwYOxaQ0dIELfynggjEUkYnJhIspyoWV3\nvaD4dpZ4EE4e4r+0Y2vZtQfuxMMEpVKK4CMM5VIGcNUQLU5q32p3o/YTR0krwveoIUbUIt/HmJkW\noyKozP18SBY9eE8wArOOCryRcwouYFESBrTtbpdWiltNyzwt6JzIsbXoviXLLCHLqeipTUYdLHe7\nhltVy6btqaJUJXpbEHVO0BLzR+fSBBKSMWstDtj6nnX0tHqGCQXReTIfOc48C91zNJsTXODunZrF\n4oiIJjOwLC9S1zVGzYlomrYWcSMVaZsGbTU/+sy7eO/7fhzn4fNfeJYPf/jf0FYVZVmyqStpshsW\nkdFEpeh93GFJxvzAxKsz+SQ5KIpsGsRLEtpuXPIytBUWK7uFzbqj3bSisr5cYstSvDQ8szy1eJNY\nr7QVKUkUIamVxTRjxFnRRC/g/yGXUZYlwQeCd1iTWLzjIM+Q+Ct0RBEwhtHTUDpirCjMWaXJjKZP\nz4mB0hiToZWWjmcNmU7E1k4Eraz2qRLniXh00ETJtWPMnyG49zAR7n186jEMoUTqhzgY51U29seE\nI1FNUJnsPjumhf79fYd7w5Y9L0gpcWHFtbjvMUrvu7LT/E1AFhFaiZYFmqrpOKnW3F03bKqWpnMo\ntb/LHXpIsJuoAzR7gC6bdB55lie+UDE0RV5QlnOh0NOWxXLBsZWu0jdvvYktJG9xtFjS9g1tXaGi\n4q//jZ+n73uuv/oGH//4x7l9+zZlWY7apcO1mbKkTe/PYfVg31OMo0eqtE6CQEk0Ou6wMM45VquV\nyD3CmGMZOCaGKspe4vFwDsU4JiIHMJ+HPZ2PKb3CvveRGNa85Lp8COOcj3iYsHEP3/MQRj59zzhY\nrHPm0P28iB+Ed/FAGAvY/zLnlZqGnIE8v7/AJZy4v6GI4w6VYLAxpJfvbg6kyTIyNk8w+OxAP/c7\n7/NKY3sTXQ34/91nDZTtw/FW6TGDrZTwdXgiaIULHucNWkXwmsZ5IZttWu6ebri7rqk6gXubIkvg\nIT2C24zSslNG2dliVGMisu8EM5Fk6yUUKEuKTIBQ0Udu3b1Jns1TNl4WW0yGRimF74VPZD5fYPuM\n1vX46Hn/+99P0/V89OMf48tf/srOKFhD7Hah3nnl7On/03s0qoRNAHFaKWErSyGiSYs4REm6rtdr\nYhSR47IsxzABtWPcdun9xuSj2m8unLr+0e/ce2uFNLjrOvGSDpC/MK1a7Co8hwZi+L7Tv6dGdG+u\nxd29PH9O7pd2fxDjATEW+wvtfl7G8DsOlP0HSaBd2cqn9xwSXNOk0L1Er9O/jdkxek8LTlMLPz1u\navHvtytFNCruynECANOEsJ/RNhOvAqQM6gOgLV3oaXuPQkts6hWbpmNdtVRtR907+iBwb6M1mTEE\nLwncncue4uloCEF23K7rRg/G9+I+i3izwWAIztH0gcsXLqO1pekcneup1hv6EOmTh5AXOfPZnKPj\nJZ3r2dQbNtWG5XLJ5z//eT796d+jaVuOj4/Z1NVeTmoax0/HeZ7F9PoqtcNlKLM/D7QSY9I0LVVV\nEVHkRcF8Pmc2W4zGNCqVlM08Ku4AVzHGvV6OqQcq5dKdUZ+iOgcjM1TJhk1peqxU+A49ZpCGOJUa\nAg/mmRmwJfd6FIcb1jjvtbrvJvf9jAfCWAzfUYRbhsfuf1FCcuMg7O1IMqbGIACJTh8R+FMHcYYK\nu89RSmryw0t0HMqHjIK29577+YZur+yqkncT46i9eegZqRBRZkAlymO99zR9h9dHBKeSkntAR4PT\nhturDXc3FZve06Hp5Nuz0MIRqlQKWgl4hQgsTYcP4IOosKNQeU6GJjcZVhtJfhLH6oZShi4pnzkX\nCMhOPVclgLj5q0DTtVIqnS+5desWv/6v/iWnp4LklDzFwAK+6+M5dN3PG4fGemxYM+k8E7WhsQp8\noGlaNpsNdVWNfBSLxYIseUzex9F7OK/dHJLcYzq/82gOpo9PAVnioaTcxjjBE49Jel6qLbvXA6MU\nxfR7+hjQ0WNstjfHhIVejfNJ0M9qFJAe3me6Ob6V8UAYC5XKYtOwYP+mDRfofnmN4T+x2MPxhoHj\nIV38JPITDgySjKH0teMV2PNaUKMROa/ctV8r3zcYA5Q8DuS/aJSaAnN2E0rOT7575xzbqiEiep8u\napSPKBVwUXPndM1p3VE5TeMVQVmUtVid7Szw8D2DuCnKJtPpw17eQiklbOJRkRmDzEPROXExcufW\nbWblEVVd4YlYm2N1Yk/XsN5u6HxH1UTavmOxXPDuZ57htz7+MV599VWMtejM0jQNWZHL71RBGK7z\ntAx5/jzZF+kZ/jfGQAwE+rQ4A945qqpis9ngnePoeDaGHsNnhQBRKVFgjwLvHwyQnEMYvYVpfmXI\n7xzmV7Ism/Qi+UnYnL4bkRAcg5QmKkyg5PeKHo9zaHQY1B75z3hvVRQmrGQkzvPQ4v6U/b7GdzQW\nSqkngF8BHkZW4y/HGP93pdT/CPx94FZ66T+KMX4kHfM/AH8PyQH91zHGf/udPmcoGcnPrrKh9U62\nbXJOwH4CNAZhvh7r7EBQKvkSAlRRg8s3zRUAAwhnr6SVkkhSQRmsdKrzT4zSMIyxybX3BzuOgoF1\n24eEPtzFowyLM00QH3YTtG467qzXdESMzmlCoI+eGBWv33qTVetxuqQ30PqALUqOL12mGJJ86dwF\nwScLa1bOaF1P1zc0dUXTNASfaPe0wSIALeUCZVlw4fiYvum5ffMW0TgeedujlPMZt+7e4fadO6AV\n+SyXtvK2wgWPj4GiLFleOOazn3sWl/ohqqrCZHZMqJq06zrn9oBX593fabgyeCVyTf3ociulaJqG\ntqtpq5rtdkvXOeazgosXL088g1R9sYJ6HPIuhc1FyT6FSH3fIjR1Q8lTXtu33QgcUyrSuhajFHmR\njbqo87n0uYTocE5Elo0WQhqUiCHJXE5RyxD66F0uyVpLUZZYst1GlN4zpHk+3bcOQ+rhmqDMKE3w\nVsZ341k44L+LMf47pdQR8AWl1G+l5/63GOP/Mn2xUurHgP8U+HHgUeBjSql3x2+jnzZYXokFe+nB\n10NJavc6raXJLEwW1OiFkNwyhoUou6fNCrqmlbJWEP7Kvt+BaJz3mNR6PEy2Pb1VlMgOao1z+0mo\nqbEYdqfheZiUZM3u/XzKTYQQaBs5xis9TtBhp/QhYLKcvu85XW248NAVTOghdJycnHD3ZE3rPZWH\nJmqy+QyblbgAISlzTXdJhZZEqXN0dUPd1OPnhdBjMwt9xBhFcBGtYLvd4rcdi9mS3BZkWYHvek5T\nmJFlmRiGouD44gW2zZZvvvwS0SicC3z9hedpgyzsgcdy2OHm8/nY7j9FIw6ehnBu6ntYqYf8ymA4\nRkMTAiZxUrZty2olsoPHx8cs54vRUIQQ6L0YIWvz9FtyFiMpkg+ERGQ0CBl3XTe2j0/D4REKHgKh\nF0nELMsI0e0oEdRgbKDvAzH4vUrN8P2zzIzHlGWJTp3Yw3ybQt611mmDjBJ2DevPaIzeeWDeCW3C\nDyLR+R2NRYzxDeCN9PdaKfU14LFvc8gvAP8ixtgCLymlXgR+Cvj97+8UD1FoavIzPJSgvAw54IBA\nGzyaDq0h0waXdn6NGY2FChKze/yeVR7yFUPcMvUGv9dxT/5lCCrv89rhxroQ6GKk7ZwYtRjxLtD2\nTr5LlPJhJEKUZN25bzsEsOzH24c/znlssaAwGRaDjYYYxHjMy5IQIk3T0HmRWez6nqhgu62Fmbvr\npGM1kdpaa2m7/vzzQY27K7DnQUy9h6lXqSb5hf2KiQga19uKutlSVdVonLJMUK9jV/E4n3ahzmH4\nGKNUG4aGQJjMhfuM8Rqm3/k9CdvAjpEV9mfT8Nh3Q9c/6pqRwM7y37n3fbd2wlu3Fd9bzkIp9Q7g\n/cAfAD8D/JJS6u8An0e8jxPEkHx2cthrnGNclFL/APgH6d/N85/7yB3g9vd4/n9a4yp/guf6wlee\nfatv8Sd6vm9x/DCdK/xwne8zb+Xg79pYKKWWwK8C/02McaWU+j+Af4KYyH8C/K/A3+V8+3vPphxj\n/GXglyfv//kY4we/t9P/0xk/TOcKP1zn+8N0rvDDdb5Kqc+/leO/KzEBpVSGGIp/HmP8NYAY440Y\no49SNP4/kVADxJN4YnL448D1t3KSfz7+fPz5+NMf39FYKAkO/xnwtRjjP508/sjkZX8b+Er6+1/B\n/9/e2YNGEYRh+HkRkkIE/1CCWCRikyoEkYCSUs01sdRCU1hYRLCxiKZJq2AjiGARiBaKjZhGUESw\nUkTJrxKTaEBUTCGInRZjMXNyXnYvq7nczOn3wLGbuSkePpbvZmffJRyT1CqpHdgLrHkdbRhGXIrc\nhhwATgDTkibC2AXguKQu/C3GEnAawDk3K+kO8Ar/JGWw1pOQCq6vPiUZmskVmsu3mVyhuXzX5Kp6\nJLsMw/j3Wfs/QDQM478gerOQdETSnKQFSUOxfbKQtCRpWtJEeUdZ0lZJDyXNh+OWSG6jkpYlzVSM\nZbrJcyXUekpSdyK+I5I+hPpOSCpVfHc++M5JOtxg192SHkt6LWlW0tkwnlx9a7jWr7a1Qjrr/cGn\nUBaBDqAFmAQ6YzrleC4B26vGLgFD4XwIuBjJrRfoBmZWcwNKwH384+0e4FkiviPAuYy5neGaaAXa\nw7WyoYGubUB3ON8EvAlOydW3hmvdaht7ZbEfWHDOvXXOfQdu4xOgzUA/MBbOx4CjMSScc0+AL1XD\neW79wA3neQpsrnqqte7k+ObxKw3snHsHlNPADcE598k59zKcfwPK6eXk6lvDNY8/rm3sZrELeF/x\nd2baMwEc8EDSi5A8BdjpfBSecNwRzW4leW4p1/tMWLqPVtzSJeNblV5Our5VrlCn2sZuFoXSnglw\nwDnXDfQBg5J6Ywv9JanW+xqwB+jCv4d0OYwn4VudXq41NWOsob4ZrnWrbexm0RRpT+fcx3BcBu7i\nl2ufy0vMcFyOZ7iCPLck6+0STgNnpZdJtL7rnbSO3SyeA3sltUtqwb/aPh7Z6TckbZR/NR9JG4FD\n+LTqODAQpg0A9+IYZpLnNg6cDLv2PcDX8nI6JqmmgfPSyyRY34YkrRu1W1tjF7eE37ldBIZj+2T4\ndeB3jSeB2bIjsA14BMyH49ZIfrfwy8sf+F+LU3lu+KXn1VDraWBfIr43g89UuIjbKuYPB985oK/B\nrgfxS/MpYCJ8SinWt4Zr3WprCU7DMAoR+zbEMIwmwZqFYRiFsGZhGEYhrFkYhlEIaxaGYRTCmoVh\nGIWwZmEYRiGsWRiGUYifF72swan0/osAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e79db66748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ For human images, 98% of them has a detected face and for dog images this number is equal to 12%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__ In my opinion it is a reasonable expectation as you can not expect good results providing a bad quality input. \n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ This time the accuracy for dog detector is 100% (wow) and for human images, only 1% (1 image) was classified incorrectly as a dog :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                 | 1007/6680 [00:09<00:55, 102.37it/s]C:\\Users\\Ewa\\Anaconda4\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "100%|| 6680/6680 [01:02<00:00, 106.88it/s]\n",
      "100%|| 835/835 [00:07<00:00, 110.29it/s]\n",
      "100%|| 836/836 [00:07<00:00, 113.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ \n",
    "I started with convolutional network similar to the one presented here: https://elitedatascience.com/keras-tutorial-deep-learning-in-python. This CNN was used with MNIST dataset and it aimed to classify the images of digits (10 categories). I have added one additional convolutional layer and adjusted the input and output shape. I decided to give it a try and check the acurracy I will get with this CNN. I trained the model with 100 epochs. The obtained test accuracy was 7.0574%. The accuracy is much better than a random guess and more than 1% as required.\n",
    "architecture #1: convolutional layer (32,3,3) relu (input) + 2 x convolutional layer (32,3,3) relu with maxpooling layer (2,2) and dropout 0.25 + 2 fully connected layers : relu 128, dropout 0.5, softmax 133 (output).\n",
    "\n",
    "I tested another architecture because I was curious if the accuracy will improve if I add an extra convolutional layer (With max pooling layer and dropout as above). Accuracy improved to 12.0813%.\n",
    "\n",
    "Then, I read that I should set all dropout probabilities to 1 when i validate the model. I did that and checked the performance again and it was approximately 0.7%. I was surprised by this change and I researched a bit about dropout layers. First, I found this post https://stats.stackexchange.com/questions/299292/dropout-makes-performance-worse. I decided to drop all dropouts and see how my model is performing now. After all, the CNN is shallow and the dataset size is pretty big (point #2 in the post). Also, I am training only for 20 epochs now so the point #3 probably also applies.\n",
    "This time I obtained test accuracy of 5.3828%\n",
    "\n",
    "I decided to perform one last set and have my CL of increasing depth: 32,64,128, 256 and 512 as recommended in Udacity forum: https://discussions.udacity.com/t/tips-for-image-classification-project/238506. However it lowered the accuracy to 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogInceptionV3Data.npz')\n",
    "train_V3 = bottleneck_features['train']\n",
    "valid_V3 = bottleneck_features['valid']\n",
    "test_V3 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 111, 111, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 111, 111, 32) 96          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 111, 111, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 109, 109, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 109, 109, 32) 96          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 109, 109, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 109, 109, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 109, 109, 64) 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 109, 109, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 54, 54, 80)   240         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 54, 54, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 52, 52, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 52, 52, 192)  576         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 52, 52, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 25, 25, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 25, 25, 48)   9216        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 25, 25, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 48)   144         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 25, 25, 192)  0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 25, 25, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 25, 25, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 25, 25, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 25, 64)   192         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 25, 25, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 25, 25, 32)   96          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 25, 25, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 25, 25, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 25, 25, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 25, 25, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 25, 25, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 25, 25, 48)   144         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 25, 25, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 25, 25, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 25, 25, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 25, 25, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 25, 25, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 25, 25, 64)   192         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 25, 25, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 25, 25, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 25, 25, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 25, 25, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 25, 25, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 25, 25, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 25, 25, 48)   144         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 25, 25, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 25, 25, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 25, 25, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_3[0][0]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 25, 25, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 25, 25, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 25, 25, 64)   192         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 25, 25, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 25, 25, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 25, 25, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 25, 25, 64)   192         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 25, 25, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 25, 25, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 25, 25, 96)   288         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 25, 25, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 12, 12, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 12, 12, 384)  1152        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 12, 12, 96)   288         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 12, 12, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 12, 12, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 12, 12, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 12, 12, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 12, 12, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 12, 12, 128)  384         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 12, 12, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 12, 12, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 12, 12, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 12, 12, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 12, 12, 192)  576         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 12, 12, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 12, 12, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 12, 12, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 12, 12, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 12, 12, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 12, 12, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 12, 12, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 12, 12, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 12, 12, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 12, 12, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 12, 12, 192)  576         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 12, 12, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 12, 12, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 12, 12, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 12, 12, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 12, 12, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 12, 12, 160)  480         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 12, 12, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 12, 12, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 12, 12, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 12, 12, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 12, 12, 192)  576         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 12, 12, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 12, 12, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 12, 12, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 12, 12, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 12, 12, 192)  576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 12, 12, 192)  576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 12, 12, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 12, 12, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 5, 5, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 5, 5, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 5, 5, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 5, 5, 192)    576         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 5, 5, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 5, 5, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 5, 5, 448)    1344        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 5, 5, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 5, 5, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 5, 5, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 5, 5, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 5, 5, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 5, 5, 384)    1152        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 5, 5, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 5, 5, 320)    960         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 5, 5, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 5, 5, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 5, 5, 192)    576         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 5, 5, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 5, 5, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 5, 5, 448)    1344        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 5, 5, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 5, 5, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 5, 5, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 5, 5, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 5, 5, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 5, 5, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 5, 5, 384)    1152        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 5, 5, 384)    1152        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 5, 5, 320)    960         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 5, 5, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 5, 5, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 5, 5, 192)    576         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 5, 5, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 5, 5, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 5, 5, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_86[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 133)          136325      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 24,037,285\n",
      "Trainable params: 24,002,853\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications.inception_v3 import decode_predictions\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "base_model=InceptionV3(weights='imagenet', include_top=False,input_shape=(224,224,3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x) \n",
    "predictions = Dense(133, activation='softmax')(x) \n",
    "model_V3 = Model(inputs=base_model.input, outputs=predictions)\n",
    "print(model_V3.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "from keras import optimizers\n",
    "NB_IV3_LAYERS_TO_FREEZE=2\n",
    "for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:\n",
    "      layer.trainable = False\n",
    "for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "      layer.trainable = True\n",
    "model_V3.compile(optimizer=optimizers.SGD(lr=0.0001, momentum=0.9),   \n",
    "                 loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5940/6680 [=========================>....] - ETA: 3:03:46 - loss: 5.05 - ETA: 2:37:16 - loss: 5.08 - ETA: 2:25:24 - loss: 5.07 - ETA: 2:19:34 - loss: 5.00 - ETA: 2:25:21 - loss: 5.01 - ETA: 2:30:33 - loss: 4.99 - ETA: 2:36:02 - loss: 5.01 - ETA: 2:35:48 - loss: 5.02 - ETA: 2:34:36 - loss: 5.03 - ETA: 2:33:37 - loss: 5.01 - ETA: 2:33:25 - loss: 4.99 - ETA: 2:32:42 - loss: 5.00 - ETA: 2:33:05 - loss: 4.99 - ETA: 2:31:21 - loss: 4.96 - ETA: 2:29:30 - loss: 4.97 - ETA: 2:28:13 - loss: 4.97 - ETA: 2:26:57 - loss: 4.97 - ETA: 2:25:53 - loss: 4.96 - ETA: 2:24:41 - loss: 4.98 - ETA: 2:23:29 - loss: 4.98 - ETA: 2:22:05 - loss: 4.97 - ETA: 2:20:41 - loss: 4.98 - ETA: 2:19:43 - loss: 4.98 - ETA: 2:18:39 - loss: 4.98 - ETA: 2:17:40 - loss: 4.98 - ETA: 2:16:40 - loss: 4.97 - ETA: 2:15:45 - loss: 4.98 - ETA: 2:14:52 - loss: 4.97 - ETA: 2:14:05 - loss: 4.98 - ETA: 2:13:22 - loss: 4.98 - ETA: 2:12:41 - loss: 4.98 - ETA: 2:11:52 - loss: 4.98 - ETA: 2:11:08 - loss: 4.98 - ETA: 2:10:23 - loss: 4.98 - ETA: 2:09:39 - loss: 4.99 - ETA: 2:09:01 - loss: 4.99 - ETA: 2:08:19 - loss: 4.99 - ETA: 2:07:39 - loss: 5.00 - ETA: 2:06:59 - loss: 5.00 - ETA: 2:06:17 - loss: 5.00 - ETA: 2:05:37 - loss: 5.00 - ETA: 2:05:05 - loss: 5.00 - ETA: 2:04:27 - loss: 5.00 - ETA: 2:03:55 - loss: 5.00 - ETA: 2:03:22 - loss: 5.00 - ETA: 2:02:47 - loss: 4.99 - ETA: 2:02:10 - loss: 4.99 - ETA: 2:01:35 - loss: 4.99 - ETA: 2:01:01 - loss: 4.99 - ETA: 2:00:25 - loss: 4.99 - ETA: 1:59:50 - loss: 5.00 - ETA: 1:59:14 - loss: 5.00 - ETA: 1:58:40 - loss: 4.99 - ETA: 1:58:06 - loss: 4.99 - ETA: 1:57:32 - loss: 4.99 - ETA: 1:57:01 - loss: 4.99 - ETA: 1:56:32 - loss: 4.99 - ETA: 1:56:00 - loss: 4.99 - ETA: 1:55:29 - loss: 4.98 - ETA: 1:54:56 - loss: 4.98 - ETA: 1:54:25 - loss: 4.99 - ETA: 1:53:54 - loss: 4.98 - ETA: 1:53:23 - loss: 4.99 - ETA: 1:52:53 - loss: 4.99 - ETA: 1:52:22 - loss: 4.99 - ETA: 1:51:52 - loss: 4.99 - ETA: 1:51:21 - loss: 4.99 - ETA: 1:50:51 - loss: 4.99 - ETA: 1:50:24 - loss: 4.98 - ETA: 1:49:53 - loss: 4.98 - ETA: 1:49:23 - loss: 4.98 - ETA: 1:48:53 - loss: 4.98 - ETA: 1:48:23 - loss: 4.98 - ETA: 1:47:56 - loss: 4.97 - ETA: 1:47:27 - loss: 4.97 - ETA: 1:46:58 - loss: 4.97 - ETA: 1:46:29 - loss: 4.97 - ETA: 1:46:00 - loss: 4.97 - ETA: 1:45:32 - loss: 4.97 - ETA: 1:45:03 - loss: 4.97 - ETA: 1:44:35 - loss: 4.97 - ETA: 1:44:08 - loss: 4.97 - ETA: 1:43:41 - loss: 4.97 - ETA: 1:43:16 - loss: 4.96 - ETA: 1:42:49 - loss: 4.96 - ETA: 1:42:22 - loss: 4.96 - ETA: 1:41:56 - loss: 4.96 - ETA: 1:41:30 - loss: 4.96 - ETA: 1:41:04 - loss: 4.97 - ETA: 1:40:36 - loss: 4.96 - ETA: 1:40:09 - loss: 4.96 - ETA: 1:39:42 - loss: 4.96 - ETA: 1:39:16 - loss: 4.96 - ETA: 1:38:49 - loss: 4.96 - ETA: 1:38:22 - loss: 4.96 - ETA: 1:37:55 - loss: 4.96 - ETA: 1:37:28 - loss: 4.96 - ETA: 1:37:02 - loss: 4.96 - ETA: 1:36:37 - loss: 4.96 - ETA: 1:36:11 - loss: 4.96 - ETA: 1:35:44 - loss: 4.96 - ETA: 1:35:17 - loss: 4.96 - ETA: 1:34:52 - loss: 4.96 - ETA: 1:34:27 - loss: 4.95 - ETA: 1:34:01 - loss: 4.95 - ETA: 1:33:35 - loss: 4.95 - ETA: 1:33:10 - loss: 4.95 - ETA: 1:32:44 - loss: 4.95 - ETA: 1:32:18 - loss: 4.95 - ETA: 1:31:52 - loss: 4.95 - ETA: 1:31:25 - loss: 4.95 - ETA: 1:30:59 - loss: 4.95 - ETA: 1:30:33 - loss: 4.95 - ETA: 1:30:07 - loss: 4.95 - ETA: 1:29:41 - loss: 4.95 - ETA: 1:29:15 - loss: 4.95 - ETA: 1:28:49 - loss: 4.95 - ETA: 1:28:23 - loss: 4.95 - ETA: 1:27:58 - loss: 4.95 - ETA: 1:27:34 - loss: 4.95 - ETA: 1:27:08 - loss: 4.95 - ETA: 1:26:43 - loss: 4.95 - ETA: 1:26:17 - loss: 4.95 - ETA: 1:25:52 - loss: 4.95 - ETA: 1:25:27 - loss: 4.95 - ETA: 1:25:02 - loss: 4.95 - ETA: 1:24:36 - loss: 4.95 - ETA: 1:24:12 - loss: 4.95 - ETA: 1:23:47 - loss: 4.95 - ETA: 1:23:21 - loss: 4.95 - ETA: 1:22:55 - loss: 4.95 - ETA: 1:22:29 - loss: 4.95 - ETA: 1:22:04 - loss: 4.95 - ETA: 1:21:38 - loss: 4.95 - ETA: 1:21:13 - loss: 4.95 - ETA: 1:20:47 - loss: 4.95 - ETA: 1:20:21 - loss: 4.94 - ETA: 1:19:55 - loss: 4.94 - ETA: 1:19:30 - loss: 4.94 - ETA: 1:19:05 - loss: 4.94 - ETA: 1:18:42 - loss: 4.94 - ETA: 1:18:22 - loss: 4.94 - ETA: 1:18:02 - loss: 4.94 - ETA: 1:17:42 - loss: 4.94 - ETA: 1:17:21 - loss: 4.94 - ETA: 1:16:59 - loss: 4.94 - ETA: 1:16:33 - loss: 4.94 - ETA: 1:16:08 - loss: 4.94 - ETA: 1:15:42 - loss: 4.94 - ETA: 1:15:17 - loss: 4.94 - ETA: 1:14:51 - loss: 4.93 - ETA: 1:14:26 - loss: 4.93 - ETA: 1:14:00 - loss: 4.93 - ETA: 1:13:35 - loss: 4.93 - ETA: 1:13:09 - loss: 4.93 - ETA: 1:12:45 - loss: 4.93 - ETA: 1:12:20 - loss: 4.93 - ETA: 1:11:55 - loss: 4.93 - ETA: 1:11:29 - loss: 4.93 - ETA: 1:11:04 - loss: 4.93 - ETA: 1:10:38 - loss: 4.93 - ETA: 1:10:13 - loss: 4.93 - ETA: 1:09:48 - loss: 4.93 - ETA: 1:09:23 - loss: 4.93 - ETA: 1:08:58 - loss: 4.93 - ETA: 1:08:33 - loss: 4.93 - ETA: 1:08:08 - loss: 4.93 - ETA: 1:07:43 - loss: 4.93 - ETA: 1:07:18 - loss: 4.92 - ETA: 1:06:52 - loss: 4.92 - ETA: 1:06:27 - loss: 4.92 - ETA: 1:06:02 - loss: 4.92 - ETA: 1:05:37 - loss: 4.92 - ETA: 1:05:13 - loss: 4.92 - ETA: 1:04:47 - loss: 4.92 - ETA: 1:04:22 - loss: 4.92 - ETA: 1:03:57 - loss: 4.92 - ETA: 1:03:33 - loss: 4.92 - ETA: 1:03:08 - loss: 4.92 - ETA: 1:02:43 - loss: 4.92 - ETA: 1:02:18 - loss: 4.92 - ETA: 1:01:53 - loss: 4.91 - ETA: 1:01:28 - loss: 4.91 - ETA: 1:01:03 - loss: 4.91 - ETA: 1:00:38 - loss: 4.91 - ETA: 1:00:14 - loss: 4.91 - ETA: 59:48 - loss: 4.9163 - ETA: 59:24 - loss: 4.91 - ETA: 58:59 - loss: 4.91 - ETA: 58:34 - loss: 4.91 - ETA: 58:09 - loss: 4.91 - ETA: 57:44 - loss: 4.91 - ETA: 57:19 - loss: 4.91 - ETA: 56:55 - loss: 4.91 - ETA: 56:30 - loss: 4.91 - ETA: 56:06 - loss: 4.91 - ETA: 55:41 - loss: 4.91 - ETA: 55:16 - loss: 4.91 - ETA: 54:51 - loss: 4.91 - ETA: 54:26 - loss: 4.91 - ETA: 54:01 - loss: 4.91 - ETA: 53:36 - loss: 4.91 - ETA: 53:12 - loss: 4.90 - ETA: 52:47 - loss: 4.90 - ETA: 52:22 - loss: 4.90 - ETA: 51:57 - loss: 4.90 - ETA: 51:33 - loss: 4.90 - ETA: 51:08 - loss: 4.90 - ETA: 50:44 - loss: 4.90 - ETA: 50:19 - loss: 4.90 - ETA: 49:54 - loss: 4.90 - ETA: 49:29 - loss: 4.90 - ETA: 49:05 - loss: 4.90 - ETA: 48:40 - loss: 4.90 - ETA: 48:15 - loss: 4.90 - ETA: 47:51 - loss: 4.90 - ETA: 47:26 - loss: 4.90 - ETA: 47:01 - loss: 4.90 - ETA: 46:37 - loss: 4.90 - ETA: 46:12 - loss: 4.90 - ETA: 45:48 - loss: 4.90 - ETA: 45:23 - loss: 4.90 - ETA: 44:59 - loss: 4.90 - ETA: 44:34 - loss: 4.90 - ETA: 44:10 - loss: 4.90 - ETA: 43:45 - loss: 4.90 - ETA: 43:20 - loss: 4.89 - ETA: 42:56 - loss: 4.89 - ETA: 42:32 - loss: 4.89 - ETA: 42:07 - loss: 4.89 - ETA: 41:42 - loss: 4.89 - ETA: 41:18 - loss: 4.89 - ETA: 40:54 - loss: 4.89 - ETA: 40:29 - loss: 4.89 - ETA: 40:05 - loss: 4.89 - ETA: 39:40 - loss: 4.89 - ETA: 39:16 - loss: 4.89 - ETA: 38:51 - loss: 4.89 - ETA: 38:27 - loss: 4.89 - ETA: 38:02 - loss: 4.89 - ETA: 37:38 - loss: 4.89 - ETA: 37:13 - loss: 4.89 - ETA: 36:49 - loss: 4.89 - ETA: 36:25 - loss: 4.89 - ETA: 36:00 - loss: 4.89 - ETA: 35:36 - loss: 4.89 - ETA: 35:11 - loss: 4.89 - ETA: 34:47 - loss: 4.88 - ETA: 34:23 - loss: 4.88 - ETA: 33:58 - loss: 4.88 - ETA: 33:34 - loss: 4.88 - ETA: 33:10 - loss: 4.88 - ETA: 32:45 - loss: 4.88 - ETA: 32:21 - loss: 4.88 - ETA: 31:57 - loss: 4.88 - ETA: 31:33 - loss: 4.88 - ETA: 31:08 - loss: 4.88 - ETA: 30:44 - loss: 4.88 - ETA: 30:20 - loss: 4.88 - ETA: 29:55 - loss: 4.88 - ETA: 29:31 - loss: 4.88 - ETA: 29:06 - loss: 4.88 - ETA: 28:42 - loss: 4.88 - ETA: 28:18 - loss: 4.88 - ETA: 27:53 - loss: 4.88 - ETA: 27:29 - loss: 4.88 - ETA: 27:05 - loss: 4.87 - ETA: 26:40 - loss: 4.87 - ETA: 26:16 - loss: 4.87 - ETA: 25:52 - loss: 4.87 - ETA: 25:28 - loss: 4.87 - ETA: 25:03 - loss: 4.87 - ETA: 24:39 - loss: 4.87 - ETA: 24:15 - loss: 4.87 - ETA: 23:51 - loss: 4.87 - ETA: 23:26 - loss: 4.87 - ETA: 23:02 - loss: 4.87 - ETA: 22:37 - loss: 4.87 - ETA: 22:13 - loss: 4.87 - ETA: 21:49 - loss: 4.87 - ETA: 21:24 - loss: 4.87 - ETA: 21:00 - loss: 4.87 - ETA: 20:36 - loss: 4.86 - ETA: 20:11 - loss: 4.86 - ETA: 19:47 - loss: 4.86 - ETA: 19:23 - loss: 4.86 - ETA: 18:58 - loss: 4.86 - ETA: 18:34 - loss: 4.86 - ETA: 18:10 - loss: 4.86 - ETA: 17:46 - loss: 4.86 - ETA: 17:21 - loss: 4.86 - ETA: 16:57 - loss: 4.86 - ETA: 16:33 - loss: 4.86 - ETA: 16:09 - loss: 4.86 - ETA: 15:44 - loss: 4.86 - ETA: 15:20 - loss: 4.86 - ETA: 14:56 - loss: 4.866660/6680 [============================>.] - ETA: 14:31 - loss: 4.86 - ETA: 14:07 - loss: 4.86 - ETA: 13:43 - loss: 4.86 - ETA: 13:19 - loss: 4.86 - ETA: 12:54 - loss: 4.86 - ETA: 12:30 - loss: 4.86 - ETA: 12:06 - loss: 4.86 - ETA: 11:42 - loss: 4.86 - ETA: 11:17 - loss: 4.86 - ETA: 10:53 - loss: 4.86 - ETA: 10:29 - loss: 4.86 - ETA: 10:05 - loss: 4.86 - ETA: 9:41 - loss: 4.8600 - ETA: 9:16 - loss: 4.859 - ETA: 8:52 - loss: 4.859 - ETA: 8:28 - loss: 4.859 - ETA: 8:04 - loss: 4.859 - ETA: 7:39 - loss: 4.858 - ETA: 7:15 - loss: 4.857 - ETA: 6:51 - loss: 4.857 - ETA: 6:27 - loss: 4.857 - ETA: 6:02 - loss: 4.856 - ETA: 5:38 - loss: 4.855 - ETA: 5:14 - loss: 4.855 - ETA: 4:50 - loss: 4.855 - ETA: 4:26 - loss: 4.854 - ETA: 4:02 - loss: 4.853 - ETA: 3:37 - loss: 4.852 - ETA: 3:13 - loss: 4.852 - ETA: 2:49 - loss: 4.851 - ETA: 2:25 - loss: 4.850 - ETA: 2:01 - loss: 4.850 - ETA: 1:36 - loss: 4.849 - ETA: 1:12 - loss: 4.849 - ETA: 48s - loss: 4.849 - ETA: 24s - loss: 4.8487\n",
      "Epoch 00001: val_loss improved from inf to 4.66748, saving model to saved_models/weights.best.Model_V3.hdf5\n",
      "6680/6680 [==============================] - 8468s 1s/step - loss: 4.8487 - val_loss: 4.6675\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6680 [=========================>....] - ETA: 2:14:59 - loss: 4.66 - ETA: 2:13:37 - loss: 4.66 - ETA: 2:12:29 - loss: 4.70 - ETA: 2:12:21 - loss: 4.70 - ETA: 2:11:51 - loss: 4.70 - ETA: 2:11:15 - loss: 4.70 - ETA: 2:10:39 - loss: 4.68 - ETA: 2:09:53 - loss: 4.66 - ETA: 2:09:41 - loss: 4.65 - ETA: 2:09:05 - loss: 4.65 - ETA: 2:08:37 - loss: 4.65 - ETA: 2:08:11 - loss: 4.65 - ETA: 2:07:43 - loss: 4.64 - ETA: 2:07:24 - loss: 4.63 - ETA: 2:06:57 - loss: 4.63 - ETA: 2:06:28 - loss: 4.63 - ETA: 2:05:57 - loss: 4.63 - ETA: 2:05:34 - loss: 4.63 - ETA: 2:05:15 - loss: 4.63 - ETA: 2:04:47 - loss: 4.64 - ETA: 2:04:25 - loss: 4.64 - ETA: 2:04:09 - loss: 4.64 - ETA: 2:03:48 - loss: 4.64 - ETA: 2:03:28 - loss: 4.64 - ETA: 2:03:04 - loss: 4.64 - ETA: 2:02:41 - loss: 4.63 - ETA: 2:02:17 - loss: 4.63 - ETA: 2:01:53 - loss: 4.63 - ETA: 2:01:31 - loss: 4.63 - ETA: 2:01:07 - loss: 4.62 - ETA: 2:00:43 - loss: 4.63 - ETA: 2:00:27 - loss: 4.63 - ETA: 2:00:03 - loss: 4.63 - ETA: 1:59:39 - loss: 4.62 - ETA: 1:59:20 - loss: 4.63 - ETA: 1:58:59 - loss: 4.63 - ETA: 1:58:39 - loss: 4.63 - ETA: 1:58:16 - loss: 4.63 - ETA: 1:57:53 - loss: 4.63 - ETA: 1:57:27 - loss: 4.62 - ETA: 1:57:02 - loss: 4.62 - ETA: 1:56:39 - loss: 4.62 - ETA: 1:56:11 - loss: 4.62 - ETA: 1:55:49 - loss: 4.62 - ETA: 1:55:23 - loss: 4.62 - ETA: 1:54:59 - loss: 4.62 - ETA: 1:54:35 - loss: 4.62 - ETA: 1:54:09 - loss: 4.61 - ETA: 1:53:47 - loss: 4.61 - ETA: 1:53:24 - loss: 4.62 - ETA: 1:53:01 - loss: 4.62 - ETA: 1:52:34 - loss: 4.62 - ETA: 1:52:09 - loss: 4.62 - ETA: 1:51:45 - loss: 4.62 - ETA: 1:51:20 - loss: 4.62 - ETA: 1:50:55 - loss: 4.62 - ETA: 1:50:33 - loss: 4.62 - ETA: 1:50:10 - loss: 4.63 - ETA: 1:49:47 - loss: 4.62 - ETA: 1:49:23 - loss: 4.63 - ETA: 1:48:59 - loss: 4.63 - ETA: 1:48:34 - loss: 4.62 - ETA: 1:48:11 - loss: 4.62 - ETA: 1:47:51 - loss: 4.62 - ETA: 1:47:26 - loss: 4.62 - ETA: 1:47:01 - loss: 4.62 - ETA: 1:46:37 - loss: 4.62 - ETA: 1:46:12 - loss: 4.62 - ETA: 1:45:51 - loss: 4.62 - ETA: 1:45:27 - loss: 4.61 - ETA: 1:45:02 - loss: 4.61 - ETA: 1:44:38 - loss: 4.61 - ETA: 1:44:15 - loss: 4.61 - ETA: 1:43:52 - loss: 4.61 - ETA: 1:43:29 - loss: 4.61 - ETA: 1:43:04 - loss: 4.61 - ETA: 1:42:40 - loss: 4.61 - ETA: 1:42:16 - loss: 4.61 - ETA: 1:41:54 - loss: 4.61 - ETA: 1:41:28 - loss: 4.60 - ETA: 1:41:03 - loss: 4.60 - ETA: 1:40:39 - loss: 4.61 - ETA: 1:40:15 - loss: 4.60 - ETA: 1:39:51 - loss: 4.60 - ETA: 1:39:27 - loss: 4.60 - ETA: 1:39:02 - loss: 4.60 - ETA: 1:38:37 - loss: 4.60 - ETA: 1:38:12 - loss: 4.60 - ETA: 1:37:48 - loss: 4.60 - ETA: 1:37:23 - loss: 4.60 - ETA: 1:36:58 - loss: 4.60 - ETA: 1:36:34 - loss: 4.60 - ETA: 1:36:11 - loss: 4.60 - ETA: 1:35:47 - loss: 4.60 - ETA: 1:35:23 - loss: 4.60 - ETA: 1:34:58 - loss: 4.60 - ETA: 1:34:33 - loss: 4.59 - ETA: 1:34:08 - loss: 4.59 - ETA: 1:33:44 - loss: 4.59 - ETA: 1:33:20 - loss: 4.59 - ETA: 1:32:55 - loss: 4.59 - ETA: 1:32:31 - loss: 4.59 - ETA: 1:32:06 - loss: 4.58 - ETA: 1:31:43 - loss: 4.58 - ETA: 1:31:19 - loss: 4.58 - ETA: 1:30:55 - loss: 4.58 - ETA: 1:30:32 - loss: 4.58 - ETA: 1:30:09 - loss: 4.58 - ETA: 1:29:46 - loss: 4.58 - ETA: 1:29:21 - loss: 4.58 - ETA: 1:28:56 - loss: 4.58 - ETA: 1:28:32 - loss: 4.58 - ETA: 1:28:07 - loss: 4.58 - ETA: 1:27:43 - loss: 4.58 - ETA: 1:27:19 - loss: 4.58 - ETA: 1:26:55 - loss: 4.58 - ETA: 1:26:30 - loss: 4.57 - ETA: 1:26:06 - loss: 4.58 - ETA: 1:25:42 - loss: 4.58 - ETA: 1:25:19 - loss: 4.58 - ETA: 1:24:55 - loss: 4.57 - ETA: 1:24:30 - loss: 4.57 - ETA: 1:24:06 - loss: 4.57 - ETA: 1:23:42 - loss: 4.57 - ETA: 1:23:18 - loss: 4.57 - ETA: 1:22:54 - loss: 4.57 - ETA: 1:22:29 - loss: 4.57 - ETA: 1:22:04 - loss: 4.57 - ETA: 1:21:40 - loss: 4.57 - ETA: 1:21:16 - loss: 4.57 - ETA: 1:20:53 - loss: 4.57 - ETA: 1:20:29 - loss: 4.57 - ETA: 1:20:05 - loss: 4.57 - ETA: 1:19:41 - loss: 4.57 - ETA: 1:19:17 - loss: 4.57 - ETA: 1:18:53 - loss: 4.57 - ETA: 1:18:28 - loss: 4.57 - ETA: 1:18:04 - loss: 4.57 - ETA: 1:17:40 - loss: 4.56 - ETA: 1:17:16 - loss: 4.56 - ETA: 1:16:52 - loss: 4.56 - ETA: 1:16:27 - loss: 4.56 - ETA: 1:16:03 - loss: 4.56 - ETA: 1:15:40 - loss: 4.56 - ETA: 1:15:18 - loss: 4.56 - ETA: 1:14:54 - loss: 4.56 - ETA: 1:14:30 - loss: 4.56 - ETA: 1:14:06 - loss: 4.56 - ETA: 1:13:42 - loss: 4.56 - ETA: 1:13:18 - loss: 4.56 - ETA: 1:12:54 - loss: 4.56 - ETA: 1:12:30 - loss: 4.56 - ETA: 1:12:06 - loss: 4.56 - ETA: 1:11:42 - loss: 4.56 - ETA: 1:11:18 - loss: 4.56 - ETA: 1:10:54 - loss: 4.56 - ETA: 1:10:30 - loss: 4.56 - ETA: 1:10:05 - loss: 4.56 - ETA: 1:09:41 - loss: 4.56 - ETA: 1:09:17 - loss: 4.56 - ETA: 1:08:53 - loss: 4.56 - ETA: 1:08:30 - loss: 4.56 - ETA: 1:08:05 - loss: 4.56 - ETA: 1:07:41 - loss: 4.56 - ETA: 1:07:17 - loss: 4.56 - ETA: 1:06:53 - loss: 4.55 - ETA: 1:06:29 - loss: 4.55 - ETA: 1:06:05 - loss: 4.55 - ETA: 1:05:41 - loss: 4.55 - ETA: 1:05:17 - loss: 4.55 - ETA: 1:04:53 - loss: 4.55 - ETA: 1:04:29 - loss: 4.55 - ETA: 1:04:04 - loss: 4.55 - ETA: 1:03:40 - loss: 4.55 - ETA: 1:03:16 - loss: 4.55 - ETA: 1:02:53 - loss: 4.55 - ETA: 1:02:29 - loss: 4.55 - ETA: 1:02:05 - loss: 4.55 - ETA: 1:01:41 - loss: 4.55 - ETA: 1:01:17 - loss: 4.55 - ETA: 1:00:53 - loss: 4.55 - ETA: 1:00:29 - loss: 4.54 - ETA: 1:00:05 - loss: 4.54 - ETA: 59:41 - loss: 4.5466 - ETA: 59:18 - loss: 4.54 - ETA: 58:54 - loss: 4.54 - ETA: 58:30 - loss: 4.54 - ETA: 58:06 - loss: 4.54 - ETA: 57:42 - loss: 4.54 - ETA: 57:18 - loss: 4.54 - ETA: 56:54 - loss: 4.54 - ETA: 56:30 - loss: 4.54 - ETA: 56:06 - loss: 4.54 - ETA: 55:42 - loss: 4.53 - ETA: 55:18 - loss: 4.53 - ETA: 54:54 - loss: 4.53 - ETA: 54:30 - loss: 4.53 - ETA: 54:06 - loss: 4.53 - ETA: 53:42 - loss: 4.53 - ETA: 53:19 - loss: 4.53 - ETA: 52:55 - loss: 4.53 - ETA: 52:31 - loss: 4.53 - ETA: 52:07 - loss: 4.53 - ETA: 51:43 - loss: 4.53 - ETA: 51:20 - loss: 4.53 - ETA: 50:56 - loss: 4.53 - ETA: 50:31 - loss: 4.53 - ETA: 50:07 - loss: 4.53 - ETA: 49:43 - loss: 4.53 - ETA: 49:20 - loss: 4.53 - ETA: 48:56 - loss: 4.53 - ETA: 48:32 - loss: 4.53 - ETA: 48:08 - loss: 4.53 - ETA: 47:44 - loss: 4.53 - ETA: 47:20 - loss: 4.53 - ETA: 46:56 - loss: 4.53 - ETA: 46:32 - loss: 4.52 - ETA: 46:08 - loss: 4.52 - ETA: 45:44 - loss: 4.52 - ETA: 45:20 - loss: 4.52 - ETA: 44:57 - loss: 4.52 - ETA: 44:33 - loss: 4.52 - ETA: 44:09 - loss: 4.52 - ETA: 43:46 - loss: 4.52 - ETA: 43:22 - loss: 4.52 - ETA: 42:58 - loss: 4.52 - ETA: 42:34 - loss: 4.52 - ETA: 42:10 - loss: 4.52 - ETA: 41:46 - loss: 4.52 - ETA: 41:23 - loss: 4.52 - ETA: 40:59 - loss: 4.52 - ETA: 40:35 - loss: 4.51 - ETA: 40:11 - loss: 4.51 - ETA: 39:47 - loss: 4.51 - ETA: 39:23 - loss: 4.51 - ETA: 38:59 - loss: 4.51 - ETA: 38:35 - loss: 4.51 - ETA: 38:11 - loss: 4.51 - ETA: 37:47 - loss: 4.51 - ETA: 37:24 - loss: 4.51 - ETA: 37:00 - loss: 4.51 - ETA: 36:36 - loss: 4.51 - ETA: 36:12 - loss: 4.51 - ETA: 35:48 - loss: 4.51 - ETA: 35:24 - loss: 4.51 - ETA: 35:01 - loss: 4.51 - ETA: 34:37 - loss: 4.51 - ETA: 34:13 - loss: 4.51 - ETA: 33:49 - loss: 4.51 - ETA: 33:25 - loss: 4.51 - ETA: 33:01 - loss: 4.51 - ETA: 32:37 - loss: 4.51 - ETA: 32:13 - loss: 4.50 - ETA: 31:49 - loss: 4.50 - ETA: 31:25 - loss: 4.50 - ETA: 31:01 - loss: 4.50 - ETA: 30:37 - loss: 4.50 - ETA: 30:14 - loss: 4.50 - ETA: 29:50 - loss: 4.50 - ETA: 29:26 - loss: 4.50 - ETA: 29:02 - loss: 4.50 - ETA: 28:38 - loss: 4.50 - ETA: 28:14 - loss: 4.50 - ETA: 27:50 - loss: 4.50 - ETA: 27:26 - loss: 4.50 - ETA: 27:03 - loss: 4.50 - ETA: 26:39 - loss: 4.50 - ETA: 26:15 - loss: 4.50 - ETA: 25:51 - loss: 4.50 - ETA: 25:27 - loss: 4.50 - ETA: 25:03 - loss: 4.49 - ETA: 24:40 - loss: 4.49 - ETA: 24:16 - loss: 4.49 - ETA: 23:52 - loss: 4.49 - ETA: 23:28 - loss: 4.49 - ETA: 23:04 - loss: 4.49 - ETA: 22:40 - loss: 4.49 - ETA: 22:16 - loss: 4.49 - ETA: 21:52 - loss: 4.49 - ETA: 21:29 - loss: 4.49 - ETA: 21:05 - loss: 4.49 - ETA: 20:41 - loss: 4.49 - ETA: 20:17 - loss: 4.49 - ETA: 19:54 - loss: 4.49 - ETA: 19:30 - loss: 4.49 - ETA: 19:07 - loss: 4.49 - ETA: 18:43 - loss: 4.49 - ETA: 18:19 - loss: 4.49 - ETA: 17:55 - loss: 4.49 - ETA: 17:31 - loss: 4.49 - ETA: 17:07 - loss: 4.49 - ETA: 16:44 - loss: 4.49 - ETA: 16:20 - loss: 4.48 - ETA: 15:56 - loss: 4.48 - ETA: 15:32 - loss: 4.48 - ETA: 15:08 - loss: 4.48 - ETA: 14:44 - loss: 4.48 - ETA: 14:20 - loss: 4.48 - ETA: 13:56 - loss: 4.48 - ETA: 13:32 - loss: 4.4870"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 13:09 - loss: 4.48 - ETA: 12:45 - loss: 4.48 - ETA: 12:21 - loss: 4.48 - ETA: 11:57 - loss: 4.48 - ETA: 11:33 - loss: 4.48 - ETA: 11:09 - loss: 4.48 - ETA: 10:45 - loss: 4.48 - ETA: 10:21 - loss: 4.48 - ETA: 9:57 - loss: 4.4818 - ETA: 9:33 - loss: 4.480 - ETA: 9:10 - loss: 4.479 - ETA: 8:46 - loss: 4.478 - ETA: 8:22 - loss: 4.477 - ETA: 7:58 - loss: 4.477 - ETA: 7:34 - loss: 4.476 - ETA: 7:10 - loss: 4.476 - ETA: 6:46 - loss: 4.476 - ETA: 6:22 - loss: 4.475 - ETA: 5:58 - loss: 4.475 - ETA: 5:34 - loss: 4.474 - ETA: 5:10 - loss: 4.472 - ETA: 4:46 - loss: 4.472 - ETA: 4:23 - loss: 4.472 - ETA: 3:59 - loss: 4.471 - ETA: 3:35 - loss: 4.471 - ETA: 3:11 - loss: 4.470 - ETA: 2:47 - loss: 4.470 - ETA: 2:23 - loss: 4.469 - ETA: 1:59 - loss: 4.468 - ETA: 1:35 - loss: 4.467 - ETA: 1:11 - loss: 4.468 - ETA: 47s - loss: 4.466 - ETA: 23s - loss: 4.4666\n",
      "Epoch 00002: val_loss improved from 4.66748 to 4.24397, saving model to saved_models/weights.best.Model_V3.hdf5\n",
      "6680/6680 [==============================] - 8354s 1s/step - loss: 4.4667 - val_loss: 4.2440\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6680 [=========================>....] - ETA: 2:18:15 - loss: 4.15 - ETA: 2:14:31 - loss: 4.11 - ETA: 2:13:31 - loss: 4.16 - ETA: 2:14:08 - loss: 4.16 - ETA: 2:13:27 - loss: 4.17 - ETA: 2:12:53 - loss: 4.19 - ETA: 2:12:07 - loss: 4.21 - ETA: 2:11:22 - loss: 4.21 - ETA: 2:10:57 - loss: 4.22 - ETA: 2:10:42 - loss: 4.22 - ETA: 2:10:16 - loss: 4.23 - ETA: 2:09:46 - loss: 4.23 - ETA: 2:09:05 - loss: 4.24 - ETA: 2:08:34 - loss: 4.24 - ETA: 2:08:05 - loss: 4.24 - ETA: 2:07:42 - loss: 4.23 - ETA: 2:07:13 - loss: 4.24 - ETA: 2:06:43 - loss: 4.24 - ETA: 2:06:23 - loss: 4.25 - ETA: 2:06:02 - loss: 4.24 - ETA: 2:05:54 - loss: 4.24 - ETA: 2:05:39 - loss: 4.22 - ETA: 2:05:18 - loss: 4.22 - ETA: 2:04:53 - loss: 4.22 - ETA: 2:05:20 - loss: 4.21 - ETA: 2:05:04 - loss: 4.21 - ETA: 2:04:48 - loss: 4.23 - ETA: 2:04:21 - loss: 4.23 - ETA: 2:03:54 - loss: 4.23 - ETA: 2:03:30 - loss: 4.24 - ETA: 2:03:03 - loss: 4.24 - ETA: 2:02:36 - loss: 4.24 - ETA: 2:02:06 - loss: 4.24 - ETA: 2:01:37 - loss: 4.23 - ETA: 2:01:12 - loss: 4.22 - ETA: 2:00:45 - loss: 4.22 - ETA: 2:00:17 - loss: 4.22 - ETA: 1:59:47 - loss: 4.21 - ETA: 1:59:19 - loss: 4.22 - ETA: 1:58:54 - loss: 4.22 - ETA: 1:58:29 - loss: 4.22 - ETA: 1:58:00 - loss: 4.21 - ETA: 1:57:32 - loss: 4.21 - ETA: 1:57:05 - loss: 4.22 - ETA: 1:56:42 - loss: 4.22 - ETA: 1:56:18 - loss: 4.21 - ETA: 1:55:51 - loss: 4.21 - ETA: 1:55:23 - loss: 4.21 - ETA: 1:54:56 - loss: 4.21 - ETA: 1:54:27 - loss: 4.21 - ETA: 1:53:59 - loss: 4.21 - ETA: 1:53:35 - loss: 4.21 - ETA: 1:53:09 - loss: 4.21 - ETA: 1:52:44 - loss: 4.21 - ETA: 1:52:18 - loss: 4.21 - ETA: 1:51:53 - loss: 4.21 - ETA: 1:51:25 - loss: 4.21 - ETA: 1:51:03 - loss: 4.21 - ETA: 1:50:40 - loss: 4.21 - ETA: 1:50:17 - loss: 4.21 - ETA: 1:49:53 - loss: 4.21 - ETA: 1:49:27 - loss: 4.21 - ETA: 1:49:01 - loss: 4.22 - ETA: 1:48:35 - loss: 4.22 - ETA: 1:48:10 - loss: 4.21 - ETA: 1:47:45 - loss: 4.21 - ETA: 1:47:19 - loss: 4.22 - ETA: 1:46:53 - loss: 4.21 - ETA: 1:46:28 - loss: 4.21 - ETA: 1:46:03 - loss: 4.21 - ETA: 1:45:39 - loss: 4.22 - ETA: 1:45:14 - loss: 4.22 - ETA: 1:44:49 - loss: 4.21 - ETA: 1:44:22 - loss: 4.22 - ETA: 1:43:58 - loss: 4.21 - ETA: 1:43:33 - loss: 4.21 - ETA: 1:43:08 - loss: 4.21 - ETA: 1:42:42 - loss: 4.21 - ETA: 1:42:17 - loss: 4.21 - ETA: 1:41:52 - loss: 4.20 - ETA: 1:41:26 - loss: 4.20 - ETA: 1:41:01 - loss: 4.20 - ETA: 1:40:36 - loss: 4.20 - ETA: 1:40:10 - loss: 4.20 - ETA: 1:39:46 - loss: 4.20 - ETA: 1:39:23 - loss: 4.19 - ETA: 1:38:58 - loss: 4.19 - ETA: 1:38:33 - loss: 4.19 - ETA: 1:38:07 - loss: 4.20 - ETA: 1:37:41 - loss: 4.20 - ETA: 1:37:15 - loss: 4.20 - ETA: 1:36:51 - loss: 4.19 - ETA: 1:36:26 - loss: 4.19 - ETA: 1:36:01 - loss: 4.19 - ETA: 1:35:36 - loss: 4.19 - ETA: 1:35:13 - loss: 4.19 - ETA: 1:34:50 - loss: 4.19 - ETA: 1:34:26 - loss: 4.19 - ETA: 1:34:02 - loss: 4.19 - ETA: 1:33:38 - loss: 4.19 - ETA: 1:33:14 - loss: 4.19 - ETA: 1:32:51 - loss: 4.19 - ETA: 1:32:26 - loss: 4.18 - ETA: 1:32:01 - loss: 4.18 - ETA: 1:31:37 - loss: 4.18 - ETA: 1:31:14 - loss: 4.18 - ETA: 1:30:49 - loss: 4.18 - ETA: 1:30:25 - loss: 4.18 - ETA: 1:30:00 - loss: 4.18 - ETA: 1:29:36 - loss: 4.18 - ETA: 1:29:12 - loss: 4.18 - ETA: 1:28:47 - loss: 4.18 - ETA: 1:28:23 - loss: 4.18 - ETA: 1:27:58 - loss: 4.18 - ETA: 1:27:33 - loss: 4.18 - ETA: 1:27:10 - loss: 4.18 - ETA: 1:26:45 - loss: 4.17 - ETA: 1:26:20 - loss: 4.17 - ETA: 1:25:56 - loss: 4.17 - ETA: 1:25:32 - loss: 4.17 - ETA: 1:25:08 - loss: 4.17 - ETA: 1:24:44 - loss: 4.16 - ETA: 1:24:20 - loss: 4.16 - ETA: 1:23:55 - loss: 4.16 - ETA: 1:23:31 - loss: 4.16 - ETA: 1:23:08 - loss: 4.16 - ETA: 1:22:43 - loss: 4.16 - ETA: 1:22:20 - loss: 4.16 - ETA: 1:21:56 - loss: 4.15 - ETA: 1:21:32 - loss: 4.15 - ETA: 1:21:08 - loss: 4.15 - ETA: 1:20:44 - loss: 4.15 - ETA: 1:20:20 - loss: 4.15 - ETA: 1:19:56 - loss: 4.15 - ETA: 1:19:31 - loss: 4.15 - ETA: 1:19:08 - loss: 4.15 - ETA: 1:18:43 - loss: 4.15 - ETA: 1:18:18 - loss: 4.15 - ETA: 1:17:54 - loss: 4.15 - ETA: 1:17:29 - loss: 4.14 - ETA: 1:17:06 - loss: 4.14 - ETA: 1:16:42 - loss: 4.14 - ETA: 1:16:18 - loss: 4.14 - ETA: 1:15:54 - loss: 4.14 - ETA: 1:15:29 - loss: 4.14 - ETA: 1:15:06 - loss: 4.14 - ETA: 1:14:41 - loss: 4.14 - ETA: 1:14:17 - loss: 4.14 - ETA: 1:13:53 - loss: 4.14 - ETA: 1:13:28 - loss: 4.14 - ETA: 1:13:05 - loss: 4.13 - ETA: 1:12:40 - loss: 4.14 - ETA: 1:12:16 - loss: 4.13 - ETA: 1:11:52 - loss: 4.13 - ETA: 1:11:28 - loss: 4.13 - ETA: 1:11:04 - loss: 4.13 - ETA: 1:10:40 - loss: 4.13 - ETA: 1:10:16 - loss: 4.13 - ETA: 1:09:52 - loss: 4.13 - ETA: 1:09:28 - loss: 4.13 - ETA: 1:09:04 - loss: 4.13 - ETA: 1:08:40 - loss: 4.13 - ETA: 1:08:16 - loss: 4.13 - ETA: 1:07:51 - loss: 4.13 - ETA: 1:07:27 - loss: 4.13 - ETA: 1:07:04 - loss: 4.13 - ETA: 1:06:40 - loss: 4.13 - ETA: 1:06:15 - loss: 4.13 - ETA: 1:05:52 - loss: 4.12 - ETA: 1:05:28 - loss: 4.12 - ETA: 1:05:05 - loss: 4.12 - ETA: 1:04:41 - loss: 4.12 - ETA: 1:04:17 - loss: 4.12 - ETA: 1:03:53 - loss: 4.12 - ETA: 1:03:28 - loss: 4.12 - ETA: 1:03:05 - loss: 4.12 - ETA: 1:02:41 - loss: 4.12 - ETA: 1:02:17 - loss: 4.12 - ETA: 1:01:53 - loss: 4.12 - ETA: 1:01:29 - loss: 4.12 - ETA: 1:01:05 - loss: 4.12 - ETA: 1:00:41 - loss: 4.12 - ETA: 1:00:17 - loss: 4.12 - ETA: 59:53 - loss: 4.1238 - ETA: 59:29 - loss: 4.12 - ETA: 59:05 - loss: 4.12 - ETA: 58:41 - loss: 4.12 - ETA: 58:17 - loss: 4.12 - ETA: 57:53 - loss: 4.12 - ETA: 57:29 - loss: 4.12 - ETA: 57:05 - loss: 4.11 - ETA: 56:42 - loss: 4.11 - ETA: 56:18 - loss: 4.11 - ETA: 55:54 - loss: 4.11 - ETA: 55:30 - loss: 4.11 - ETA: 55:06 - loss: 4.11 - ETA: 54:42 - loss: 4.11 - ETA: 54:18 - loss: 4.11 - ETA: 53:54 - loss: 4.11 - ETA: 53:30 - loss: 4.11 - ETA: 53:06 - loss: 4.11 - ETA: 52:42 - loss: 4.10 - ETA: 52:18 - loss: 4.10 - ETA: 51:54 - loss: 4.10 - ETA: 51:30 - loss: 4.10 - ETA: 51:06 - loss: 4.10 - ETA: 50:42 - loss: 4.10 - ETA: 50:18 - loss: 4.10 - ETA: 49:54 - loss: 4.10 - ETA: 49:30 - loss: 4.10 - ETA: 49:07 - loss: 4.10 - ETA: 48:43 - loss: 4.10 - ETA: 48:19 - loss: 4.10 - ETA: 47:54 - loss: 4.10 - ETA: 47:30 - loss: 4.10 - ETA: 47:06 - loss: 4.10 - ETA: 46:42 - loss: 4.10 - ETA: 46:18 - loss: 4.10 - ETA: 45:54 - loss: 4.10 - ETA: 45:30 - loss: 4.10 - ETA: 45:06 - loss: 4.10 - ETA: 44:42 - loss: 4.10 - ETA: 44:18 - loss: 4.09 - ETA: 43:54 - loss: 4.09 - ETA: 43:30 - loss: 4.09 - ETA: 43:06 - loss: 4.09 - ETA: 42:42 - loss: 4.09 - ETA: 42:18 - loss: 4.09 - ETA: 41:54 - loss: 4.09 - ETA: 41:30 - loss: 4.09 - ETA: 41:07 - loss: 4.09 - ETA: 40:42 - loss: 4.09 - ETA: 40:18 - loss: 4.08 - ETA: 39:54 - loss: 4.08 - ETA: 39:30 - loss: 4.08 - ETA: 39:06 - loss: 4.08 - ETA: 38:42 - loss: 4.08 - ETA: 38:18 - loss: 4.08 - ETA: 37:55 - loss: 4.08 - ETA: 37:31 - loss: 4.08 - ETA: 37:08 - loss: 4.08 - ETA: 36:44 - loss: 4.08 - ETA: 36:21 - loss: 4.08 - ETA: 35:57 - loss: 4.08 - ETA: 35:33 - loss: 4.08 - ETA: 35:10 - loss: 4.08 - ETA: 34:46 - loss: 4.08 - ETA: 34:22 - loss: 4.08 - ETA: 33:58 - loss: 4.08 - ETA: 33:34 - loss: 4.08 - ETA: 33:10 - loss: 4.08 - ETA: 32:46 - loss: 4.08 - ETA: 32:22 - loss: 4.07 - ETA: 31:58 - loss: 4.07 - ETA: 31:34 - loss: 4.07 - ETA: 31:10 - loss: 4.07 - ETA: 30:46 - loss: 4.07 - ETA: 30:22 - loss: 4.07 - ETA: 29:58 - loss: 4.07 - ETA: 29:34 - loss: 4.07 - ETA: 29:10 - loss: 4.07 - ETA: 28:46 - loss: 4.07 - ETA: 28:22 - loss: 4.07 - ETA: 27:58 - loss: 4.07 - ETA: 27:34 - loss: 4.06 - ETA: 27:10 - loss: 4.06 - ETA: 26:46 - loss: 4.06 - ETA: 26:22 - loss: 4.06 - ETA: 25:58 - loss: 4.06 - ETA: 25:34 - loss: 4.06 - ETA: 25:10 - loss: 4.06 - ETA: 24:46 - loss: 4.06 - ETA: 24:22 - loss: 4.06 - ETA: 23:58 - loss: 4.06 - ETA: 23:34 - loss: 4.06 - ETA: 23:10 - loss: 4.05 - ETA: 22:46 - loss: 4.05 - ETA: 22:22 - loss: 4.05 - ETA: 21:58 - loss: 4.05 - ETA: 21:34 - loss: 4.05 - ETA: 21:10 - loss: 4.05 - ETA: 20:46 - loss: 4.05 - ETA: 20:22 - loss: 4.05 - ETA: 19:58 - loss: 4.05 - ETA: 19:34 - loss: 4.05 - ETA: 19:10 - loss: 4.05 - ETA: 18:46 - loss: 4.05 - ETA: 18:22 - loss: 4.04 - ETA: 17:58 - loss: 4.04 - ETA: 17:34 - loss: 4.04 - ETA: 17:10 - loss: 4.04 - ETA: 16:46 - loss: 4.04 - ETA: 16:22 - loss: 4.04 - ETA: 15:58 - loss: 4.04 - ETA: 15:34 - loss: 4.04 - ETA: 15:10 - loss: 4.04 - ETA: 14:46 - loss: 4.04 - ETA: 14:22 - loss: 4.04 - ETA: 13:58 - loss: 4.03 - ETA: 13:34 - loss: 4.03956660/6680 [============================>.] - ETA: 13:11 - loss: 4.03 - ETA: 12:47 - loss: 4.03 - ETA: 12:23 - loss: 4.03 - ETA: 11:59 - loss: 4.03 - ETA: 11:35 - loss: 4.03 - ETA: 11:11 - loss: 4.03 - ETA: 10:47 - loss: 4.03 - ETA: 10:23 - loss: 4.03 - ETA: 9:59 - loss: 4.0350 - ETA: 9:35 - loss: 4.033 - ETA: 9:11 - loss: 4.032 - ETA: 8:47 - loss: 4.032 - ETA: 8:23 - loss: 4.031 - ETA: 7:59 - loss: 4.031 - ETA: 7:35 - loss: 4.031 - ETA: 7:11 - loss: 4.030 - ETA: 6:47 - loss: 4.029 - ETA: 6:23 - loss: 4.028 - ETA: 5:59 - loss: 4.026 - ETA: 5:35 - loss: 4.025 - ETA: 5:11 - loss: 4.024 - ETA: 4:47 - loss: 4.022 - ETA: 4:23 - loss: 4.023 - ETA: 3:59 - loss: 4.022 - ETA: 3:35 - loss: 4.021 - ETA: 3:11 - loss: 4.020 - ETA: 2:47 - loss: 4.021 - ETA: 2:23 - loss: 4.019 - ETA: 1:59 - loss: 4.018 - ETA: 1:35 - loss: 4.017 - ETA: 1:11 - loss: 4.016 - ETA: 47s - loss: 4.016 - ETA: 23s - loss: 4.0154\n",
      "Epoch 00003: val_loss improved from 4.24397 to 3.64678, saving model to saved_models/weights.best.Model_V3.hdf5\n",
      "6680/6680 [==============================] - 8369s 1s/step - loss: 4.0145 - val_loss: 3.6468\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6680 [=========================>....] - ETA: 2:17:05 - loss: 3.60 - ETA: 2:15:16 - loss: 3.86 - ETA: 2:14:08 - loss: 3.82 - ETA: 2:13:08 - loss: 3.77 - ETA: 2:12:10 - loss: 3.77 - ETA: 2:11:30 - loss: 3.78 - ETA: 2:11:09 - loss: 3.76 - ETA: 2:10:35 - loss: 3.76 - ETA: 2:10:03 - loss: 3.74 - ETA: 2:09:35 - loss: 3.73 - ETA: 2:09:04 - loss: 3.72 - ETA: 2:08:42 - loss: 3.72 - ETA: 2:08:12 - loss: 3.71 - ETA: 2:07:48 - loss: 3.71 - ETA: 2:07:16 - loss: 3.70 - ETA: 2:06:46 - loss: 3.69 - ETA: 2:06:26 - loss: 3.69 - ETA: 2:05:57 - loss: 3.68 - ETA: 2:05:31 - loss: 3.69 - ETA: 2:05:04 - loss: 3.69 - ETA: 2:04:37 - loss: 3.69 - ETA: 2:04:14 - loss: 3.71 - ETA: 2:03:49 - loss: 3.71 - ETA: 2:03:22 - loss: 3.73 - ETA: 2:02:56 - loss: 3.73 - ETA: 2:02:34 - loss: 3.74 - ETA: 2:02:17 - loss: 3.74 - ETA: 2:01:49 - loss: 3.74 - ETA: 2:01:24 - loss: 3.74 - ETA: 2:00:57 - loss: 3.73 - ETA: 2:00:32 - loss: 3.73 - ETA: 2:00:10 - loss: 3.73 - ETA: 1:59:44 - loss: 3.72 - ETA: 1:59:20 - loss: 3.73 - ETA: 1:58:54 - loss: 3.72 - ETA: 1:58:29 - loss: 3.72 - ETA: 1:58:05 - loss: 3.72 - ETA: 1:57:40 - loss: 3.73 - ETA: 1:57:16 - loss: 3.72 - ETA: 1:56:50 - loss: 3.72 - ETA: 1:56:25 - loss: 3.73 - ETA: 1:56:06 - loss: 3.72 - ETA: 1:55:42 - loss: 3.72 - ETA: 1:55:17 - loss: 3.73 - ETA: 1:54:52 - loss: 3.72 - ETA: 1:54:27 - loss: 3.73 - ETA: 1:54:06 - loss: 3.72 - ETA: 1:53:46 - loss: 3.71 - ETA: 1:53:23 - loss: 3.71 - ETA: 1:53:00 - loss: 3.71 - ETA: 1:52:35 - loss: 3.71 - ETA: 1:52:12 - loss: 3.71 - ETA: 1:51:51 - loss: 3.72 - ETA: 1:51:27 - loss: 3.71 - ETA: 1:51:02 - loss: 3.70 - ETA: 1:50:37 - loss: 3.71 - ETA: 1:50:15 - loss: 3.71 - ETA: 1:49:51 - loss: 3.71 - ETA: 1:49:26 - loss: 3.72 - ETA: 1:49:03 - loss: 3.72 - ETA: 1:48:39 - loss: 3.71 - ETA: 1:48:15 - loss: 3.71 - ETA: 1:47:51 - loss: 3.71 - ETA: 1:47:26 - loss: 3.70 - ETA: 1:47:01 - loss: 3.71 - ETA: 1:46:36 - loss: 3.71 - ETA: 1:46:12 - loss: 3.70 - ETA: 1:45:49 - loss: 3.70 - ETA: 1:45:25 - loss: 3.70 - ETA: 1:45:00 - loss: 3.70 - ETA: 1:44:36 - loss: 3.70 - ETA: 1:44:14 - loss: 3.70 - ETA: 1:43:49 - loss: 3.70 - ETA: 1:43:25 - loss: 3.70 - ETA: 1:43:00 - loss: 3.70 - ETA: 1:42:36 - loss: 3.69 - ETA: 1:42:12 - loss: 3.69 - ETA: 1:41:48 - loss: 3.69 - ETA: 1:41:23 - loss: 3.69 - ETA: 1:40:59 - loss: 3.69 - ETA: 1:40:35 - loss: 3.69 - ETA: 1:40:11 - loss: 3.68 - ETA: 1:39:48 - loss: 3.68 - ETA: 1:39:25 - loss: 3.68 - ETA: 1:39:02 - loss: 3.68 - ETA: 1:38:40 - loss: 3.68 - ETA: 1:38:16 - loss: 3.68 - ETA: 1:37:51 - loss: 3.68 - ETA: 1:37:27 - loss: 3.67 - ETA: 1:37:03 - loss: 3.67 - ETA: 1:36:39 - loss: 3.67 - ETA: 1:36:14 - loss: 3.66 - ETA: 1:35:51 - loss: 3.66 - ETA: 1:35:27 - loss: 3.66 - ETA: 1:35:02 - loss: 3.65 - ETA: 1:34:38 - loss: 3.66 - ETA: 1:34:14 - loss: 3.66 - ETA: 1:33:52 - loss: 3.66 - ETA: 1:33:28 - loss: 3.65 - ETA: 1:33:04 - loss: 3.65 - ETA: 1:32:40 - loss: 3.65 - ETA: 1:32:16 - loss: 3.65 - ETA: 1:31:52 - loss: 3.65 - ETA: 1:31:28 - loss: 3.65 - ETA: 1:31:04 - loss: 3.65 - ETA: 1:30:40 - loss: 3.65 - ETA: 1:30:16 - loss: 3.65 - ETA: 1:29:52 - loss: 3.65 - ETA: 1:29:29 - loss: 3.65 - ETA: 1:29:05 - loss: 3.65 - ETA: 1:28:41 - loss: 3.65 - ETA: 1:28:17 - loss: 3.65 - ETA: 1:27:53 - loss: 3.65 - ETA: 1:27:29 - loss: 3.65 - ETA: 1:27:05 - loss: 3.64 - ETA: 1:26:41 - loss: 3.64 - ETA: 1:26:17 - loss: 3.64 - ETA: 1:25:53 - loss: 3.64 - ETA: 1:25:29 - loss: 3.64 - ETA: 1:25:05 - loss: 3.64 - ETA: 1:24:41 - loss: 3.64 - ETA: 1:24:17 - loss: 3.64 - ETA: 1:23:55 - loss: 3.64 - ETA: 1:23:32 - loss: 3.64 - ETA: 1:23:09 - loss: 3.64 - ETA: 1:22:44 - loss: 3.64 - ETA: 1:22:21 - loss: 3.64 - ETA: 1:21:57 - loss: 3.64 - ETA: 1:21:34 - loss: 3.64 - ETA: 1:21:11 - loss: 3.63 - ETA: 1:20:46 - loss: 3.64 - ETA: 1:20:22 - loss: 3.63 - ETA: 1:19:59 - loss: 3.64 - ETA: 1:19:34 - loss: 3.64 - ETA: 1:19:10 - loss: 3.63 - ETA: 1:18:46 - loss: 3.63 - ETA: 1:18:22 - loss: 3.63 - ETA: 1:18:00 - loss: 3.63 - ETA: 1:17:36 - loss: 3.63 - ETA: 1:17:11 - loss: 3.63 - ETA: 1:16:47 - loss: 3.62 - ETA: 1:16:23 - loss: 3.63 - ETA: 1:15:59 - loss: 3.62 - ETA: 1:15:35 - loss: 3.62 - ETA: 1:15:11 - loss: 3.62 - ETA: 1:14:47 - loss: 3.62 - ETA: 1:14:23 - loss: 3.62 - ETA: 1:14:00 - loss: 3.62 - ETA: 1:13:36 - loss: 3.62 - ETA: 1:13:12 - loss: 3.62 - ETA: 1:12:48 - loss: 3.62 - ETA: 1:12:24 - loss: 3.62 - ETA: 1:12:01 - loss: 3.62 - ETA: 1:11:37 - loss: 3.62 - ETA: 1:11:13 - loss: 3.62 - ETA: 1:10:49 - loss: 3.62 - ETA: 1:10:25 - loss: 3.61 - ETA: 1:10:01 - loss: 3.61 - ETA: 1:09:37 - loss: 3.61 - ETA: 1:09:13 - loss: 3.61 - ETA: 1:08:50 - loss: 3.61 - ETA: 1:08:26 - loss: 3.61 - ETA: 1:08:03 - loss: 3.61 - ETA: 1:07:39 - loss: 3.61 - ETA: 1:07:15 - loss: 3.61 - ETA: 1:06:51 - loss: 3.61 - ETA: 1:06:27 - loss: 3.61 - ETA: 1:06:04 - loss: 3.61 - ETA: 1:05:40 - loss: 3.61 - ETA: 1:05:16 - loss: 3.60 - ETA: 1:04:52 - loss: 3.61 - ETA: 1:04:27 - loss: 3.61 - ETA: 1:04:04 - loss: 3.60 - ETA: 1:03:40 - loss: 3.61 - ETA: 1:03:16 - loss: 3.60 - ETA: 1:02:52 - loss: 3.60 - ETA: 1:02:28 - loss: 3.61 - ETA: 1:02:04 - loss: 3.60 - ETA: 1:01:40 - loss: 3.61 - ETA: 1:01:16 - loss: 3.60 - ETA: 1:00:53 - loss: 3.61 - ETA: 1:00:29 - loss: 3.61 - ETA: 1:00:05 - loss: 3.60 - ETA: 59:41 - loss: 3.6065 - ETA: 59:17 - loss: 3.60 - ETA: 58:53 - loss: 3.60 - ETA: 58:29 - loss: 3.60 - ETA: 58:05 - loss: 3.60 - ETA: 57:41 - loss: 3.60 - ETA: 57:18 - loss: 3.59 - ETA: 56:54 - loss: 3.59 - ETA: 56:30 - loss: 3.59 - ETA: 56:06 - loss: 3.59 - ETA: 55:43 - loss: 3.59 - ETA: 55:19 - loss: 3.59 - ETA: 54:55 - loss: 3.59 - ETA: 54:31 - loss: 3.59 - ETA: 54:07 - loss: 3.59 - ETA: 53:44 - loss: 3.59 - ETA: 53:21 - loss: 3.59 - ETA: 52:58 - loss: 3.59 - ETA: 52:35 - loss: 3.58 - ETA: 52:12 - loss: 3.58 - ETA: 51:49 - loss: 3.58 - ETA: 51:26 - loss: 3.58 - ETA: 51:03 - loss: 3.58 - ETA: 50:40 - loss: 3.58 - ETA: 50:17 - loss: 3.58 - ETA: 49:54 - loss: 3.58 - ETA: 49:30 - loss: 3.57 - ETA: 49:06 - loss: 3.57 - ETA: 48:42 - loss: 3.57 - ETA: 48:18 - loss: 3.57 - ETA: 47:54 - loss: 3.57 - ETA: 47:30 - loss: 3.57 - ETA: 47:06 - loss: 3.57 - ETA: 46:42 - loss: 3.57 - ETA: 46:18 - loss: 3.57 - ETA: 45:54 - loss: 3.57 - ETA: 45:30 - loss: 3.57 - ETA: 45:06 - loss: 3.57 - ETA: 44:42 - loss: 3.57 - ETA: 44:18 - loss: 3.56 - ETA: 43:54 - loss: 3.56 - ETA: 43:30 - loss: 3.56 - ETA: 43:06 - loss: 3.56 - ETA: 42:42 - loss: 3.56 - ETA: 42:19 - loss: 3.56 - ETA: 41:55 - loss: 3.56 - ETA: 41:31 - loss: 3.56 - ETA: 41:07 - loss: 3.56 - ETA: 40:43 - loss: 3.56 - ETA: 40:19 - loss: 3.56 - ETA: 39:55 - loss: 3.56 - ETA: 39:31 - loss: 3.56 - ETA: 39:07 - loss: 3.56 - ETA: 38:43 - loss: 3.56 - ETA: 38:19 - loss: 3.56 - ETA: 37:55 - loss: 3.56 - ETA: 37:31 - loss: 3.56 - ETA: 37:07 - loss: 3.56 - ETA: 36:44 - loss: 3.55 - ETA: 36:20 - loss: 3.55 - ETA: 35:56 - loss: 3.55 - ETA: 35:32 - loss: 3.55 - ETA: 35:08 - loss: 3.55 - ETA: 34:44 - loss: 3.55 - ETA: 34:20 - loss: 3.55 - ETA: 33:56 - loss: 3.55 - ETA: 33:32 - loss: 3.55 - ETA: 33:08 - loss: 3.55 - ETA: 32:44 - loss: 3.55 - ETA: 32:20 - loss: 3.55 - ETA: 31:56 - loss: 3.55 - ETA: 31:32 - loss: 3.55 - ETA: 31:08 - loss: 3.55 - ETA: 30:44 - loss: 3.55 - ETA: 30:20 - loss: 3.55 - ETA: 29:56 - loss: 3.55 - ETA: 29:32 - loss: 3.55 - ETA: 29:08 - loss: 3.55 - ETA: 28:44 - loss: 3.54 - ETA: 28:20 - loss: 3.54 - ETA: 27:56 - loss: 3.54 - ETA: 27:32 - loss: 3.54 - ETA: 27:08 - loss: 3.54 - ETA: 26:44 - loss: 3.54 - ETA: 26:21 - loss: 3.54 - ETA: 25:57 - loss: 3.54 - ETA: 25:33 - loss: 3.54 - ETA: 25:09 - loss: 3.54 - ETA: 24:45 - loss: 3.53 - ETA: 24:21 - loss: 3.53 - ETA: 23:57 - loss: 3.53 - ETA: 23:33 - loss: 3.53 - ETA: 23:09 - loss: 3.53 - ETA: 22:45 - loss: 3.53 - ETA: 22:21 - loss: 3.53 - ETA: 21:57 - loss: 3.53 - ETA: 21:34 - loss: 3.53 - ETA: 21:10 - loss: 3.53 - ETA: 20:46 - loss: 3.53 - ETA: 20:22 - loss: 3.52 - ETA: 19:58 - loss: 3.52 - ETA: 19:34 - loss: 3.52 - ETA: 19:10 - loss: 3.52 - ETA: 18:46 - loss: 3.52 - ETA: 18:22 - loss: 3.52 - ETA: 17:58 - loss: 3.52 - ETA: 17:34 - loss: 3.52 - ETA: 17:10 - loss: 3.52 - ETA: 16:46 - loss: 3.52 - ETA: 16:22 - loss: 3.52 - ETA: 15:58 - loss: 3.52 - ETA: 15:34 - loss: 3.52 - ETA: 15:10 - loss: 3.52 - ETA: 14:46 - loss: 3.51 - ETA: 14:22 - loss: 3.51 - ETA: 13:58 - loss: 3.51 - ETA: 13:34 - loss: 3.5152"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 13:10 - loss: 3.51 - ETA: 12:46 - loss: 3.51 - ETA: 12:22 - loss: 3.51 - ETA: 11:58 - loss: 3.51 - ETA: 11:34 - loss: 3.51 - ETA: 11:10 - loss: 3.51 - ETA: 10:46 - loss: 3.51 - ETA: 10:22 - loss: 3.50 - ETA: 9:58 - loss: 3.5087 - ETA: 9:35 - loss: 3.508 - ETA: 9:11 - loss: 3.508 - ETA: 8:47 - loss: 3.508 - ETA: 8:23 - loss: 3.506 - ETA: 7:59 - loss: 3.506 - ETA: 7:35 - loss: 3.504 - ETA: 7:11 - loss: 3.504 - ETA: 6:47 - loss: 3.504 - ETA: 6:23 - loss: 3.503 - ETA: 5:59 - loss: 3.502 - ETA: 5:35 - loss: 3.502 - ETA: 5:11 - loss: 3.501 - ETA: 4:47 - loss: 3.501 - ETA: 4:23 - loss: 3.500 - ETA: 3:59 - loss: 3.498 - ETA: 3:35 - loss: 3.498 - ETA: 3:11 - loss: 3.499 - ETA: 2:47 - loss: 3.497 - ETA: 2:23 - loss: 3.496 - ETA: 1:59 - loss: 3.495 - ETA: 1:35 - loss: 3.494 - ETA: 1:11 - loss: 3.493 - ETA: 47s - loss: 3.492 - ETA: 23s - loss: 3.4915\n",
      "Epoch 00004: val_loss improved from 3.64678 to 3.07508, saving model to saved_models/weights.best.Model_V3.hdf5\n",
      "6680/6680 [==============================] - 8368s 1s/step - loss: 3.4919 - val_loss: 3.0751\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6680 [=========================>....] - ETA: 2:15:27 - loss: 3.49 - ETA: 2:13:38 - loss: 3.35 - ETA: 2:12:57 - loss: 3.30 - ETA: 2:12:10 - loss: 3.41 - ETA: 2:11:39 - loss: 3.33 - ETA: 2:11:06 - loss: 3.29 - ETA: 2:10:47 - loss: 3.31 - ETA: 2:10:49 - loss: 3.25 - ETA: 2:10:20 - loss: 3.21 - ETA: 2:09:46 - loss: 3.21 - ETA: 2:09:15 - loss: 3.20 - ETA: 2:08:44 - loss: 3.18 - ETA: 2:08:18 - loss: 3.17 - ETA: 2:07:50 - loss: 3.18 - ETA: 2:07:20 - loss: 3.17 - ETA: 2:06:52 - loss: 3.15 - ETA: 2:06:25 - loss: 3.16 - ETA: 2:06:00 - loss: 3.16 - ETA: 2:05:36 - loss: 3.15 - ETA: 2:05:09 - loss: 3.15 - ETA: 2:04:44 - loss: 3.14 - ETA: 2:04:19 - loss: 3.15 - ETA: 2:04:03 - loss: 3.15 - ETA: 2:03:39 - loss: 3.15 - ETA: 2:03:11 - loss: 3.14 - ETA: 2:02:46 - loss: 3.14 - ETA: 2:02:20 - loss: 3.15 - ETA: 2:01:56 - loss: 3.13 - ETA: 2:01:32 - loss: 3.14 - ETA: 2:01:07 - loss: 3.15 - ETA: 2:00:40 - loss: 3.15 - ETA: 2:00:16 - loss: 3.15 - ETA: 1:59:54 - loss: 3.15 - ETA: 1:59:30 - loss: 3.14 - ETA: 1:59:05 - loss: 3.15 - ETA: 1:58:43 - loss: 3.15 - ETA: 1:58:24 - loss: 3.15 - ETA: 1:58:00 - loss: 3.15 - ETA: 1:57:36 - loss: 3.14 - ETA: 1:57:12 - loss: 3.15 - ETA: 1:56:48 - loss: 3.16 - ETA: 1:56:24 - loss: 3.15 - ETA: 1:56:03 - loss: 3.16 - ETA: 1:55:39 - loss: 3.16 - ETA: 1:55:15 - loss: 3.16 - ETA: 1:54:50 - loss: 3.15 - ETA: 1:54:29 - loss: 3.15 - ETA: 1:54:06 - loss: 3.15 - ETA: 1:53:46 - loss: 3.14 - ETA: 1:53:21 - loss: 3.15 - ETA: 1:52:58 - loss: 3.15 - ETA: 1:52:32 - loss: 3.15 - ETA: 1:52:11 - loss: 3.15 - ETA: 1:51:48 - loss: 3.15 - ETA: 1:51:23 - loss: 3.15 - ETA: 1:50:58 - loss: 3.15 - ETA: 1:50:33 - loss: 3.16 - ETA: 1:50:09 - loss: 3.15 - ETA: 1:49:46 - loss: 3.15 - ETA: 1:49:21 - loss: 3.15 - ETA: 1:48:57 - loss: 3.15 - ETA: 1:48:33 - loss: 3.14 - ETA: 1:48:09 - loss: 3.14 - ETA: 1:47:50 - loss: 3.15 - ETA: 1:47:30 - loss: 3.14 - ETA: 1:47:06 - loss: 3.14 - ETA: 1:46:41 - loss: 3.15 - ETA: 1:46:17 - loss: 3.15 - ETA: 1:45:54 - loss: 3.15 - ETA: 1:45:28 - loss: 3.14 - ETA: 1:45:04 - loss: 3.14 - ETA: 1:44:39 - loss: 3.14 - ETA: 1:44:15 - loss: 3.15 - ETA: 1:43:55 - loss: 3.15 - ETA: 1:43:30 - loss: 3.15 - ETA: 1:43:06 - loss: 3.15 - ETA: 1:42:42 - loss: 3.15 - ETA: 1:42:18 - loss: 3.15 - ETA: 1:41:54 - loss: 3.15 - ETA: 1:41:30 - loss: 3.15 - ETA: 1:41:07 - loss: 3.15 - ETA: 1:40:43 - loss: 3.15 - ETA: 1:40:19 - loss: 3.15 - ETA: 1:39:55 - loss: 3.15 - ETA: 1:39:31 - loss: 3.14 - ETA: 1:39:06 - loss: 3.14 - ETA: 1:38:42 - loss: 3.14 - ETA: 1:38:17 - loss: 3.14 - ETA: 1:37:53 - loss: 3.14 - ETA: 1:37:30 - loss: 3.14 - ETA: 1:37:06 - loss: 3.13 - ETA: 1:36:42 - loss: 3.13 - ETA: 1:36:17 - loss: 3.13 - ETA: 1:35:54 - loss: 3.13 - ETA: 1:35:30 - loss: 3.13 - ETA: 1:35:07 - loss: 3.12 - ETA: 1:34:42 - loss: 3.13 - ETA: 1:34:19 - loss: 3.13 - ETA: 1:33:56 - loss: 3.13 - ETA: 1:33:32 - loss: 3.13 - ETA: 1:33:08 - loss: 3.13 - ETA: 1:32:43 - loss: 3.12 - ETA: 1:32:20 - loss: 3.12 - ETA: 1:31:57 - loss: 3.12 - ETA: 1:31:34 - loss: 3.13 - ETA: 1:31:10 - loss: 3.12 - ETA: 1:30:46 - loss: 3.13 - ETA: 1:30:22 - loss: 3.12 - ETA: 1:29:59 - loss: 3.13 - ETA: 1:29:34 - loss: 3.12 - ETA: 1:29:11 - loss: 3.12 - ETA: 1:28:48 - loss: 3.12 - ETA: 1:28:24 - loss: 3.12 - ETA: 1:28:00 - loss: 3.12 - ETA: 1:27:35 - loss: 3.11 - ETA: 1:27:11 - loss: 3.12 - ETA: 1:26:46 - loss: 3.11 - ETA: 1:26:23 - loss: 3.12 - ETA: 1:26:00 - loss: 3.12 - ETA: 1:25:35 - loss: 3.12 - ETA: 1:25:11 - loss: 3.12 - ETA: 1:24:47 - loss: 3.12 - ETA: 1:24:23 - loss: 3.12 - ETA: 1:23:59 - loss: 3.12 - ETA: 1:23:35 - loss: 3.11 - ETA: 1:23:11 - loss: 3.12 - ETA: 1:22:47 - loss: 3.11 - ETA: 1:22:23 - loss: 3.11 - ETA: 1:21:59 - loss: 3.11 - ETA: 1:21:35 - loss: 3.11 - ETA: 1:21:11 - loss: 3.11 - ETA: 1:20:47 - loss: 3.11 - ETA: 1:20:23 - loss: 3.11 - ETA: 1:19:59 - loss: 3.11 - ETA: 1:19:35 - loss: 3.11 - ETA: 1:19:11 - loss: 3.11 - ETA: 1:18:46 - loss: 3.11 - ETA: 1:18:23 - loss: 3.11 - ETA: 1:17:59 - loss: 3.10 - ETA: 1:17:35 - loss: 3.10 - ETA: 1:17:10 - loss: 3.10 - ETA: 1:16:46 - loss: 3.10 - ETA: 1:16:22 - loss: 3.10 - ETA: 1:15:59 - loss: 3.10 - ETA: 1:15:35 - loss: 3.10 - ETA: 1:15:11 - loss: 3.10 - ETA: 1:14:47 - loss: 3.10 - ETA: 1:14:23 - loss: 3.09 - ETA: 1:14:00 - loss: 3.09 - ETA: 1:13:36 - loss: 3.09 - ETA: 1:13:11 - loss: 3.09 - ETA: 1:12:47 - loss: 3.09 - ETA: 1:12:23 - loss: 3.09 - ETA: 1:11:59 - loss: 3.09 - ETA: 1:11:35 - loss: 3.09 - ETA: 1:11:10 - loss: 3.09 - ETA: 1:10:46 - loss: 3.09 - ETA: 1:10:22 - loss: 3.09 - ETA: 1:09:58 - loss: 3.09 - ETA: 1:09:34 - loss: 3.09 - ETA: 1:09:11 - loss: 3.09 - ETA: 1:08:47 - loss: 3.09 - ETA: 1:08:24 - loss: 3.09 - ETA: 1:08:02 - loss: 3.09 - ETA: 1:07:39 - loss: 3.09 - ETA: 1:07:15 - loss: 3.09 - ETA: 1:06:51 - loss: 3.09 - ETA: 1:06:26 - loss: 3.09 - ETA: 1:06:02 - loss: 3.09 - ETA: 1:05:37 - loss: 3.09 - ETA: 1:05:13 - loss: 3.08 - ETA: 1:04:49 - loss: 3.08 - ETA: 1:04:26 - loss: 3.08 - ETA: 1:04:01 - loss: 3.08 - ETA: 1:03:37 - loss: 3.08 - ETA: 1:03:13 - loss: 3.08 - ETA: 1:02:49 - loss: 3.08 - ETA: 1:02:25 - loss: 3.08 - ETA: 1:02:01 - loss: 3.08 - ETA: 1:01:37 - loss: 3.08 - ETA: 1:01:12 - loss: 3.08 - ETA: 1:00:48 - loss: 3.07 - ETA: 1:00:24 - loss: 3.07 - ETA: 1:00:00 - loss: 3.07 - ETA: 59:36 - loss: 3.0739 - ETA: 59:12 - loss: 3.07 - ETA: 58:48 - loss: 3.07 - ETA: 58:24 - loss: 3.07 - ETA: 58:00 - loss: 3.07 - ETA: 57:36 - loss: 3.07 - ETA: 57:12 - loss: 3.07 - ETA: 56:48 - loss: 3.07 - ETA: 56:24 - loss: 3.06 - ETA: 56:00 - loss: 3.06 - ETA: 55:36 - loss: 3.06 - ETA: 55:12 - loss: 3.06 - ETA: 54:47 - loss: 3.06 - ETA: 54:24 - loss: 3.06 - ETA: 53:59 - loss: 3.06 - ETA: 53:35 - loss: 3.06 - ETA: 53:11 - loss: 3.06 - ETA: 52:47 - loss: 3.06 - ETA: 52:23 - loss: 3.05 - ETA: 51:59 - loss: 3.05 - ETA: 51:34 - loss: 3.05 - ETA: 51:10 - loss: 3.05 - ETA: 50:46 - loss: 3.05 - ETA: 50:22 - loss: 3.05 - ETA: 49:58 - loss: 3.05 - ETA: 49:34 - loss: 3.05 - ETA: 49:10 - loss: 3.05 - ETA: 48:46 - loss: 3.05 - ETA: 48:22 - loss: 3.05 - ETA: 47:58 - loss: 3.05 - ETA: 47:34 - loss: 3.05 - ETA: 47:10 - loss: 3.04 - ETA: 46:45 - loss: 3.05 - ETA: 46:21 - loss: 3.05 - ETA: 45:57 - loss: 3.05 - ETA: 45:33 - loss: 3.05 - ETA: 45:09 - loss: 3.05 - ETA: 44:45 - loss: 3.04 - ETA: 44:21 - loss: 3.04 - ETA: 43:58 - loss: 3.04 - ETA: 43:34 - loss: 3.04 - ETA: 43:10 - loss: 3.04 - ETA: 42:46 - loss: 3.04 - ETA: 42:22 - loss: 3.04 - ETA: 41:58 - loss: 3.04 - ETA: 41:34 - loss: 3.04 - ETA: 41:10 - loss: 3.04 - ETA: 40:46 - loss: 3.04 - ETA: 40:22 - loss: 3.04 - ETA: 39:58 - loss: 3.04 - ETA: 39:34 - loss: 3.03 - ETA: 39:10 - loss: 3.03 - ETA: 38:46 - loss: 3.03 - ETA: 38:22 - loss: 3.03 - ETA: 37:58 - loss: 3.03 - ETA: 37:34 - loss: 3.03 - ETA: 37:10 - loss: 3.03 - ETA: 36:46 - loss: 3.02 - ETA: 36:22 - loss: 3.02 - ETA: 35:58 - loss: 3.03 - ETA: 35:34 - loss: 3.02 - ETA: 35:10 - loss: 3.02 - ETA: 34:46 - loss: 3.02 - ETA: 34:23 - loss: 3.02 - ETA: 34:04 - loss: 3.02 - ETA: 33:45 - loss: 3.02 - ETA: 33:24 - loss: 3.02 - ETA: 33:02 - loss: 3.02 - ETA: 32:39 - loss: 3.02 - ETA: 32:16 - loss: 3.01 - ETA: 31:52 - loss: 3.01 - ETA: 31:28 - loss: 3.01 - ETA: 31:04 - loss: 3.01 - ETA: 30:41 - loss: 3.01 - ETA: 30:17 - loss: 3.01 - ETA: 29:53 - loss: 3.01 - ETA: 29:29 - loss: 3.01 - ETA: 29:06 - loss: 3.01 - ETA: 28:42 - loss: 3.01 - ETA: 28:17 - loss: 3.01 - ETA: 27:53 - loss: 3.01 - ETA: 27:29 - loss: 3.01 - ETA: 27:05 - loss: 3.01 - ETA: 26:41 - loss: 3.01 - ETA: 26:17 - loss: 3.00 - ETA: 25:53 - loss: 3.00 - ETA: 25:29 - loss: 3.00 - ETA: 25:04 - loss: 3.00 - ETA: 24:40 - loss: 3.00 - ETA: 24:16 - loss: 3.00 - ETA: 23:52 - loss: 3.00 - ETA: 23:28 - loss: 3.00 - ETA: 23:03 - loss: 3.00 - ETA: 22:39 - loss: 3.00 - ETA: 22:15 - loss: 3.00 - ETA: 21:51 - loss: 3.00 - ETA: 21:26 - loss: 2.99 - ETA: 21:02 - loss: 2.99 - ETA: 20:38 - loss: 2.99 - ETA: 20:14 - loss: 2.99 - ETA: 19:49 - loss: 2.99 - ETA: 19:25 - loss: 2.99 - ETA: 19:01 - loss: 2.99 - ETA: 18:37 - loss: 2.99 - ETA: 18:12 - loss: 2.99 - ETA: 17:48 - loss: 2.99 - ETA: 17:24 - loss: 2.99 - ETA: 17:00 - loss: 2.99 - ETA: 16:35 - loss: 2.99 - ETA: 16:11 - loss: 2.99 - ETA: 15:47 - loss: 2.99 - ETA: 15:23 - loss: 2.98 - ETA: 14:59 - loss: 2.98 - ETA: 14:34 - loss: 2.98 - ETA: 14:10 - loss: 2.98 - ETA: 13:46 - loss: 2.98826660/6680 [============================>.] - ETA: 13:21 - loss: 2.98 - ETA: 12:57 - loss: 2.98 - ETA: 12:33 - loss: 2.98 - ETA: 12:09 - loss: 2.98 - ETA: 11:44 - loss: 2.98 - ETA: 11:21 - loss: 2.98 - ETA: 10:57 - loss: 2.98 - ETA: 10:34 - loss: 2.98 - ETA: 10:10 - loss: 2.98 - ETA: 9:46 - loss: 2.9823 - ETA: 9:23 - loss: 2.982 - ETA: 8:59 - loss: 2.981 - ETA: 8:35 - loss: 2.979 - ETA: 8:11 - loss: 2.978 - ETA: 7:47 - loss: 2.977 - ETA: 7:23 - loss: 2.975 - ETA: 6:58 - loss: 2.974 - ETA: 6:34 - loss: 2.974 - ETA: 6:10 - loss: 2.973 - ETA: 5:46 - loss: 2.972 - ETA: 5:21 - loss: 2.970 - ETA: 4:57 - loss: 2.970 - ETA: 4:32 - loss: 2.969 - ETA: 4:08 - loss: 2.967 - ETA: 3:43 - loss: 2.966 - ETA: 3:19 - loss: 2.966 - ETA: 2:54 - loss: 2.965 - ETA: 2:29 - loss: 2.966 - ETA: 2:04 - loss: 2.965 - ETA: 1:39 - loss: 2.965 - ETA: 1:15 - loss: 2.963 - ETA: 50s - loss: 2.963 - ETA: 25s - loss: 2.9627"
     ]
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Model_V3.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model_V3.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (Unable to open file: name = 'saved_models/weights.best.model_v3.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-eed69d6cf03f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### TODO: Load the model weights with the best validation loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_V3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved_models/weights.best.Model_V3.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[0;32m   2636\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2637\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`load_weights` requires h5py.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2638\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'layer_names'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'model_weights'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2640\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = 'saved_models/weights.best.model_v3.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "model_V3.load_weights('saved_models/weights.best.Model_V3.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
